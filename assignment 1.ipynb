{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZv6wKZv3Uxz"
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xx893mXg4D14"
   },
   "outputs": [],
   "source": [
    "# Define the transform to convert the images to PyTorch tensors and normalize the pixel values\n",
    "trsform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))]) #transforms.Normalize : normalize the color range to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "1651dc0de6d24f58b17f8efeac64a692",
      "6cb91f0412434f22b3e059487c7d1224",
      "65a1b6403b974035878c886adc934dd5",
      "4a517d4427f84b0086bbfbc8a4293df9",
      "b31be24fbebb49b9ac176550846f30bb",
      "b914fb7adafe48ce8a170dfe020c163b",
      "4a2e7850b4064076a7bcc15cf3ac0e4a",
      "33bd9b6d29e4409f9a6f679f349bd3fc",
      "a3949979b0e44739bd7d5ba02295b887",
      "a40b7331fdd64a34abe201482ad8e500",
      "449464383d1a4b08aeaf1024afd60ed9"
     ]
    },
    "id": "_LMiDlce4u5L",
    "outputId": "3ceb9ec3-0c44-4264-88c9-cacbfed27780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1651dc0de6d24f58b17f8efeac64a692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/561753746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n"
     ]
    }
   ],
   "source": [
    "# training dataset and test dataset\n",
    "train_set = torchvision.datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=trsform)\n",
    "test_set = torchvision.datasets.EMNIST(root='./data', split='balanced', train=False,download=True, transform=trsform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVYRS2-S5PVh"
   },
   "outputs": [],
   "source": [
    "# Define the data loaders to load the data in batches\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REa8J_Q85UbR"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# build neural network\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.hidden = torch.nn.Linear(28*28, 256) # input layer 784 neurons , hidden layer with 256 neurons\n",
    "        self.bnormal1 = nn.BatchNorm1d(256)\n",
    "        self.sigmoid = torch.nn.Sigmoid() \n",
    "        self.hidden1 = torch.nn.Linear(256, 64) #hidden layer with 256 neurons\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bnormal2 = nn.BatchNorm1d(64)\n",
    "        self.output = torch.nn.Linear(64, 47)  #output layer\n",
    "        self.dropout = nn.Dropout(dropout_rate) # add dropout layer with specified dropout rate\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.hidden(x) \n",
    "        x = self.bnormal1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dropout(x) #dropout for the first hidden layer\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bnormal2(x)\n",
    "        x = self.dropout(x) #dropout for the second hidden layer\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSWXVgmy5Z0q",
    "outputId": "771f5868-40ba-4a4e-d3a8-aec44ada9852"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (bnormal1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (hidden1): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (bnormal2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (output): Linear(in_features=64, out_features=47, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.1 # specify the dropout rate\n",
    "net = NeuralNetwork(dropout_rate) # instantiate the MLP neural network\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awUmI6Ki5c4x"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Define the loss function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8y2peAAJ7nuE",
    "outputId": "1d21f6d2-3c13-4501-b54c-ec84570a501a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6Tf35Pq7smF",
    "outputId": "128b306f-621b-40af-c27d-2633765c104a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (bnormal1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (hidden1): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (bnormal2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (output): Linear(in_features=64, out_features=47, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we can gpu, let use it!\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J610rCgz7thm",
    "outputId": "c9ec7805-1e8d-4eb1-b387-9d983e40a54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "epoch:8, batch: 290,  loss: 1.1411795616149902\n",
      "epoch:8, batch: 291,  loss: 1.179434061050415\n",
      "epoch:8, batch: 292,  loss: 1.120952844619751\n",
      "epoch:8, batch: 293,  loss: 1.2933733463287354\n",
      "epoch:8, batch: 294,  loss: 1.0598194599151611\n",
      "epoch:8, batch: 295,  loss: 1.1394129991531372\n",
      "epoch:8, batch: 296,  loss: 1.432161808013916\n",
      "epoch:8, batch: 297,  loss: 1.0578584671020508\n",
      "epoch:8, batch: 298,  loss: 0.8489642143249512\n",
      "epoch:8, batch: 299,  loss: 0.9986521005630493\n",
      "epoch:8, batch: 300,  loss: 1.0471547842025757\n",
      "epoch:8, batch: 301,  loss: 1.023817777633667\n",
      "epoch:8, batch: 302,  loss: 1.1571261882781982\n",
      "epoch:8, batch: 303,  loss: 0.8168584704399109\n",
      "epoch:8, batch: 304,  loss: 0.921758234500885\n",
      "epoch:8, batch: 305,  loss: 0.8985430002212524\n",
      "epoch:8, batch: 306,  loss: 1.0149047374725342\n",
      "epoch:8, batch: 307,  loss: 1.560626745223999\n",
      "epoch:8, batch: 308,  loss: 1.0742335319519043\n",
      "epoch:8, batch: 309,  loss: 0.9289897084236145\n",
      "epoch:8, batch: 310,  loss: 1.2737141847610474\n",
      "epoch:8, batch: 311,  loss: 0.8868160247802734\n",
      "epoch:8, batch: 312,  loss: 1.1665050983428955\n",
      "epoch:8, batch: 313,  loss: 1.2129911184310913\n",
      "epoch:8, batch: 314,  loss: 1.3260960578918457\n",
      "epoch:8, batch: 315,  loss: 1.1867191791534424\n",
      "epoch:8, batch: 316,  loss: 1.0288423299789429\n",
      "epoch:8, batch: 317,  loss: 0.9821271896362305\n",
      "epoch:8, batch: 318,  loss: 0.9206987023353577\n",
      "epoch:8, batch: 319,  loss: 0.7675698399543762\n",
      "epoch:8, batch: 320,  loss: 1.299067735671997\n",
      "epoch:8, batch: 321,  loss: 1.0963375568389893\n",
      "epoch:8, batch: 322,  loss: 1.2253036499023438\n",
      "epoch:8, batch: 323,  loss: 1.1748849153518677\n",
      "epoch:8, batch: 324,  loss: 1.124221920967102\n",
      "epoch:8, batch: 325,  loss: 0.9137640595436096\n",
      "epoch:8, batch: 326,  loss: 1.1922284364700317\n",
      "epoch:8, batch: 327,  loss: 0.7599599957466125\n",
      "epoch:8, batch: 328,  loss: 1.3793046474456787\n",
      "epoch:8, batch: 329,  loss: 1.0819275379180908\n",
      "epoch:8, batch: 330,  loss: 1.039864182472229\n",
      "epoch:8, batch: 331,  loss: 0.695004940032959\n",
      "epoch:8, batch: 332,  loss: 1.0443994998931885\n",
      "epoch:8, batch: 333,  loss: 0.8926811814308167\n",
      "epoch:8, batch: 334,  loss: 1.0369832515716553\n",
      "epoch:8, batch: 335,  loss: 1.128316044807434\n",
      "epoch:8, batch: 336,  loss: 0.9446550011634827\n",
      "epoch:8, batch: 337,  loss: 0.9586529731750488\n",
      "epoch:8, batch: 338,  loss: 1.4894181489944458\n",
      "epoch:8, batch: 339,  loss: 0.9701668620109558\n",
      "epoch:8, batch: 340,  loss: 0.8452122211456299\n",
      "epoch:8, batch: 341,  loss: 1.0311825275421143\n",
      "epoch:8, batch: 342,  loss: 0.8590070605278015\n",
      "epoch:8, batch: 343,  loss: 0.8936873078346252\n",
      "epoch:8, batch: 344,  loss: 0.8308228254318237\n",
      "epoch:8, batch: 345,  loss: 0.8804840445518494\n",
      "epoch:8, batch: 346,  loss: 0.9827243685722351\n",
      "epoch:8, batch: 347,  loss: 0.8504555821418762\n",
      "epoch:8, batch: 348,  loss: 1.1724367141723633\n",
      "epoch:8, batch: 349,  loss: 1.1210719347000122\n",
      "epoch:8, batch: 350,  loss: 1.0343310832977295\n",
      "epoch:8, batch: 351,  loss: 0.8808416724205017\n",
      "epoch:8, batch: 352,  loss: 0.8621225953102112\n",
      "epoch:8, batch: 353,  loss: 1.083045244216919\n",
      "epoch:8, batch: 354,  loss: 1.126353144645691\n",
      "epoch:8, batch: 355,  loss: 1.1344085931777954\n",
      "epoch:8, batch: 356,  loss: 1.024422287940979\n",
      "epoch:8, batch: 357,  loss: 0.9706302285194397\n",
      "epoch:8, batch: 358,  loss: 1.2726385593414307\n",
      "epoch:8, batch: 359,  loss: 1.3035942316055298\n",
      "epoch:8, batch: 360,  loss: 0.9031827449798584\n",
      "epoch:8, batch: 361,  loss: 1.4567761421203613\n",
      "epoch:8, batch: 362,  loss: 1.286036491394043\n",
      "epoch:8, batch: 363,  loss: 0.8468087911605835\n",
      "epoch:8, batch: 364,  loss: 1.1198898553848267\n",
      "epoch:8, batch: 365,  loss: 1.0476369857788086\n",
      "epoch:8, batch: 366,  loss: 1.2085821628570557\n",
      "epoch:8, batch: 367,  loss: 1.3296515941619873\n",
      "epoch:8, batch: 368,  loss: 0.7672073841094971\n",
      "epoch:8, batch: 369,  loss: 1.1199294328689575\n",
      "epoch:8, batch: 370,  loss: 1.0476449728012085\n",
      "epoch:8, batch: 371,  loss: 1.0942405462265015\n",
      "epoch:8, batch: 372,  loss: 0.7717088460922241\n",
      "epoch:8, batch: 373,  loss: 1.0938125848770142\n",
      "epoch:8, batch: 374,  loss: 0.8389118313789368\n",
      "epoch:8, batch: 375,  loss: 1.2500637769699097\n",
      "epoch:8, batch: 376,  loss: 1.301206111907959\n",
      "epoch:8, batch: 377,  loss: 0.8839557766914368\n",
      "epoch:8, batch: 378,  loss: 1.0249629020690918\n",
      "epoch:8, batch: 379,  loss: 1.10474693775177\n",
      "epoch:8, batch: 380,  loss: 1.3530890941619873\n",
      "epoch:8, batch: 381,  loss: 0.7744559645652771\n",
      "epoch:8, batch: 382,  loss: 1.110516905784607\n",
      "epoch:8, batch: 383,  loss: 0.9796843528747559\n",
      "epoch:8, batch: 384,  loss: 1.015027403831482\n",
      "epoch:8, batch: 385,  loss: 0.9757465124130249\n",
      "epoch:8, batch: 386,  loss: 1.095912218093872\n",
      "epoch:8, batch: 387,  loss: 1.0143787860870361\n",
      "epoch:8, batch: 388,  loss: 0.8735643625259399\n",
      "epoch:8, batch: 389,  loss: 1.0522583723068237\n",
      "epoch:8, batch: 390,  loss: 1.0188535451889038\n",
      "epoch:8, batch: 391,  loss: 1.431597113609314\n",
      "epoch:8, batch: 392,  loss: 0.9371172785758972\n",
      "epoch:8, batch: 393,  loss: 1.1265709400177002\n",
      "epoch:8, batch: 394,  loss: 1.0875638723373413\n",
      "epoch:8, batch: 395,  loss: 1.1315871477127075\n",
      "epoch:8, batch: 396,  loss: 0.8933649659156799\n",
      "epoch:8, batch: 397,  loss: 1.1106380224227905\n",
      "epoch:8, batch: 398,  loss: 1.1343610286712646\n",
      "epoch:8, batch: 399,  loss: 0.9761818647384644\n",
      "epoch:8, batch: 400,  loss: 0.9300071001052856\n",
      "epoch:8, batch: 401,  loss: 1.352274775505066\n",
      "epoch:8, batch: 402,  loss: 0.9859983921051025\n",
      "epoch:8, batch: 403,  loss: 1.3116878271102905\n",
      "epoch:8, batch: 404,  loss: 0.9495148062705994\n",
      "epoch:8, batch: 405,  loss: 0.6795024275779724\n",
      "epoch:8, batch: 406,  loss: 1.1347479820251465\n",
      "epoch:8, batch: 407,  loss: 1.0585569143295288\n",
      "epoch:8, batch: 408,  loss: 0.9011668562889099\n",
      "epoch:8, batch: 409,  loss: 0.9514256119728088\n",
      "epoch:8, batch: 410,  loss: 1.237858772277832\n",
      "epoch:8, batch: 411,  loss: 0.830390989780426\n",
      "epoch:8, batch: 412,  loss: 0.7510738372802734\n",
      "epoch:8, batch: 413,  loss: 0.8517468571662903\n",
      "epoch:8, batch: 414,  loss: 0.7763381004333496\n",
      "epoch:8, batch: 415,  loss: 1.0460574626922607\n",
      "epoch:8, batch: 416,  loss: 1.1168999671936035\n",
      "epoch:8, batch: 417,  loss: 1.1426199674606323\n",
      "epoch:8, batch: 418,  loss: 1.1933382749557495\n",
      "epoch:8, batch: 419,  loss: 1.0522568225860596\n",
      "epoch:8, batch: 420,  loss: 0.8756563663482666\n",
      "epoch:8, batch: 421,  loss: 1.4402847290039062\n",
      "epoch:8, batch: 422,  loss: 1.0846236944198608\n",
      "epoch:8, batch: 423,  loss: 1.0341012477874756\n",
      "epoch:8, batch: 424,  loss: 1.0759929418563843\n",
      "epoch:8, batch: 425,  loss: 0.9184681177139282\n",
      "epoch:8, batch: 426,  loss: 0.8101912140846252\n",
      "epoch:8, batch: 427,  loss: 0.8511720895767212\n",
      "epoch:8, batch: 428,  loss: 1.0530346632003784\n",
      "epoch:8, batch: 429,  loss: 1.2330659627914429\n",
      "epoch:8, batch: 430,  loss: 1.1772714853286743\n",
      "epoch:8, batch: 431,  loss: 0.9741230010986328\n",
      "epoch:8, batch: 432,  loss: 1.1226686239242554\n",
      "epoch:8, batch: 433,  loss: 1.139665961265564\n",
      "epoch:8, batch: 434,  loss: 1.3753467798233032\n",
      "epoch:8, batch: 435,  loss: 1.036566138267517\n",
      "epoch:8, batch: 436,  loss: 1.0342597961425781\n",
      "epoch:8, batch: 437,  loss: 1.2243335247039795\n",
      "epoch:8, batch: 438,  loss: 0.9985998272895813\n",
      "epoch:8, batch: 439,  loss: 1.093173623085022\n",
      "epoch:8, batch: 440,  loss: 1.1430366039276123\n",
      "epoch:8, batch: 441,  loss: 0.9191355109214783\n",
      "epoch:8, batch: 442,  loss: 1.1517189741134644\n",
      "epoch:8, batch: 443,  loss: 0.8335666060447693\n",
      "epoch:8, batch: 444,  loss: 1.169408917427063\n",
      "epoch:8, batch: 445,  loss: 0.9906187057495117\n",
      "epoch:8, batch: 446,  loss: 1.1733239889144897\n",
      "epoch:8, batch: 447,  loss: 0.859626054763794\n",
      "epoch:8, batch: 448,  loss: 0.8422722220420837\n",
      "epoch:8, batch: 449,  loss: 0.7844192385673523\n",
      "epoch:8, batch: 450,  loss: 0.9507278800010681\n",
      "epoch:8, batch: 451,  loss: 0.9719392657279968\n",
      "epoch:8, batch: 452,  loss: 1.2040599584579468\n",
      "epoch:8, batch: 453,  loss: 1.242757797241211\n",
      "epoch:8, batch: 454,  loss: 0.9929303526878357\n",
      "epoch:8, batch: 455,  loss: 1.160255789756775\n",
      "epoch:8, batch: 456,  loss: 1.340641736984253\n",
      "epoch:8, batch: 457,  loss: 0.8705361485481262\n",
      "epoch:8, batch: 458,  loss: 1.1517854928970337\n",
      "epoch:8, batch: 459,  loss: 0.621991753578186\n",
      "epoch:8, batch: 460,  loss: 1.2006142139434814\n",
      "epoch:8, batch: 461,  loss: 1.2971826791763306\n",
      "epoch:8, batch: 462,  loss: 0.886749267578125\n",
      "epoch:8, batch: 463,  loss: 1.078063726425171\n",
      "epoch:8, batch: 464,  loss: 1.1300832033157349\n",
      "epoch:8, batch: 465,  loss: 1.0650091171264648\n",
      "epoch:8, batch: 466,  loss: 0.9326239228248596\n",
      "epoch:8, batch: 467,  loss: 1.0917472839355469\n",
      "epoch:8, batch: 468,  loss: 0.8235093355178833\n",
      "epoch:8, batch: 469,  loss: 1.2184123992919922\n",
      "epoch:8, batch: 470,  loss: 0.9571924805641174\n",
      "epoch:8, batch: 471,  loss: 1.2536274194717407\n",
      "epoch:8, batch: 472,  loss: 1.1681069135665894\n",
      "epoch:8, batch: 473,  loss: 0.7747311592102051\n",
      "epoch:8, batch: 474,  loss: 1.1969364881515503\n",
      "epoch:8, batch: 475,  loss: 1.1037206649780273\n",
      "epoch:8, batch: 476,  loss: 0.9410027861595154\n",
      "epoch:8, batch: 477,  loss: 1.3782398700714111\n",
      "epoch:8, batch: 478,  loss: 0.9983771443367004\n",
      "epoch:8, batch: 479,  loss: 1.453503131866455\n",
      "epoch:8, batch: 480,  loss: 1.012808918952942\n",
      "epoch:8, batch: 481,  loss: 1.1571208238601685\n",
      "epoch:8, batch: 482,  loss: 0.7820279002189636\n",
      "epoch:8, batch: 483,  loss: 1.1501575708389282\n",
      "epoch:8, batch: 484,  loss: 1.4587979316711426\n",
      "epoch:8, batch: 485,  loss: 0.8366451859474182\n",
      "epoch:8, batch: 486,  loss: 0.9993234872817993\n",
      "epoch:8, batch: 487,  loss: 1.091491460800171\n",
      "epoch:8, batch: 488,  loss: 1.3136842250823975\n",
      "epoch:8, batch: 489,  loss: 1.0970399379730225\n",
      "epoch:8, batch: 490,  loss: 1.301609754562378\n",
      "epoch:8, batch: 491,  loss: 1.1493695974349976\n",
      "epoch:8, batch: 492,  loss: 1.271807312965393\n",
      "epoch:8, batch: 493,  loss: 0.9596211910247803\n",
      "epoch:8, batch: 494,  loss: 0.8994373083114624\n",
      "epoch:8, batch: 495,  loss: 1.033132791519165\n",
      "epoch:8, batch: 496,  loss: 1.3390963077545166\n",
      "epoch:8, batch: 497,  loss: 1.2907719612121582\n",
      "epoch:8, batch: 498,  loss: 1.0120795965194702\n",
      "epoch:8, batch: 499,  loss: 0.953495442867279\n",
      "epoch:8, batch: 500,  loss: 1.067274570465088\n",
      "epoch:8, batch: 501,  loss: 1.1390888690948486\n",
      "epoch:8, batch: 502,  loss: 1.1337147951126099\n",
      "epoch:8, batch: 503,  loss: 1.3755911588668823\n",
      "epoch:8, batch: 504,  loss: 1.1040542125701904\n",
      "epoch:8, batch: 505,  loss: 1.2835874557495117\n",
      "epoch:8, batch: 506,  loss: 1.024918794631958\n",
      "epoch:8, batch: 507,  loss: 1.0071663856506348\n",
      "epoch:8, batch: 508,  loss: 0.952890157699585\n",
      "epoch:8, batch: 509,  loss: 0.9641693234443665\n",
      "epoch:8, batch: 510,  loss: 0.9956977367401123\n",
      "epoch:8, batch: 511,  loss: 1.0043909549713135\n",
      "epoch:8, batch: 512,  loss: 1.2776216268539429\n",
      "epoch:8, batch: 513,  loss: 0.8392714262008667\n",
      "epoch:8, batch: 514,  loss: 1.0882207155227661\n",
      "epoch:8, batch: 515,  loss: 0.9727115631103516\n",
      "epoch:8, batch: 516,  loss: 0.8670382499694824\n",
      "epoch:8, batch: 517,  loss: 0.908286452293396\n",
      "epoch:8, batch: 518,  loss: 1.036163330078125\n",
      "epoch:8, batch: 519,  loss: 0.7913032174110413\n",
      "epoch:8, batch: 520,  loss: 0.9180644750595093\n",
      "epoch:8, batch: 521,  loss: 1.3750773668289185\n",
      "epoch:8, batch: 522,  loss: 1.3219430446624756\n",
      "epoch:8, batch: 523,  loss: 0.9223840236663818\n",
      "epoch:8, batch: 524,  loss: 1.2168519496917725\n",
      "epoch:8, batch: 525,  loss: 0.7626980543136597\n",
      "epoch:8, batch: 526,  loss: 1.0365840196609497\n",
      "epoch:8, batch: 527,  loss: 1.1195317506790161\n",
      "epoch:8, batch: 528,  loss: 0.9967953562736511\n",
      "epoch:8, batch: 529,  loss: 1.0913050174713135\n",
      "epoch:8, batch: 530,  loss: 0.9946483969688416\n",
      "epoch:8, batch: 531,  loss: 0.9210628867149353\n",
      "epoch:8, batch: 532,  loss: 1.383190631866455\n",
      "epoch:8, batch: 533,  loss: 1.2367156744003296\n",
      "epoch:8, batch: 534,  loss: 1.1667872667312622\n",
      "epoch:8, batch: 535,  loss: 0.9033406376838684\n",
      "epoch:8, batch: 536,  loss: 0.9563632607460022\n",
      "epoch:8, batch: 537,  loss: 1.2534337043762207\n",
      "epoch:8, batch: 538,  loss: 0.7482325434684753\n",
      "epoch:8, batch: 539,  loss: 1.0213227272033691\n",
      "epoch:8, batch: 540,  loss: 1.1685504913330078\n",
      "epoch:8, batch: 541,  loss: 0.8964287638664246\n",
      "epoch:8, batch: 542,  loss: 1.0001763105392456\n",
      "epoch:8, batch: 543,  loss: 0.8057131171226501\n",
      "epoch:8, batch: 544,  loss: 1.0941163301467896\n",
      "epoch:8, batch: 545,  loss: 1.1742782592773438\n",
      "epoch:8, batch: 546,  loss: 0.9613639116287231\n",
      "epoch:8, batch: 547,  loss: 0.9564723968505859\n",
      "epoch:8, batch: 548,  loss: 0.8736609816551208\n",
      "epoch:8, batch: 549,  loss: 0.9237926006317139\n",
      "epoch:8, batch: 550,  loss: 1.1611981391906738\n",
      "epoch:8, batch: 551,  loss: 1.1004610061645508\n",
      "epoch:8, batch: 552,  loss: 0.9966352581977844\n",
      "epoch:8, batch: 553,  loss: 1.197090744972229\n",
      "epoch:8, batch: 554,  loss: 0.9956693649291992\n",
      "epoch:8, batch: 555,  loss: 0.9714750647544861\n",
      "epoch:8, batch: 556,  loss: 0.8856571316719055\n",
      "epoch:8, batch: 557,  loss: 0.8797352313995361\n",
      "epoch:8, batch: 558,  loss: 1.0005934238433838\n",
      "epoch:8, batch: 559,  loss: 1.4386508464813232\n",
      "epoch:8, batch: 560,  loss: 1.1293479204177856\n",
      "epoch:8, batch: 561,  loss: 1.0750497579574585\n",
      "epoch:8, batch: 562,  loss: 1.049679160118103\n",
      "epoch:8, batch: 563,  loss: 1.2143350839614868\n",
      "epoch:8, batch: 564,  loss: 0.9177752137184143\n",
      "epoch:8, batch: 565,  loss: 0.975227415561676\n",
      "epoch:8, batch: 566,  loss: 1.123895287513733\n",
      "epoch:8, batch: 567,  loss: 0.9989485740661621\n",
      "epoch:8, batch: 568,  loss: 1.0620863437652588\n",
      "epoch:8, batch: 569,  loss: 0.8995633721351624\n",
      "epoch:8, batch: 570,  loss: 0.9557462930679321\n",
      "epoch:8, batch: 571,  loss: 1.32659912109375\n",
      "epoch:8, batch: 572,  loss: 1.1248345375061035\n",
      "epoch:8, batch: 573,  loss: 1.0086071491241455\n",
      "epoch:8, batch: 574,  loss: 0.9563334584236145\n",
      "epoch:8, batch: 575,  loss: 0.8205502033233643\n",
      "epoch:8, batch: 576,  loss: 0.9737275838851929\n",
      "epoch:8, batch: 577,  loss: 1.045759916305542\n",
      "epoch:8, batch: 578,  loss: 1.115763783454895\n",
      "epoch:8, batch: 579,  loss: 0.9308222532272339\n",
      "epoch:8, batch: 580,  loss: 1.2144172191619873\n",
      "epoch:8, batch: 581,  loss: 1.1773136854171753\n",
      "epoch:8, batch: 582,  loss: 0.9574734568595886\n",
      "epoch:8, batch: 583,  loss: 0.6593446135520935\n",
      "epoch:8, batch: 584,  loss: 1.2589912414550781\n",
      "epoch:8, batch: 585,  loss: 1.1215531826019287\n",
      "epoch:8, batch: 586,  loss: 0.9698977470397949\n",
      "epoch:8, batch: 587,  loss: 1.1461756229400635\n",
      "epoch:8, batch: 588,  loss: 1.242629051208496\n",
      "epoch:8, batch: 589,  loss: 1.2795923948287964\n",
      "epoch:8, batch: 590,  loss: 0.9809970855712891\n",
      "epoch:8, batch: 591,  loss: 0.8435476422309875\n",
      "epoch:8, batch: 592,  loss: 1.1366641521453857\n",
      "epoch:8, batch: 593,  loss: 1.0096266269683838\n",
      "epoch:8, batch: 594,  loss: 1.1805001497268677\n",
      "epoch:8, batch: 595,  loss: 0.9578874111175537\n",
      "epoch:8, batch: 596,  loss: 0.9946890473365784\n",
      "epoch:8, batch: 597,  loss: 1.0593905448913574\n",
      "epoch:8, batch: 598,  loss: 1.0251225233078003\n",
      "epoch:8, batch: 599,  loss: 0.9325231313705444\n",
      "epoch:8, batch: 600,  loss: 1.0059391260147095\n",
      "epoch:8, batch: 601,  loss: 1.2028075456619263\n",
      "epoch:8, batch: 602,  loss: 0.9334087371826172\n",
      "epoch:8, batch: 603,  loss: 1.2726713418960571\n",
      "epoch:8, batch: 604,  loss: 1.0165382623672485\n",
      "epoch:8, batch: 605,  loss: 1.33711838722229\n",
      "epoch:8, batch: 606,  loss: 1.322840690612793\n",
      "epoch:8, batch: 607,  loss: 1.1166104078292847\n",
      "epoch:8, batch: 608,  loss: 0.968784511089325\n",
      "epoch:8, batch: 609,  loss: 0.9025698900222778\n",
      "epoch:8, batch: 610,  loss: 0.9625239372253418\n",
      "epoch:8, batch: 611,  loss: 1.2588684558868408\n",
      "epoch:8, batch: 612,  loss: 0.9662871956825256\n",
      "epoch:8, batch: 613,  loss: 0.9620670676231384\n",
      "epoch:8, batch: 614,  loss: 1.0554605722427368\n",
      "epoch:8, batch: 615,  loss: 0.9387019872665405\n",
      "epoch:8, batch: 616,  loss: 0.9048743844032288\n",
      "epoch:8, batch: 617,  loss: 1.5000059604644775\n",
      "epoch:8, batch: 618,  loss: 1.1017483472824097\n",
      "epoch:8, batch: 619,  loss: 0.9503756761550903\n",
      "epoch:8, batch: 620,  loss: 1.1721190214157104\n",
      "epoch:8, batch: 621,  loss: 0.931472659111023\n",
      "epoch:8, batch: 622,  loss: 0.9137049913406372\n",
      "epoch:8, batch: 623,  loss: 0.9423229098320007\n",
      "epoch:8, batch: 624,  loss: 1.3299823999404907\n",
      "epoch:8, batch: 625,  loss: 0.9282184839248657\n",
      "epoch:8, batch: 626,  loss: 1.064560055732727\n",
      "epoch:8, batch: 627,  loss: 0.8081569671630859\n",
      "epoch:8, batch: 628,  loss: 1.0312875509262085\n",
      "epoch:8, batch: 629,  loss: 0.9847121834754944\n",
      "epoch:8, batch: 630,  loss: 1.0160503387451172\n",
      "epoch:8, batch: 631,  loss: 1.0884138345718384\n",
      "epoch:8, batch: 632,  loss: 1.1296439170837402\n",
      "epoch:8, batch: 633,  loss: 0.9326454401016235\n",
      "epoch:8, batch: 634,  loss: 1.3361997604370117\n",
      "epoch:8, batch: 635,  loss: 0.947149932384491\n",
      "epoch:8, batch: 636,  loss: 1.1275063753128052\n",
      "epoch:8, batch: 637,  loss: 1.2737083435058594\n",
      "epoch:8, batch: 638,  loss: 0.8518664240837097\n",
      "epoch:8, batch: 639,  loss: 1.1539981365203857\n",
      "epoch:8, batch: 640,  loss: 0.9417035579681396\n",
      "epoch:8, batch: 641,  loss: 1.0714333057403564\n",
      "epoch:8, batch: 642,  loss: 0.9756413102149963\n",
      "epoch:8, batch: 643,  loss: 1.1595734357833862\n",
      "epoch:8, batch: 644,  loss: 1.1139397621154785\n",
      "epoch:8, batch: 645,  loss: 1.0801186561584473\n",
      "epoch:8, batch: 646,  loss: 1.015843152999878\n",
      "epoch:8, batch: 647,  loss: 0.8217668533325195\n",
      "epoch:8, batch: 648,  loss: 0.991326630115509\n",
      "epoch:8, batch: 649,  loss: 1.062302589416504\n",
      "epoch:8, batch: 650,  loss: 1.2884012460708618\n",
      "epoch:8, batch: 651,  loss: 0.9116280674934387\n",
      "epoch:8, batch: 652,  loss: 0.9159153699874878\n",
      "epoch:8, batch: 653,  loss: 1.154810905456543\n",
      "epoch:8, batch: 654,  loss: 1.3049379587173462\n",
      "epoch:8, batch: 655,  loss: 0.8595365285873413\n",
      "epoch:8, batch: 656,  loss: 0.8421277403831482\n",
      "epoch:8, batch: 657,  loss: 1.0969239473342896\n",
      "epoch:8, batch: 658,  loss: 1.1016206741333008\n",
      "epoch:8, batch: 659,  loss: 0.9957449436187744\n",
      "epoch:8, batch: 660,  loss: 1.2712340354919434\n",
      "epoch:8, batch: 661,  loss: 1.1870005130767822\n",
      "epoch:8, batch: 662,  loss: 0.9376282095909119\n",
      "epoch:8, batch: 663,  loss: 1.4372937679290771\n",
      "epoch:8, batch: 664,  loss: 1.2029682397842407\n",
      "epoch:8, batch: 665,  loss: 1.0790067911148071\n",
      "epoch:8, batch: 666,  loss: 1.3550490140914917\n",
      "epoch:8, batch: 667,  loss: 1.087390661239624\n",
      "epoch:8, batch: 668,  loss: 1.167000651359558\n",
      "epoch:8, batch: 669,  loss: 1.2232089042663574\n",
      "epoch:8, batch: 670,  loss: 0.8285043239593506\n",
      "epoch:8, batch: 671,  loss: 1.4416313171386719\n",
      "epoch:8, batch: 672,  loss: 1.0274262428283691\n",
      "epoch:8, batch: 673,  loss: 1.0110021829605103\n",
      "epoch:8, batch: 674,  loss: 0.9612365365028381\n",
      "epoch:8, batch: 675,  loss: 1.326621651649475\n",
      "epoch:8, batch: 676,  loss: 1.0740017890930176\n",
      "epoch:8, batch: 677,  loss: 1.1183356046676636\n",
      "epoch:8, batch: 678,  loss: 1.0742077827453613\n",
      "epoch:8, batch: 679,  loss: 0.8859257102012634\n",
      "epoch:8, batch: 680,  loss: 1.1871662139892578\n",
      "epoch:8, batch: 681,  loss: 1.0312821865081787\n",
      "epoch:8, batch: 682,  loss: 1.3696976900100708\n",
      "epoch:8, batch: 683,  loss: 1.1858068704605103\n",
      "epoch:8, batch: 684,  loss: 1.270180344581604\n",
      "epoch:8, batch: 685,  loss: 1.1222859621047974\n",
      "epoch:8, batch: 686,  loss: 0.8684035539627075\n",
      "epoch:8, batch: 687,  loss: 1.0807276964187622\n",
      "epoch:8, batch: 688,  loss: 1.3158921003341675\n",
      "epoch:8, batch: 689,  loss: 0.9328484535217285\n",
      "epoch:8, batch: 690,  loss: 0.9894434809684753\n",
      "epoch:8, batch: 691,  loss: 1.4412217140197754\n",
      "epoch:8, batch: 692,  loss: 1.0796916484832764\n",
      "epoch:8, batch: 693,  loss: 1.4596977233886719\n",
      "epoch:8, batch: 694,  loss: 1.219784140586853\n",
      "epoch:8, batch: 695,  loss: 1.323809027671814\n",
      "epoch:8, batch: 696,  loss: 1.2784978151321411\n",
      "epoch:8, batch: 697,  loss: 0.8982239365577698\n",
      "epoch:8, batch: 698,  loss: 1.1151973009109497\n",
      "epoch:8, batch: 699,  loss: 0.8373288512229919\n",
      "epoch:8, batch: 700,  loss: 0.9779566526412964\n",
      "epoch:8, batch: 701,  loss: 0.8691434264183044\n",
      "epoch:8, batch: 702,  loss: 1.1305309534072876\n",
      "epoch:8, batch: 703,  loss: 1.2044597864151\n",
      "epoch:8, batch: 704,  loss: 1.0736011266708374\n",
      "epoch:8, batch: 705,  loss: 0.9639586806297302\n",
      "epoch:8, batch: 706,  loss: 0.7754648327827454\n",
      "epoch:8, batch: 707,  loss: 0.9022268056869507\n",
      "epoch:8, batch: 708,  loss: 0.747840404510498\n",
      "epoch:8, batch: 709,  loss: 0.9253331422805786\n",
      "epoch:8, batch: 710,  loss: 1.165610909461975\n",
      "epoch:8, batch: 711,  loss: 1.2580196857452393\n",
      "epoch:8, batch: 712,  loss: 1.0434024333953857\n",
      "epoch:8, batch: 713,  loss: 1.6149568557739258\n",
      "epoch:8, batch: 714,  loss: 0.6624908447265625\n",
      "epoch:8, batch: 715,  loss: 1.032286286354065\n",
      "epoch:8, batch: 716,  loss: 1.1550735235214233\n",
      "epoch:8, batch: 717,  loss: 0.927652895450592\n",
      "epoch:8, batch: 718,  loss: 1.078352689743042\n",
      "epoch:8, batch: 719,  loss: 1.2308498620986938\n",
      "epoch:8, batch: 720,  loss: 1.1813105344772339\n",
      "epoch:8, batch: 721,  loss: 0.9335945248603821\n",
      "epoch:8, batch: 722,  loss: 1.0389440059661865\n",
      "epoch:8, batch: 723,  loss: 0.731894850730896\n",
      "epoch:8, batch: 724,  loss: 1.312426209449768\n",
      "epoch:8, batch: 725,  loss: 1.0674258470535278\n",
      "epoch:8, batch: 726,  loss: 0.9332985877990723\n",
      "epoch:8, batch: 727,  loss: 0.6064521670341492\n",
      "epoch:8, batch: 728,  loss: 1.409216046333313\n",
      "epoch:8, batch: 729,  loss: 0.8017373085021973\n",
      "epoch:8, batch: 730,  loss: 1.3524093627929688\n",
      "epoch:8, batch: 731,  loss: 0.9844567775726318\n",
      "epoch:8, batch: 732,  loss: 1.0815343856811523\n",
      "epoch:8, batch: 733,  loss: 1.1799582242965698\n",
      "epoch:8, batch: 734,  loss: 0.9696575999259949\n",
      "epoch:8, batch: 735,  loss: 1.225111484527588\n",
      "epoch:8, batch: 736,  loss: 0.9323869943618774\n",
      "epoch:8, batch: 737,  loss: 0.9968597888946533\n",
      "epoch:8, batch: 738,  loss: 1.1881219148635864\n",
      "epoch:8, batch: 739,  loss: 1.083322525024414\n",
      "epoch:8, batch: 740,  loss: 1.155287504196167\n",
      "epoch:8, batch: 741,  loss: 0.8663718700408936\n",
      "epoch:8, batch: 742,  loss: 1.1755932569503784\n",
      "epoch:8, batch: 743,  loss: 1.1561745405197144\n",
      "epoch:8, batch: 744,  loss: 0.9889615178108215\n",
      "epoch:8, batch: 745,  loss: 0.9491856694221497\n",
      "epoch:8, batch: 746,  loss: 1.1470354795455933\n",
      "epoch:8, batch: 747,  loss: 1.1267106533050537\n",
      "epoch:8, batch: 748,  loss: 1.1669673919677734\n",
      "epoch:8, batch: 749,  loss: 0.8863759636878967\n",
      "epoch:8, batch: 750,  loss: 1.0867952108383179\n",
      "epoch:8, batch: 751,  loss: 1.140464425086975\n",
      "epoch:8, batch: 752,  loss: 1.3806782960891724\n",
      "epoch:8, batch: 753,  loss: 0.9819974899291992\n",
      "epoch:8, batch: 754,  loss: 1.0239832401275635\n",
      "epoch:8, batch: 755,  loss: 0.9795013666152954\n",
      "epoch:8, batch: 756,  loss: 0.7578569650650024\n",
      "epoch:8, batch: 757,  loss: 0.9308267831802368\n",
      "epoch:8, batch: 758,  loss: 1.0878546237945557\n",
      "epoch:8, batch: 759,  loss: 1.2174394130706787\n",
      "epoch:8, batch: 760,  loss: 1.144460916519165\n",
      "epoch:8, batch: 761,  loss: 0.9266006350517273\n",
      "epoch:8, batch: 762,  loss: 0.9566712379455566\n",
      "epoch:8, batch: 763,  loss: 0.7641401290893555\n",
      "epoch:8, batch: 764,  loss: 1.3934125900268555\n",
      "epoch:8, batch: 765,  loss: 1.1682432889938354\n",
      "epoch:8, batch: 766,  loss: 0.8120632767677307\n",
      "epoch:8, batch: 767,  loss: 0.8882550001144409\n",
      "epoch:8, batch: 768,  loss: 1.4748222827911377\n",
      "epoch:8, batch: 769,  loss: 1.034828782081604\n",
      "epoch:8, batch: 770,  loss: 1.2550123929977417\n",
      "epoch:8, batch: 771,  loss: 1.296818494796753\n",
      "epoch:8, batch: 772,  loss: 1.1576969623565674\n",
      "epoch:8, batch: 773,  loss: 1.0314968824386597\n",
      "epoch:8, batch: 774,  loss: 1.1302258968353271\n",
      "epoch:8, batch: 775,  loss: 1.2234536409378052\n",
      "epoch:8, batch: 776,  loss: 1.0102930068969727\n",
      "epoch:8, batch: 777,  loss: 0.8012198209762573\n",
      "epoch:8, batch: 778,  loss: 1.1336565017700195\n",
      "epoch:8, batch: 779,  loss: 1.2055243253707886\n",
      "epoch:8, batch: 780,  loss: 0.8273510336875916\n",
      "epoch:8, batch: 781,  loss: 1.077492356300354\n",
      "epoch:8, batch: 782,  loss: 1.1069453954696655\n",
      "epoch:8, batch: 783,  loss: 1.0384396314620972\n",
      "epoch:8, batch: 784,  loss: 1.006218433380127\n",
      "epoch:8, batch: 785,  loss: 0.9343183636665344\n",
      "epoch:8, batch: 786,  loss: 1.0005338191986084\n",
      "epoch:8, batch: 787,  loss: 1.3018434047698975\n",
      "epoch:8, batch: 788,  loss: 0.7720763683319092\n",
      "epoch:8, batch: 789,  loss: 0.9155982136726379\n",
      "epoch:8, batch: 790,  loss: 0.9629784226417542\n",
      "epoch:8, batch: 791,  loss: 1.2436195611953735\n",
      "epoch:8, batch: 792,  loss: 1.0351848602294922\n",
      "epoch:8, batch: 793,  loss: 0.9424203038215637\n",
      "epoch:8, batch: 794,  loss: 1.2150413990020752\n",
      "epoch:8, batch: 795,  loss: 1.4112234115600586\n",
      "epoch:8, batch: 796,  loss: 0.8923330307006836\n",
      "epoch:8, batch: 797,  loss: 1.0428686141967773\n",
      "epoch:8, batch: 798,  loss: 0.746829628944397\n",
      "epoch:8, batch: 799,  loss: 0.9317231774330139\n",
      "epoch:8, batch: 800,  loss: 1.003574013710022\n",
      "epoch:8, batch: 801,  loss: 1.0645604133605957\n",
      "epoch:8, batch: 802,  loss: 0.9357385039329529\n",
      "epoch:8, batch: 803,  loss: 0.7507101893424988\n",
      "epoch:8, batch: 804,  loss: 1.0255980491638184\n",
      "epoch:8, batch: 805,  loss: 1.019195795059204\n",
      "epoch:8, batch: 806,  loss: 0.7591540813446045\n",
      "epoch:8, batch: 807,  loss: 0.9772289991378784\n",
      "epoch:8, batch: 808,  loss: 0.8579312562942505\n",
      "epoch:8, batch: 809,  loss: 1.403725028038025\n",
      "epoch:8, batch: 810,  loss: 1.102299451828003\n",
      "epoch:8, batch: 811,  loss: 0.8437584638595581\n",
      "epoch:8, batch: 812,  loss: 0.953973650932312\n",
      "epoch:8, batch: 813,  loss: 1.1582063436508179\n",
      "epoch:8, batch: 814,  loss: 1.0539833307266235\n",
      "epoch:8, batch: 815,  loss: 0.9251534938812256\n",
      "epoch:8, batch: 816,  loss: 1.4154019355773926\n",
      "epoch:8, batch: 817,  loss: 0.9765164852142334\n",
      "epoch:8, batch: 818,  loss: 0.9867064952850342\n",
      "epoch:8, batch: 819,  loss: 1.119696021080017\n",
      "epoch:8, batch: 820,  loss: 1.0393728017807007\n",
      "epoch:8, batch: 821,  loss: 0.8803583383560181\n",
      "epoch:8, batch: 822,  loss: 0.9006590843200684\n",
      "epoch:8, batch: 823,  loss: 1.0212785005569458\n",
      "epoch:8, batch: 824,  loss: 0.86942458152771\n",
      "epoch:8, batch: 825,  loss: 0.9311243295669556\n",
      "epoch:8, batch: 826,  loss: 1.120211124420166\n",
      "epoch:8, batch: 827,  loss: 1.146043300628662\n",
      "epoch:8, batch: 828,  loss: 0.8271185159683228\n",
      "epoch:8, batch: 829,  loss: 1.0344425439834595\n",
      "epoch:8, batch: 830,  loss: 0.9630705118179321\n",
      "epoch:8, batch: 831,  loss: 1.2149876356124878\n",
      "epoch:8, batch: 832,  loss: 1.0118707418441772\n",
      "epoch:8, batch: 833,  loss: 1.2092479467391968\n",
      "epoch:8, batch: 834,  loss: 1.1557021141052246\n",
      "epoch:8, batch: 835,  loss: 1.0695184469223022\n",
      "epoch:8, batch: 836,  loss: 1.0060595273971558\n",
      "epoch:8, batch: 837,  loss: 1.1510146856307983\n",
      "epoch:8, batch: 838,  loss: 1.0927740335464478\n",
      "epoch:8, batch: 839,  loss: 0.9256335496902466\n",
      "epoch:8, batch: 840,  loss: 1.1135021448135376\n",
      "epoch:8, batch: 841,  loss: 1.049793004989624\n",
      "epoch:8, batch: 842,  loss: 1.0751484632492065\n",
      "epoch:8, batch: 843,  loss: 1.1922935247421265\n",
      "epoch:8, batch: 844,  loss: 1.0233255624771118\n",
      "epoch:8, batch: 845,  loss: 0.8065463304519653\n",
      "epoch:8, batch: 846,  loss: 1.1794958114624023\n",
      "epoch:8, batch: 847,  loss: 1.274722933769226\n",
      "epoch:8, batch: 848,  loss: 1.389194369316101\n",
      "epoch:8, batch: 849,  loss: 1.062874674797058\n",
      "epoch:8, batch: 850,  loss: 1.2424769401550293\n",
      "epoch:8, batch: 851,  loss: 1.052456021308899\n",
      "epoch:8, batch: 852,  loss: 1.122553825378418\n",
      "epoch:8, batch: 853,  loss: 0.9359491467475891\n",
      "epoch:8, batch: 854,  loss: 1.237741231918335\n",
      "epoch:8, batch: 855,  loss: 0.894123911857605\n",
      "epoch:8, batch: 856,  loss: 0.9117032289505005\n",
      "epoch:8, batch: 857,  loss: 0.5472133755683899\n",
      "epoch:8, batch: 858,  loss: 1.029443383216858\n",
      "epoch:8, batch: 859,  loss: 1.3121986389160156\n",
      "epoch:8, batch: 860,  loss: 0.8738657832145691\n",
      "epoch:8, batch: 861,  loss: 1.2670292854309082\n",
      "epoch:8, batch: 862,  loss: 1.1872472763061523\n",
      "epoch:8, batch: 863,  loss: 0.9019930362701416\n",
      "epoch:8, batch: 864,  loss: 0.8682791590690613\n",
      "epoch:8, batch: 865,  loss: 0.9238324761390686\n",
      "epoch:8, batch: 866,  loss: 1.0215331315994263\n",
      "epoch:8, batch: 867,  loss: 1.2163432836532593\n",
      "epoch:8, batch: 868,  loss: 0.8814773559570312\n",
      "epoch:8, batch: 869,  loss: 0.8415380120277405\n",
      "epoch:8, batch: 870,  loss: 1.1923352479934692\n",
      "epoch:8, batch: 871,  loss: 1.3054802417755127\n",
      "epoch:8, batch: 872,  loss: 1.5616718530654907\n",
      "epoch:8, batch: 873,  loss: 0.826440691947937\n",
      "epoch:8, batch: 874,  loss: 0.9485445022583008\n",
      "epoch:8, batch: 875,  loss: 1.0788626670837402\n",
      "epoch:8, batch: 876,  loss: 1.0660370588302612\n",
      "epoch:8, batch: 877,  loss: 0.9262303113937378\n",
      "epoch:8, batch: 878,  loss: 0.9435961842536926\n",
      "epoch:8, batch: 879,  loss: 1.4926502704620361\n",
      "epoch:8, batch: 880,  loss: 1.268385648727417\n",
      "epoch:8, batch: 881,  loss: 0.8120831251144409\n",
      "epoch:8, batch: 882,  loss: 0.9264559149742126\n",
      "epoch:8, batch: 883,  loss: 0.7542992234230042\n",
      "epoch:8, batch: 884,  loss: 0.7894142866134644\n",
      "epoch:8, batch: 885,  loss: 1.0707999467849731\n",
      "epoch:8, batch: 886,  loss: 1.0424962043762207\n",
      "epoch:8, batch: 887,  loss: 0.9434565901756287\n",
      "epoch:8, batch: 888,  loss: 1.2670420408248901\n",
      "epoch:8, batch: 889,  loss: 1.1500121355056763\n",
      "epoch:8, batch: 890,  loss: 0.924659788608551\n",
      "epoch:8, batch: 891,  loss: 0.9330101609230042\n",
      "epoch:8, batch: 892,  loss: 1.3120185136795044\n",
      "epoch:8, batch: 893,  loss: 1.0545860528945923\n",
      "epoch:8, batch: 894,  loss: 0.9702231884002686\n",
      "epoch:8, batch: 895,  loss: 0.8655964136123657\n",
      "epoch:8, batch: 896,  loss: 0.9337590336799622\n",
      "epoch:8, batch: 897,  loss: 1.0265015363693237\n",
      "epoch:8, batch: 898,  loss: 1.1845183372497559\n",
      "epoch:8, batch: 899,  loss: 1.4686359167099\n",
      "epoch:8, batch: 900,  loss: 0.7623429298400879\n",
      "epoch:8, batch: 901,  loss: 1.3021690845489502\n",
      "epoch:8, batch: 902,  loss: 0.9181957244873047\n",
      "epoch:8, batch: 903,  loss: 1.0799167156219482\n",
      "epoch:8, batch: 904,  loss: 0.8915294408798218\n",
      "epoch:8, batch: 905,  loss: 1.2931615114212036\n",
      "epoch:8, batch: 906,  loss: 0.8847365975379944\n",
      "epoch:8, batch: 907,  loss: 0.9943227171897888\n",
      "epoch:8, batch: 908,  loss: 1.2460218667984009\n",
      "epoch:8, batch: 909,  loss: 0.9634349346160889\n",
      "epoch:8, batch: 910,  loss: 1.2768526077270508\n",
      "epoch:8, batch: 911,  loss: 0.847955584526062\n",
      "epoch:8, batch: 912,  loss: 0.8493386507034302\n",
      "epoch:8, batch: 913,  loss: 1.1149219274520874\n",
      "epoch:8, batch: 914,  loss: 1.0303966999053955\n",
      "epoch:8, batch: 915,  loss: 1.2648067474365234\n",
      "epoch:8, batch: 916,  loss: 1.1680742502212524\n",
      "epoch:8, batch: 917,  loss: 1.0563865900039673\n",
      "epoch:8, batch: 918,  loss: 1.2767643928527832\n",
      "epoch:8, batch: 919,  loss: 1.0664013624191284\n",
      "epoch:8, batch: 920,  loss: 0.918237030506134\n",
      "epoch:8, batch: 921,  loss: 1.2188549041748047\n",
      "epoch:8, batch: 922,  loss: 0.9354656934738159\n",
      "epoch:8, batch: 923,  loss: 1.290602684020996\n",
      "epoch:8, batch: 924,  loss: 1.0822336673736572\n",
      "epoch:8, batch: 925,  loss: 0.9395029544830322\n",
      "epoch:8, batch: 926,  loss: 1.1168681383132935\n",
      "epoch:8, batch: 927,  loss: 1.19545578956604\n",
      "epoch:8, batch: 928,  loss: 0.8135462403297424\n",
      "epoch:8, batch: 929,  loss: 0.9008467197418213\n",
      "epoch:8, batch: 930,  loss: 0.900511622428894\n",
      "epoch:8, batch: 931,  loss: 1.1215318441390991\n",
      "epoch:8, batch: 932,  loss: 0.9883081912994385\n",
      "epoch:8, batch: 933,  loss: 1.3034074306488037\n",
      "epoch:8, batch: 934,  loss: 1.1456432342529297\n",
      "epoch:8, batch: 935,  loss: 0.9059465527534485\n",
      "epoch:8, batch: 936,  loss: 0.9902977347373962\n",
      "epoch:8, batch: 937,  loss: 1.1676702499389648\n",
      "epoch:8, batch: 938,  loss: 1.2896604537963867\n",
      "epoch:8, batch: 939,  loss: 0.9873858690261841\n",
      "epoch:8, batch: 940,  loss: 1.09469735622406\n",
      "epoch:8, batch: 941,  loss: 0.893977165222168\n",
      "epoch:8, batch: 942,  loss: 0.9933004379272461\n",
      "epoch:8, batch: 943,  loss: 1.0633221864700317\n",
      "epoch:8, batch: 944,  loss: 1.0938105583190918\n",
      "epoch:8, batch: 945,  loss: 1.0747902393341064\n",
      "epoch:8, batch: 946,  loss: 0.913432776927948\n",
      "epoch:8, batch: 947,  loss: 0.9008944630622864\n",
      "epoch:8, batch: 948,  loss: 1.234607219696045\n",
      "epoch:8, batch: 949,  loss: 0.9419531226158142\n",
      "epoch:8, batch: 950,  loss: 0.9956864714622498\n",
      "epoch:8, batch: 951,  loss: 1.337563157081604\n",
      "epoch:8, batch: 952,  loss: 1.0126652717590332\n",
      "epoch:8, batch: 953,  loss: 0.7935478687286377\n",
      "epoch:8, batch: 954,  loss: 1.43173086643219\n",
      "epoch:8, batch: 955,  loss: 1.200352430343628\n",
      "epoch:8, batch: 956,  loss: 1.4103260040283203\n",
      "epoch:8, batch: 957,  loss: 1.425557255744934\n",
      "epoch:8, batch: 958,  loss: 1.047641396522522\n",
      "epoch:8, batch: 959,  loss: 1.041535496711731\n",
      "epoch:8, batch: 960,  loss: 0.9285025000572205\n",
      "epoch:8, batch: 961,  loss: 1.3667460680007935\n",
      "epoch:8, batch: 962,  loss: 1.3289356231689453\n",
      "epoch:8, batch: 963,  loss: 1.0210922956466675\n",
      "epoch:8, batch: 964,  loss: 1.217437744140625\n",
      "epoch:8, batch: 965,  loss: 0.7374276518821716\n",
      "epoch:8, batch: 966,  loss: 0.8720666170120239\n",
      "epoch:8, batch: 967,  loss: 1.006384253501892\n",
      "epoch:8, batch: 968,  loss: 0.9165802597999573\n",
      "epoch:8, batch: 969,  loss: 1.1030012369155884\n",
      "epoch:8, batch: 970,  loss: 1.1217349767684937\n",
      "epoch:8, batch: 971,  loss: 1.364340901374817\n",
      "epoch:8, batch: 972,  loss: 1.3914467096328735\n",
      "epoch:8, batch: 973,  loss: 1.0691410303115845\n",
      "epoch:8, batch: 974,  loss: 0.9441331624984741\n",
      "epoch:8, batch: 975,  loss: 1.0198554992675781\n",
      "epoch:8, batch: 976,  loss: 1.0419844388961792\n",
      "epoch:8, batch: 977,  loss: 1.260268211364746\n",
      "epoch:8, batch: 978,  loss: 0.9519100189208984\n",
      "epoch:8, batch: 979,  loss: 1.1482537984848022\n",
      "epoch:8, batch: 980,  loss: 0.811573326587677\n",
      "epoch:8, batch: 981,  loss: 1.332653522491455\n",
      "epoch:8, batch: 982,  loss: 1.1327568292617798\n",
      "epoch:8, batch: 983,  loss: 1.4076610803604126\n",
      "epoch:8, batch: 984,  loss: 0.9955124258995056\n",
      "epoch:8, batch: 985,  loss: 0.889814555644989\n",
      "epoch:8, batch: 986,  loss: 1.0594720840454102\n",
      "epoch:8, batch: 987,  loss: 0.9746854901313782\n",
      "epoch:8, batch: 988,  loss: 1.0624912977218628\n",
      "epoch:8, batch: 989,  loss: 0.9712553024291992\n",
      "epoch:8, batch: 990,  loss: 1.155742883682251\n",
      "epoch:8, batch: 991,  loss: 1.1964277029037476\n",
      "epoch:8, batch: 992,  loss: 0.9796180725097656\n",
      "epoch:8, batch: 993,  loss: 1.5879851579666138\n",
      "epoch:8, batch: 994,  loss: 0.9714956283569336\n",
      "epoch:8, batch: 995,  loss: 0.974867582321167\n",
      "epoch:8, batch: 996,  loss: 0.8573301434516907\n",
      "epoch:8, batch: 997,  loss: 0.9630873203277588\n",
      "epoch:8, batch: 998,  loss: 0.7066720128059387\n",
      "epoch:8, batch: 999,  loss: 1.1688916683197021\n",
      "epoch:8, batch: 1000,  loss: 1.0339542627334595\n",
      "epoch:8, batch: 1001,  loss: 1.1331238746643066\n",
      "epoch:8, batch: 1002,  loss: 1.1157914400100708\n",
      "epoch:8, batch: 1003,  loss: 1.0153945684432983\n",
      "epoch:8, batch: 1004,  loss: 0.9809808731079102\n",
      "epoch:8, batch: 1005,  loss: 1.0577869415283203\n",
      "epoch:8, batch: 1006,  loss: 1.0099167823791504\n",
      "epoch:8, batch: 1007,  loss: 0.8065332174301147\n",
      "epoch:8, batch: 1008,  loss: 1.3712466955184937\n",
      "epoch:8, batch: 1009,  loss: 0.9875087738037109\n",
      "epoch:8, batch: 1010,  loss: 0.894318163394928\n",
      "epoch:8, batch: 1011,  loss: 1.441197156906128\n",
      "epoch:8, batch: 1012,  loss: 0.7900146842002869\n",
      "epoch:8, batch: 1013,  loss: 1.0255457162857056\n",
      "epoch:8, batch: 1014,  loss: 1.0479360818862915\n",
      "epoch:8, batch: 1015,  loss: 0.9288755059242249\n",
      "epoch:8, batch: 1016,  loss: 1.1910667419433594\n",
      "epoch:8, batch: 1017,  loss: 1.1530823707580566\n",
      "epoch:8, batch: 1018,  loss: 1.2833170890808105\n",
      "epoch:8, batch: 1019,  loss: 0.8485125303268433\n",
      "epoch:8, batch: 1020,  loss: 0.7626004219055176\n",
      "epoch:8, batch: 1021,  loss: 1.1609410047531128\n",
      "epoch:8, batch: 1022,  loss: 0.7576209306716919\n",
      "epoch:8, batch: 1023,  loss: 1.088356852531433\n",
      "epoch:8, batch: 1024,  loss: 0.9494673013687134\n",
      "epoch:8, batch: 1025,  loss: 1.08843994140625\n",
      "epoch:8, batch: 1026,  loss: 0.8681657314300537\n",
      "epoch:8, batch: 1027,  loss: 1.0443254709243774\n",
      "epoch:8, batch: 1028,  loss: 0.9592650532722473\n",
      "epoch:8, batch: 1029,  loss: 1.2334309816360474\n",
      "epoch:8, batch: 1030,  loss: 1.0728449821472168\n",
      "epoch:8, batch: 1031,  loss: 1.0409458875656128\n",
      "epoch:8, batch: 1032,  loss: 1.1306284666061401\n",
      "epoch:8, batch: 1033,  loss: 1.1005827188491821\n",
      "epoch:8, batch: 1034,  loss: 1.1426408290863037\n",
      "epoch:8, batch: 1035,  loss: 0.9764297008514404\n",
      "epoch:8, batch: 1036,  loss: 0.9609822034835815\n",
      "epoch:8, batch: 1037,  loss: 1.5730324983596802\n",
      "epoch:8, batch: 1038,  loss: 1.0399248600006104\n",
      "epoch:8, batch: 1039,  loss: 1.1256622076034546\n",
      "epoch:8, batch: 1040,  loss: 1.0401298999786377\n",
      "epoch:8, batch: 1041,  loss: 0.9159727692604065\n",
      "epoch:8, batch: 1042,  loss: 0.930468738079071\n",
      "epoch:8, batch: 1043,  loss: 1.1559752225875854\n",
      "epoch:8, batch: 1044,  loss: 1.225659966468811\n",
      "epoch:8, batch: 1045,  loss: 1.364427089691162\n",
      "epoch:8, batch: 1046,  loss: 0.7131876945495605\n",
      "epoch:8, batch: 1047,  loss: 1.005147933959961\n",
      "epoch:8, batch: 1048,  loss: 1.2205129861831665\n",
      "epoch:8, batch: 1049,  loss: 1.154338002204895\n",
      "epoch:8, batch: 1050,  loss: 1.0243451595306396\n",
      "epoch:8, batch: 1051,  loss: 1.1218178272247314\n",
      "epoch:8, batch: 1052,  loss: 1.1457703113555908\n",
      "epoch:8, batch: 1053,  loss: 1.3754287958145142\n",
      "epoch:8, batch: 1054,  loss: 1.1526588201522827\n",
      "epoch:8, batch: 1055,  loss: 1.007176160812378\n",
      "epoch:8, batch: 1056,  loss: 0.8522074222564697\n",
      "epoch:8, batch: 1057,  loss: 1.37102210521698\n",
      "epoch:8, batch: 1058,  loss: 0.8933805823326111\n",
      "epoch:8, batch: 1059,  loss: 1.087347149848938\n",
      "epoch:8, batch: 1060,  loss: 0.881028950214386\n",
      "epoch:8, batch: 1061,  loss: 1.110166072845459\n",
      "epoch:8, batch: 1062,  loss: 0.7943345904350281\n",
      "epoch:8, batch: 1063,  loss: 0.9934531450271606\n",
      "epoch:8, batch: 1064,  loss: 0.9237996935844421\n",
      "epoch:8, batch: 1065,  loss: 1.0738749504089355\n",
      "epoch:8, batch: 1066,  loss: 1.4409466981887817\n",
      "epoch:8, batch: 1067,  loss: 1.1714158058166504\n",
      "epoch:8, batch: 1068,  loss: 0.9921441078186035\n",
      "epoch:8, batch: 1069,  loss: 1.0826246738433838\n",
      "epoch:8, batch: 1070,  loss: 0.8987545371055603\n",
      "epoch:8, batch: 1071,  loss: 1.0183864831924438\n",
      "epoch:8, batch: 1072,  loss: 1.2193751335144043\n",
      "epoch:8, batch: 1073,  loss: 0.9169059991836548\n",
      "epoch:8, batch: 1074,  loss: 1.2706072330474854\n",
      "epoch:8, batch: 1075,  loss: 1.1840018033981323\n",
      "epoch:8, batch: 1076,  loss: 0.8852077126502991\n",
      "epoch:8, batch: 1077,  loss: 1.2615464925765991\n",
      "epoch:8, batch: 1078,  loss: 1.0130075216293335\n",
      "epoch:8, batch: 1079,  loss: 1.1150844097137451\n",
      "epoch:8, batch: 1080,  loss: 0.9825032353401184\n",
      "epoch:8, batch: 1081,  loss: 0.8558455109596252\n",
      "epoch:8, batch: 1082,  loss: 1.090965986251831\n",
      "epoch:8, batch: 1083,  loss: 1.2477463483810425\n",
      "epoch:8, batch: 1084,  loss: 0.926706075668335\n",
      "epoch:8, batch: 1085,  loss: 1.0198925733566284\n",
      "epoch:8, batch: 1086,  loss: 1.6527700424194336\n",
      "epoch:8, batch: 1087,  loss: 1.0568463802337646\n",
      "epoch:8, batch: 1088,  loss: 0.9408539533615112\n",
      "epoch:8, batch: 1089,  loss: 1.295210599899292\n",
      "epoch:8, batch: 1090,  loss: 1.0277255773544312\n",
      "epoch:8, batch: 1091,  loss: 1.0450962781906128\n",
      "epoch:8, batch: 1092,  loss: 0.8628668785095215\n",
      "epoch:8, batch: 1093,  loss: 1.0629158020019531\n",
      "epoch:8, batch: 1094,  loss: 1.2629187107086182\n",
      "epoch:8, batch: 1095,  loss: 1.1416436433792114\n",
      "epoch:8, batch: 1096,  loss: 0.9527348279953003\n",
      "epoch:8, batch: 1097,  loss: 1.2878403663635254\n",
      "epoch:8, batch: 1098,  loss: 0.8463963866233826\n",
      "epoch:8, batch: 1099,  loss: 1.293996810913086\n",
      "epoch:8, batch: 1100,  loss: 1.0865446329116821\n",
      "epoch:8, batch: 1101,  loss: 0.817719578742981\n",
      "epoch:8, batch: 1102,  loss: 1.0049387216567993\n",
      "epoch:8, batch: 1103,  loss: 1.2219367027282715\n",
      "epoch:8, batch: 1104,  loss: 1.2156484127044678\n",
      "epoch:8, batch: 1105,  loss: 1.4286880493164062\n",
      "epoch:8, batch: 1106,  loss: 1.031091332435608\n",
      "epoch:8, batch: 1107,  loss: 1.2288053035736084\n",
      "epoch:8, batch: 1108,  loss: 1.366599202156067\n",
      "epoch:8, batch: 1109,  loss: 1.266620397567749\n",
      "epoch:8, batch: 1110,  loss: 1.217527985572815\n",
      "epoch:8, batch: 1111,  loss: 1.1601731777191162\n",
      "epoch:8, batch: 1112,  loss: 0.9543542861938477\n",
      "epoch:8, batch: 1113,  loss: 0.8450373411178589\n",
      "epoch:8, batch: 1114,  loss: 0.933746337890625\n",
      "epoch:8, batch: 1115,  loss: 0.9533113241195679\n",
      "epoch:8, batch: 1116,  loss: 1.090503454208374\n",
      "epoch:8, batch: 1117,  loss: 1.0339173078536987\n",
      "epoch:8, batch: 1118,  loss: 1.022208571434021\n",
      "epoch:8, batch: 1119,  loss: 0.6523656249046326\n",
      "epoch:8, batch: 1120,  loss: 1.1285821199417114\n",
      "epoch:8, batch: 1121,  loss: 0.8355250358581543\n",
      "epoch:8, batch: 1122,  loss: 1.070890188217163\n",
      "epoch:8, batch: 1123,  loss: 0.9005002975463867\n",
      "epoch:8, batch: 1124,  loss: 0.9227426648139954\n",
      "epoch:8, batch: 1125,  loss: 0.982121467590332\n",
      "epoch:8, batch: 1126,  loss: 0.8790529370307922\n",
      "epoch:8, batch: 1127,  loss: 0.7931250929832458\n",
      "epoch:8, batch: 1128,  loss: 0.7259475588798523\n",
      "epoch:8, batch: 1129,  loss: 1.3331698179244995\n",
      "epoch:8, batch: 1130,  loss: 0.9475702047348022\n",
      "epoch:8, batch: 1131,  loss: 0.8946361541748047\n",
      "epoch:8, batch: 1132,  loss: 1.0476723909378052\n",
      "epoch:8, batch: 1133,  loss: 0.8365670442581177\n",
      "epoch:8, batch: 1134,  loss: 1.278662919998169\n",
      "epoch:8, batch: 1135,  loss: 1.1181198358535767\n",
      "epoch:8, batch: 1136,  loss: 0.8523881435394287\n",
      "epoch:8, batch: 1137,  loss: 1.2130237817764282\n",
      "epoch:8, batch: 1138,  loss: 0.9932543039321899\n",
      "epoch:8, batch: 1139,  loss: 0.932802677154541\n",
      "epoch:8, batch: 1140,  loss: 1.1997730731964111\n",
      "epoch:8, batch: 1141,  loss: 1.0270414352416992\n",
      "epoch:8, batch: 1142,  loss: 1.0686155557632446\n",
      "epoch:8, batch: 1143,  loss: 0.9122804403305054\n",
      "epoch:8, batch: 1144,  loss: 1.1689735651016235\n",
      "epoch:8, batch: 1145,  loss: 1.3968316316604614\n",
      "epoch:8, batch: 1146,  loss: 1.0691051483154297\n",
      "epoch:8, batch: 1147,  loss: 1.2202136516571045\n",
      "epoch:8, batch: 1148,  loss: 1.1034622192382812\n",
      "epoch:8, batch: 1149,  loss: 1.051039695739746\n",
      "epoch:8, batch: 1150,  loss: 0.865066647529602\n",
      "epoch:8, batch: 1151,  loss: 1.121754765510559\n",
      "epoch:8, batch: 1152,  loss: 1.3280820846557617\n",
      "epoch:8, batch: 1153,  loss: 1.0644491910934448\n",
      "epoch:8, batch: 1154,  loss: 1.1638849973678589\n",
      "epoch:8, batch: 1155,  loss: 1.2925386428833008\n",
      "epoch:8, batch: 1156,  loss: 1.0308786630630493\n",
      "epoch:8, batch: 1157,  loss: 0.832658052444458\n",
      "epoch:8, batch: 1158,  loss: 1.2401158809661865\n",
      "epoch:8, batch: 1159,  loss: 1.138684868812561\n",
      "epoch:8, batch: 1160,  loss: 0.9536392092704773\n",
      "epoch:8, batch: 1161,  loss: 1.3475130796432495\n",
      "epoch:8, batch: 1162,  loss: 1.0818617343902588\n",
      "epoch:8, batch: 1163,  loss: 0.7298842668533325\n",
      "epoch:8, batch: 1164,  loss: 1.3342617750167847\n",
      "epoch:8, batch: 1165,  loss: 1.1179161071777344\n",
      "epoch:8, batch: 1166,  loss: 1.2273730039596558\n",
      "epoch:8, batch: 1167,  loss: 1.2136681079864502\n",
      "epoch:8, batch: 1168,  loss: 0.7654358148574829\n",
      "epoch:8, batch: 1169,  loss: 1.1808693408966064\n",
      "epoch:8, batch: 1170,  loss: 1.176527738571167\n",
      "epoch:8, batch: 1171,  loss: 0.7731093764305115\n",
      "epoch:8, batch: 1172,  loss: 0.9919291734695435\n",
      "epoch:8, batch: 1173,  loss: 0.9929278492927551\n",
      "epoch:8, batch: 1174,  loss: 0.9057580232620239\n",
      "epoch:8, batch: 1175,  loss: 1.3310784101486206\n",
      "epoch:8, batch: 1176,  loss: 0.8652787804603577\n",
      "epoch:8, batch: 1177,  loss: 0.986767053604126\n",
      "epoch:8, batch: 1178,  loss: 1.143204927444458\n",
      "epoch:8, batch: 1179,  loss: 1.0108712911605835\n",
      "epoch:8, batch: 1180,  loss: 1.0580852031707764\n",
      "epoch:8, batch: 1181,  loss: 0.7855023145675659\n",
      "epoch:8, batch: 1182,  loss: 1.1742939949035645\n",
      "epoch:8, batch: 1183,  loss: 1.033612847328186\n",
      "epoch:8, batch: 1184,  loss: 0.9811936616897583\n",
      "epoch:8, batch: 1185,  loss: 1.0860726833343506\n",
      "epoch:8, batch: 1186,  loss: 1.1031982898712158\n",
      "epoch:8, batch: 1187,  loss: 0.9407018423080444\n",
      "epoch:8, batch: 1188,  loss: 0.8736128211021423\n",
      "epoch:8, batch: 1189,  loss: 1.0437476634979248\n",
      "epoch:8, batch: 1190,  loss: 1.2520288228988647\n",
      "epoch:8, batch: 1191,  loss: 1.1352359056472778\n",
      "epoch:8, batch: 1192,  loss: 0.9521074295043945\n",
      "epoch:8, batch: 1193,  loss: 1.2142086029052734\n",
      "epoch:8, batch: 1194,  loss: 1.046303153038025\n",
      "epoch:8, batch: 1195,  loss: 1.0399924516677856\n",
      "epoch:8, batch: 1196,  loss: 1.0501543283462524\n",
      "epoch:8, batch: 1197,  loss: 1.354080080986023\n",
      "epoch:8, batch: 1198,  loss: 1.0929962396621704\n",
      "epoch:8, batch: 1199,  loss: 1.1836625337600708\n",
      "epoch:8, batch: 1200,  loss: 0.9677372574806213\n",
      "epoch:8, batch: 1201,  loss: 0.9518861770629883\n",
      "epoch:8, batch: 1202,  loss: 1.0156104564666748\n",
      "epoch:8, batch: 1203,  loss: 0.8740797638893127\n",
      "epoch:8, batch: 1204,  loss: 1.3614388704299927\n",
      "epoch:8, batch: 1205,  loss: 1.1504610776901245\n",
      "epoch:8, batch: 1206,  loss: 1.142965316772461\n",
      "epoch:8, batch: 1207,  loss: 1.1594096422195435\n",
      "epoch:8, batch: 1208,  loss: 0.7253763675689697\n",
      "epoch:8, batch: 1209,  loss: 1.028630256652832\n",
      "epoch:8, batch: 1210,  loss: 1.0689302682876587\n",
      "epoch:8, batch: 1211,  loss: 1.1318459510803223\n",
      "epoch:8, batch: 1212,  loss: 1.0867942571640015\n",
      "epoch:8, batch: 1213,  loss: 1.136760950088501\n",
      "epoch:8, batch: 1214,  loss: 1.1409279108047485\n",
      "epoch:8, batch: 1215,  loss: 1.0843509435653687\n",
      "epoch:8, batch: 1216,  loss: 1.1293901205062866\n",
      "epoch:8, batch: 1217,  loss: 0.9972959160804749\n",
      "epoch:8, batch: 1218,  loss: 1.3291038274765015\n",
      "epoch:8, batch: 1219,  loss: 0.8369852304458618\n",
      "epoch:8, batch: 1220,  loss: 0.8106073141098022\n",
      "epoch:8, batch: 1221,  loss: 1.1818135976791382\n",
      "epoch:8, batch: 1222,  loss: 0.8714116811752319\n",
      "epoch:8, batch: 1223,  loss: 0.7490220665931702\n",
      "epoch:8, batch: 1224,  loss: 0.9157153367996216\n",
      "epoch:8, batch: 1225,  loss: 0.8289154767990112\n",
      "epoch:8, batch: 1226,  loss: 0.8662713766098022\n",
      "epoch:8, batch: 1227,  loss: 1.0433580875396729\n",
      "epoch:8, batch: 1228,  loss: 1.167795181274414\n",
      "epoch:8, batch: 1229,  loss: 0.9620794653892517\n",
      "epoch:8, batch: 1230,  loss: 1.0294705629348755\n",
      "epoch:8, batch: 1231,  loss: 1.1680504083633423\n",
      "epoch:8, batch: 1232,  loss: 1.1783710718154907\n",
      "epoch:8, batch: 1233,  loss: 1.1359272003173828\n",
      "epoch:8, batch: 1234,  loss: 0.8650344014167786\n",
      "epoch:8, batch: 1235,  loss: 1.2432901859283447\n",
      "epoch:8, batch: 1236,  loss: 0.924406111240387\n",
      "epoch:8, batch: 1237,  loss: 0.8858897686004639\n",
      "epoch:8, batch: 1238,  loss: 1.2890591621398926\n",
      "epoch:8, batch: 1239,  loss: 0.9595495462417603\n",
      "epoch:8, batch: 1240,  loss: 0.9482794404029846\n",
      "epoch:8, batch: 1241,  loss: 1.440740704536438\n",
      "epoch:8, batch: 1242,  loss: 1.2033601999282837\n",
      "epoch:8, batch: 1243,  loss: 1.1273828744888306\n",
      "epoch:8, batch: 1244,  loss: 1.4283753633499146\n",
      "epoch:8, batch: 1245,  loss: 0.8102799654006958\n",
      "epoch:8, batch: 1246,  loss: 1.114494800567627\n",
      "epoch:8, batch: 1247,  loss: 1.424408197402954\n",
      "epoch:8, batch: 1248,  loss: 0.8308926224708557\n",
      "epoch:8, batch: 1249,  loss: 1.1491990089416504\n",
      "epoch:8, batch: 1250,  loss: 0.9915553331375122\n",
      "epoch:8, batch: 1251,  loss: 0.7318501472473145\n",
      "epoch:8, batch: 1252,  loss: 0.7967061996459961\n",
      "epoch:8, batch: 1253,  loss: 1.1264033317565918\n",
      "epoch:8, batch: 1254,  loss: 1.4460701942443848\n",
      "epoch:8, batch: 1255,  loss: 0.8144224286079407\n",
      "epoch:8, batch: 1256,  loss: 0.927700936794281\n",
      "epoch:8, batch: 1257,  loss: 1.037842035293579\n",
      "epoch:8, batch: 1258,  loss: 0.9372278451919556\n",
      "epoch:8, batch: 1259,  loss: 0.7675057053565979\n",
      "epoch:8, batch: 1260,  loss: 0.9954863786697388\n",
      "epoch:8, batch: 1261,  loss: 0.906218945980072\n",
      "epoch:8, batch: 1262,  loss: 1.22361421585083\n",
      "epoch:8, batch: 1263,  loss: 0.9796980023384094\n",
      "epoch:8, batch: 1264,  loss: 0.9086825847625732\n",
      "epoch:8, batch: 1265,  loss: 1.1787192821502686\n",
      "epoch:8, batch: 1266,  loss: 1.0316717624664307\n",
      "epoch:8, batch: 1267,  loss: 1.0431606769561768\n",
      "epoch:8, batch: 1268,  loss: 1.0172263383865356\n",
      "epoch:8, batch: 1269,  loss: 1.0062474012374878\n",
      "epoch:8, batch: 1270,  loss: 0.986303448677063\n",
      "epoch:8, batch: 1271,  loss: 0.9002770781517029\n",
      "epoch:8, batch: 1272,  loss: 1.1790180206298828\n",
      "epoch:8, batch: 1273,  loss: 1.2460442781448364\n",
      "epoch:8, batch: 1274,  loss: 1.2316088676452637\n",
      "epoch:8, batch: 1275,  loss: 1.318359136581421\n",
      "epoch:8, batch: 1276,  loss: 1.1109501123428345\n",
      "epoch:8, batch: 1277,  loss: 1.3843469619750977\n",
      "epoch:8, batch: 1278,  loss: 1.1852613687515259\n",
      "epoch:8, batch: 1279,  loss: 0.9750776290893555\n",
      "epoch:8, batch: 1280,  loss: 0.7664108276367188\n",
      "epoch:8, batch: 1281,  loss: 1.1650753021240234\n",
      "epoch:8, batch: 1282,  loss: 0.980351984500885\n",
      "epoch:8, batch: 1283,  loss: 0.838752806186676\n",
      "epoch:8, batch: 1284,  loss: 0.9791746139526367\n",
      "epoch:8, batch: 1285,  loss: 0.9441229104995728\n",
      "epoch:8, batch: 1286,  loss: 1.0428847074508667\n",
      "epoch:8, batch: 1287,  loss: 0.8588806390762329\n",
      "epoch:8, batch: 1288,  loss: 1.123148798942566\n",
      "epoch:8, batch: 1289,  loss: 1.269212245941162\n",
      "epoch:8, batch: 1290,  loss: 0.924750804901123\n",
      "epoch:8, batch: 1291,  loss: 0.8538482189178467\n",
      "epoch:8, batch: 1292,  loss: 1.0554509162902832\n",
      "epoch:8, batch: 1293,  loss: 0.8040679097175598\n",
      "epoch:8, batch: 1294,  loss: 1.024852991104126\n",
      "epoch:8, batch: 1295,  loss: 1.037798285484314\n",
      "epoch:8, batch: 1296,  loss: 1.2923808097839355\n",
      "epoch:8, batch: 1297,  loss: 1.1369978189468384\n",
      "epoch:8, batch: 1298,  loss: 1.01606023311615\n",
      "epoch:8, batch: 1299,  loss: 1.0973697900772095\n",
      "epoch:8, batch: 1300,  loss: 1.123146653175354\n",
      "epoch:8, batch: 1301,  loss: 1.135062336921692\n",
      "epoch:8, batch: 1302,  loss: 1.0993597507476807\n",
      "epoch:8, batch: 1303,  loss: 0.830224871635437\n",
      "epoch:8, batch: 1304,  loss: 1.0155813694000244\n",
      "epoch:8, batch: 1305,  loss: 1.0439012050628662\n",
      "epoch:8, batch: 1306,  loss: 0.8812070488929749\n",
      "epoch:8, batch: 1307,  loss: 0.8802317976951599\n",
      "epoch:8, batch: 1308,  loss: 1.144885778427124\n",
      "epoch:8, batch: 1309,  loss: 1.073481559753418\n",
      "epoch:8, batch: 1310,  loss: 0.9646929502487183\n",
      "epoch:8, batch: 1311,  loss: 0.7949660420417786\n",
      "epoch:8, batch: 1312,  loss: 0.9499497413635254\n",
      "epoch:8, batch: 1313,  loss: 0.8680418729782104\n",
      "epoch:8, batch: 1314,  loss: 0.9808668494224548\n",
      "epoch:8, batch: 1315,  loss: 0.9330865144729614\n",
      "epoch:8, batch: 1316,  loss: 0.7760070562362671\n",
      "epoch:8, batch: 1317,  loss: 0.8466300964355469\n",
      "epoch:8, batch: 1318,  loss: 1.1585437059402466\n",
      "epoch:8, batch: 1319,  loss: 0.8176433444023132\n",
      "epoch:8, batch: 1320,  loss: 1.1163976192474365\n",
      "epoch:8, batch: 1321,  loss: 1.123719334602356\n",
      "epoch:8, batch: 1322,  loss: 0.9421249032020569\n",
      "epoch:8, batch: 1323,  loss: 0.9342558979988098\n",
      "epoch:8, batch: 1324,  loss: 0.8838766813278198\n",
      "epoch:8, batch: 1325,  loss: 0.9897626638412476\n",
      "epoch:8, batch: 1326,  loss: 0.9799869060516357\n",
      "epoch:8, batch: 1327,  loss: 1.219949722290039\n",
      "epoch:8, batch: 1328,  loss: 0.9875769019126892\n",
      "epoch:8, batch: 1329,  loss: 1.0949417352676392\n",
      "epoch:8, batch: 1330,  loss: 1.0698037147521973\n",
      "epoch:8, batch: 1331,  loss: 0.914175271987915\n",
      "epoch:8, batch: 1332,  loss: 1.052422046661377\n",
      "epoch:8, batch: 1333,  loss: 1.0697822570800781\n",
      "epoch:8, batch: 1334,  loss: 1.0407664775848389\n",
      "epoch:8, batch: 1335,  loss: 0.9310274124145508\n",
      "epoch:8, batch: 1336,  loss: 1.0222946405410767\n",
      "epoch:8, batch: 1337,  loss: 1.288867473602295\n",
      "epoch:8, batch: 1338,  loss: 1.506853461265564\n",
      "epoch:8, batch: 1339,  loss: 1.184126377105713\n",
      "epoch:8, batch: 1340,  loss: 0.8938389420509338\n",
      "epoch:8, batch: 1341,  loss: 0.7817826867103577\n",
      "epoch:8, batch: 1342,  loss: 0.8862312436103821\n",
      "epoch:8, batch: 1343,  loss: 1.1264889240264893\n",
      "epoch:8, batch: 1344,  loss: 1.1034750938415527\n",
      "epoch:8, batch: 1345,  loss: 0.9634471535682678\n",
      "epoch:8, batch: 1346,  loss: 0.8251650929450989\n",
      "epoch:8, batch: 1347,  loss: 1.1658880710601807\n",
      "epoch:8, batch: 1348,  loss: 1.0810139179229736\n",
      "epoch:8, batch: 1349,  loss: 0.9331434965133667\n",
      "epoch:8, batch: 1350,  loss: 1.1624013185501099\n",
      "epoch:8, batch: 1351,  loss: 0.7640862464904785\n",
      "epoch:8, batch: 1352,  loss: 0.7743908762931824\n",
      "epoch:8, batch: 1353,  loss: 1.0967035293579102\n",
      "epoch:8, batch: 1354,  loss: 1.1205767393112183\n",
      "epoch:8, batch: 1355,  loss: 1.009068489074707\n",
      "epoch:8, batch: 1356,  loss: 0.857825756072998\n",
      "epoch:8, batch: 1357,  loss: 1.065487265586853\n",
      "epoch:8, batch: 1358,  loss: 1.2588391304016113\n",
      "epoch:8, batch: 1359,  loss: 1.2127704620361328\n",
      "epoch:8, batch: 1360,  loss: 0.9985700845718384\n",
      "epoch:8, batch: 1361,  loss: 1.2826155424118042\n",
      "epoch:8, batch: 1362,  loss: 1.018532156944275\n",
      "epoch:8, batch: 1363,  loss: 0.8859747052192688\n",
      "epoch:8, batch: 1364,  loss: 0.943185567855835\n",
      "epoch:8, batch: 1365,  loss: 0.9381726980209351\n",
      "epoch:8, batch: 1366,  loss: 1.094241738319397\n",
      "epoch:8, batch: 1367,  loss: 1.1477712392807007\n",
      "epoch:8, batch: 1368,  loss: 0.8667391538619995\n",
      "epoch:8, batch: 1369,  loss: 0.730833113193512\n",
      "epoch:8, batch: 1370,  loss: 0.905084490776062\n",
      "epoch:8, batch: 1371,  loss: 1.1196268796920776\n",
      "epoch:8, batch: 1372,  loss: 0.8230329751968384\n",
      "epoch:8, batch: 1373,  loss: 0.932976245880127\n",
      "epoch:8, batch: 1374,  loss: 0.8527781367301941\n",
      "epoch:8, batch: 1375,  loss: 1.1509053707122803\n",
      "epoch:8, batch: 1376,  loss: 1.2267905473709106\n",
      "epoch:8, batch: 1377,  loss: 1.3688485622406006\n",
      "epoch:8, batch: 1378,  loss: 0.8018156886100769\n",
      "epoch:8, batch: 1379,  loss: 1.2432329654693604\n",
      "epoch:8, batch: 1380,  loss: 1.1951587200164795\n",
      "epoch:8, batch: 1381,  loss: 1.195718765258789\n",
      "epoch:8, batch: 1382,  loss: 0.8197163939476013\n",
      "epoch:8, batch: 1383,  loss: 0.8559497594833374\n",
      "epoch:8, batch: 1384,  loss: 0.8834580183029175\n",
      "epoch:8, batch: 1385,  loss: 0.9617830514907837\n",
      "epoch:8, batch: 1386,  loss: 0.8701169490814209\n",
      "epoch:8, batch: 1387,  loss: 0.9593222737312317\n",
      "epoch:8, batch: 1388,  loss: 0.8836801052093506\n",
      "epoch:8, batch: 1389,  loss: 1.0102938413619995\n",
      "epoch:8, batch: 1390,  loss: 1.0111241340637207\n",
      "epoch:8, batch: 1391,  loss: 0.963760256767273\n",
      "epoch:8, batch: 1392,  loss: 0.674595832824707\n",
      "epoch:8, batch: 1393,  loss: 0.9788426756858826\n",
      "epoch:8, batch: 1394,  loss: 1.1852365732192993\n",
      "epoch:8, batch: 1395,  loss: 1.2088966369628906\n",
      "epoch:8, batch: 1396,  loss: 0.7317424416542053\n",
      "epoch:8, batch: 1397,  loss: 0.9679168462753296\n",
      "epoch:8, batch: 1398,  loss: 1.0207573175430298\n",
      "epoch:8, batch: 1399,  loss: 1.0858445167541504\n",
      "epoch:8, batch: 1400,  loss: 0.8907703161239624\n",
      "epoch:8, batch: 1401,  loss: 1.366877794265747\n",
      "epoch:8, batch: 1402,  loss: 1.1655621528625488\n",
      "epoch:8, batch: 1403,  loss: 0.8278440237045288\n",
      "epoch:8, batch: 1404,  loss: 1.102892518043518\n",
      "epoch:8, batch: 1405,  loss: 1.3729536533355713\n",
      "epoch:8, batch: 1406,  loss: 0.8420500755310059\n",
      "epoch:8, batch: 1407,  loss: 0.9981313347816467\n",
      "epoch:8, batch: 1408,  loss: 1.5332236289978027\n",
      "epoch:8, batch: 1409,  loss: 0.8622328639030457\n",
      "epoch:8, batch: 1410,  loss: 0.8651891350746155\n",
      "epoch:8, batch: 1411,  loss: 1.020922064781189\n",
      "epoch:8, batch: 1412,  loss: 1.226039171218872\n",
      "epoch:8, batch: 1413,  loss: 1.3706111907958984\n",
      "epoch:8, batch: 1414,  loss: 1.071603536605835\n",
      "epoch:8, batch: 1415,  loss: 1.0506802797317505\n",
      "epoch:8, batch: 1416,  loss: 1.0521907806396484\n",
      "epoch:8, batch: 1417,  loss: 0.9592135548591614\n",
      "epoch:8, batch: 1418,  loss: 0.8277036547660828\n",
      "epoch:8, batch: 1419,  loss: 0.8893735408782959\n",
      "epoch:8, batch: 1420,  loss: 1.1117342710494995\n",
      "epoch:8, batch: 1421,  loss: 1.0739089250564575\n",
      "epoch:8, batch: 1422,  loss: 1.0906412601470947\n",
      "epoch:8, batch: 1423,  loss: 0.7837387919425964\n",
      "epoch:8, batch: 1424,  loss: 0.9525975584983826\n",
      "epoch:8, batch: 1425,  loss: 1.0329134464263916\n",
      "epoch:8, batch: 1426,  loss: 0.8936024904251099\n",
      "epoch:8, batch: 1427,  loss: 1.149809718132019\n",
      "epoch:8, batch: 1428,  loss: 0.9119182825088501\n",
      "epoch:8, batch: 1429,  loss: 1.095771074295044\n",
      "epoch:8, batch: 1430,  loss: 0.9451147317886353\n",
      "epoch:8, batch: 1431,  loss: 0.9564827680587769\n",
      "epoch:8, batch: 1432,  loss: 1.061835765838623\n",
      "epoch:8, batch: 1433,  loss: 0.958706259727478\n",
      "epoch:8, batch: 1434,  loss: 1.2969547510147095\n",
      "epoch:8, batch: 1435,  loss: 1.2532790899276733\n",
      "epoch:8, batch: 1436,  loss: 1.1372889280319214\n",
      "epoch:8, batch: 1437,  loss: 0.7805126309394836\n",
      "epoch:8, batch: 1438,  loss: 1.0286362171173096\n",
      "epoch:8, batch: 1439,  loss: 1.010399580001831\n",
      "epoch:8, batch: 1440,  loss: 1.3207794427871704\n",
      "epoch:8, batch: 1441,  loss: 1.1546320915222168\n",
      "epoch:8, batch: 1442,  loss: 1.3157546520233154\n",
      "epoch:8, batch: 1443,  loss: 1.2910406589508057\n",
      "epoch:8, batch: 1444,  loss: 0.908626139163971\n",
      "epoch:8, batch: 1445,  loss: 0.8810051679611206\n",
      "epoch:8, batch: 1446,  loss: 0.8139182925224304\n",
      "epoch:8, batch: 1447,  loss: 0.8684725165367126\n",
      "epoch:8, batch: 1448,  loss: 1.0372477769851685\n",
      "epoch:8, batch: 1449,  loss: 1.1054694652557373\n",
      "epoch:8, batch: 1450,  loss: 1.305833339691162\n",
      "epoch:8, batch: 1451,  loss: 0.9310213327407837\n",
      "epoch:8, batch: 1452,  loss: 1.1183019876480103\n",
      "epoch:8, batch: 1453,  loss: 1.12444007396698\n",
      "epoch:8, batch: 1454,  loss: 1.0537385940551758\n",
      "epoch:8, batch: 1455,  loss: 1.0824462175369263\n",
      "epoch:8, batch: 1456,  loss: 0.8442659974098206\n",
      "epoch:8, batch: 1457,  loss: 1.0445096492767334\n",
      "epoch:8, batch: 1458,  loss: 0.9976773858070374\n",
      "epoch:8, batch: 1459,  loss: 1.005487084388733\n",
      "epoch:8, batch: 1460,  loss: 0.8187125325202942\n",
      "epoch:8, batch: 1461,  loss: 0.9795628786087036\n",
      "epoch:8, batch: 1462,  loss: 1.263828158378601\n",
      "epoch:8, batch: 1463,  loss: 0.9727493524551392\n",
      "epoch:8, batch: 1464,  loss: 0.9510946273803711\n",
      "epoch:8, batch: 1465,  loss: 1.1007903814315796\n",
      "epoch:8, batch: 1466,  loss: 1.1788437366485596\n",
      "epoch:8, batch: 1467,  loss: 1.0633083581924438\n",
      "epoch:8, batch: 1468,  loss: 1.1256896257400513\n",
      "epoch:8, batch: 1469,  loss: 1.059104323387146\n",
      "epoch:8, batch: 1470,  loss: 1.1720179319381714\n",
      "epoch:8, batch: 1471,  loss: 1.3212149143218994\n",
      "epoch:8, batch: 1472,  loss: 1.135096549987793\n",
      "epoch:8, batch: 1473,  loss: 0.9796880483627319\n",
      "epoch:8, batch: 1474,  loss: 0.9224722981452942\n",
      "epoch:8, batch: 1475,  loss: 0.8019881248474121\n",
      "epoch:8, batch: 1476,  loss: 1.1898465156555176\n",
      "epoch:8, batch: 1477,  loss: 0.9740890264511108\n",
      "epoch:8, batch: 1478,  loss: 0.9111753106117249\n",
      "epoch:8, batch: 1479,  loss: 1.0521622896194458\n",
      "epoch:8, batch: 1480,  loss: 1.1876063346862793\n",
      "epoch:8, batch: 1481,  loss: 1.0466830730438232\n",
      "epoch:8, batch: 1482,  loss: 0.9040780663490295\n",
      "epoch:8, batch: 1483,  loss: 1.0966994762420654\n",
      "epoch:8, batch: 1484,  loss: 0.726380467414856\n",
      "epoch:8, batch: 1485,  loss: 0.938459575176239\n",
      "epoch:8, batch: 1486,  loss: 1.1699223518371582\n",
      "epoch:8, batch: 1487,  loss: 0.7853212356567383\n",
      "epoch:8, batch: 1488,  loss: 1.154299020767212\n",
      "epoch:8, batch: 1489,  loss: 1.0976336002349854\n",
      "epoch:8, batch: 1490,  loss: 1.016709566116333\n",
      "epoch:8, batch: 1491,  loss: 1.0215510129928589\n",
      "epoch:8, batch: 1492,  loss: 1.172117829322815\n",
      "epoch:8, batch: 1493,  loss: 0.6299002766609192\n",
      "epoch:8, batch: 1494,  loss: 0.9297583103179932\n",
      "epoch:8, batch: 1495,  loss: 1.0774699449539185\n",
      "epoch:8, batch: 1496,  loss: 1.188834547996521\n",
      "epoch:8, batch: 1497,  loss: 1.2064032554626465\n",
      "epoch:8, batch: 1498,  loss: 1.4446468353271484\n",
      "epoch:8, batch: 1499,  loss: 1.053562045097351\n",
      "epoch:8, batch: 1500,  loss: 1.324435830116272\n",
      "epoch:8, batch: 1501,  loss: 0.9034550189971924\n",
      "epoch:8, batch: 1502,  loss: 0.9026240110397339\n",
      "epoch:8, batch: 1503,  loss: 0.8973615169525146\n",
      "epoch:8, batch: 1504,  loss: 1.4064632654190063\n",
      "epoch:8, batch: 1505,  loss: 0.7602235674858093\n",
      "epoch:8, batch: 1506,  loss: 1.313136339187622\n",
      "epoch:8, batch: 1507,  loss: 0.940155565738678\n",
      "epoch:8, batch: 1508,  loss: 1.0398634672164917\n",
      "epoch:8, batch: 1509,  loss: 1.4371651411056519\n",
      "epoch:8, batch: 1510,  loss: 0.7060372829437256\n",
      "epoch:8, batch: 1511,  loss: 1.191919207572937\n",
      "epoch:8, batch: 1512,  loss: 1.0180912017822266\n",
      "epoch:8, batch: 1513,  loss: 1.0096814632415771\n",
      "epoch:8, batch: 1514,  loss: 0.8907627463340759\n",
      "epoch:8, batch: 1515,  loss: 1.157135248184204\n",
      "epoch:8, batch: 1516,  loss: 0.8454079031944275\n",
      "epoch:8, batch: 1517,  loss: 0.8343598246574402\n",
      "epoch:8, batch: 1518,  loss: 0.9708809852600098\n",
      "epoch:8, batch: 1519,  loss: 1.2480634450912476\n",
      "epoch:8, batch: 1520,  loss: 0.9518749713897705\n",
      "epoch:8, batch: 1521,  loss: 1.136413812637329\n",
      "epoch:8, batch: 1522,  loss: 1.1513211727142334\n",
      "epoch:8, batch: 1523,  loss: 1.1453909873962402\n",
      "epoch:8, batch: 1524,  loss: 0.9431121945381165\n",
      "epoch:8, batch: 1525,  loss: 0.9896008968353271\n",
      "epoch:8, batch: 1526,  loss: 1.1228479146957397\n",
      "epoch:8, batch: 1527,  loss: 0.9000007510185242\n",
      "epoch:8, batch: 1528,  loss: 0.9954888820648193\n",
      "epoch:8, batch: 1529,  loss: 0.851567804813385\n",
      "epoch:8, batch: 1530,  loss: 0.9826099872589111\n",
      "epoch:8, batch: 1531,  loss: 1.701838493347168\n",
      "epoch:8, batch: 1532,  loss: 1.1926450729370117\n",
      "epoch:8, batch: 1533,  loss: 1.0689164400100708\n",
      "epoch:8, batch: 1534,  loss: 1.0764650106430054\n",
      "epoch:8, batch: 1535,  loss: 1.3065842390060425\n",
      "epoch:8, batch: 1536,  loss: 0.9344783425331116\n",
      "epoch:8, batch: 1537,  loss: 1.2143194675445557\n",
      "epoch:8, batch: 1538,  loss: 1.296720027923584\n",
      "epoch:8, batch: 1539,  loss: 0.9244812726974487\n",
      "epoch:8, batch: 1540,  loss: 0.7368037700653076\n",
      "epoch:8, batch: 1541,  loss: 1.1959924697875977\n",
      "epoch:8, batch: 1542,  loss: 1.0639915466308594\n",
      "epoch:8, batch: 1543,  loss: 1.1829547882080078\n",
      "epoch:8, batch: 1544,  loss: 1.3218649625778198\n",
      "epoch:8, batch: 1545,  loss: 1.2103296518325806\n",
      "epoch:8, batch: 1546,  loss: 1.0145071744918823\n",
      "epoch:8, batch: 1547,  loss: 1.0225000381469727\n",
      "epoch:8, batch: 1548,  loss: 0.809739351272583\n",
      "epoch:8, batch: 1549,  loss: 1.0837619304656982\n",
      "epoch:8, batch: 1550,  loss: 1.0133326053619385\n",
      "epoch:8, batch: 1551,  loss: 1.0391294956207275\n",
      "epoch:8, batch: 1552,  loss: 1.2027180194854736\n",
      "epoch:8, batch: 1553,  loss: 1.332420825958252\n",
      "epoch:8, batch: 1554,  loss: 1.1184378862380981\n",
      "epoch:8, batch: 1555,  loss: 1.3357040882110596\n",
      "epoch:8, batch: 1556,  loss: 1.0203741788864136\n",
      "epoch:8, batch: 1557,  loss: 1.301782488822937\n",
      "epoch:8, batch: 1558,  loss: 1.2239692211151123\n",
      "epoch:8, batch: 1559,  loss: 1.0257444381713867\n",
      "epoch:8, batch: 1560,  loss: 1.0546867847442627\n",
      "epoch:8, batch: 1561,  loss: 1.1767914295196533\n",
      "epoch:8, batch: 1562,  loss: 1.2515068054199219\n",
      "epoch:8, batch: 1563,  loss: 0.7933062314987183\n",
      "epoch:8, batch: 1564,  loss: 1.108323335647583\n",
      "epoch:8, batch: 1565,  loss: 1.0743119716644287\n",
      "epoch:8, batch: 1566,  loss: 0.8443121910095215\n",
      "epoch:8, batch: 1567,  loss: 0.908391535282135\n",
      "epoch:8, batch: 1568,  loss: 0.8760216236114502\n",
      "epoch:8, batch: 1569,  loss: 1.0917787551879883\n",
      "epoch:8, batch: 1570,  loss: 1.0849525928497314\n",
      "epoch:8, batch: 1571,  loss: 1.0642015933990479\n",
      "epoch:8, batch: 1572,  loss: 0.9931490421295166\n",
      "epoch:8, batch: 1573,  loss: 1.242322564125061\n",
      "epoch:8, batch: 1574,  loss: 0.830894947052002\n",
      "epoch:8, batch: 1575,  loss: 1.3864995241165161\n",
      "epoch:8, batch: 1576,  loss: 1.1581354141235352\n",
      "epoch:8, batch: 1577,  loss: 1.009124517440796\n",
      "epoch:8, batch: 1578,  loss: 1.1991474628448486\n",
      "epoch:8, batch: 1579,  loss: 0.9978280663490295\n",
      "epoch:8, batch: 1580,  loss: 0.9874414205551147\n",
      "epoch:8, batch: 1581,  loss: 1.2544498443603516\n",
      "epoch:8, batch: 1582,  loss: 1.295209288597107\n",
      "epoch:8, batch: 1583,  loss: 1.0075738430023193\n",
      "epoch:8, batch: 1584,  loss: 1.141615629196167\n",
      "epoch:8, batch: 1585,  loss: 0.896000862121582\n",
      "epoch:8, batch: 1586,  loss: 0.7400369048118591\n",
      "epoch:8, batch: 1587,  loss: 0.8901410698890686\n",
      "epoch:8, batch: 1588,  loss: 0.9509618878364563\n",
      "epoch:8, batch: 1589,  loss: 0.8400424122810364\n",
      "epoch:8, batch: 1590,  loss: 1.2264924049377441\n",
      "epoch:8, batch: 1591,  loss: 1.1389018297195435\n",
      "epoch:8, batch: 1592,  loss: 1.0182607173919678\n",
      "epoch:8, batch: 1593,  loss: 0.954378604888916\n",
      "epoch:8, batch: 1594,  loss: 1.0865448713302612\n",
      "epoch:8, batch: 1595,  loss: 1.0904839038848877\n",
      "epoch:8, batch: 1596,  loss: 1.0087525844573975\n",
      "epoch:8, batch: 1597,  loss: 1.387157678604126\n",
      "epoch:8, batch: 1598,  loss: 1.3007066249847412\n",
      "epoch:8, batch: 1599,  loss: 1.048025131225586\n",
      "epoch:8, batch: 1600,  loss: 0.9627504348754883\n",
      "epoch:8, batch: 1601,  loss: 1.0389703512191772\n",
      "epoch:8, batch: 1602,  loss: 1.1222559213638306\n",
      "epoch:8, batch: 1603,  loss: 1.1026818752288818\n",
      "epoch:8, batch: 1604,  loss: 1.140822172164917\n",
      "epoch:8, batch: 1605,  loss: 1.1596475839614868\n",
      "epoch:8, batch: 1606,  loss: 1.2275749444961548\n",
      "epoch:8, batch: 1607,  loss: 1.0576728582382202\n",
      "epoch:8, batch: 1608,  loss: 1.1680021286010742\n",
      "epoch:8, batch: 1609,  loss: 0.9026780128479004\n",
      "epoch:8, batch: 1610,  loss: 1.3156527280807495\n",
      "epoch:8, batch: 1611,  loss: 1.2593421936035156\n",
      "epoch:8, batch: 1612,  loss: 1.0185891389846802\n",
      "epoch:8, batch: 1613,  loss: 0.977021336555481\n",
      "epoch:8, batch: 1614,  loss: 1.0953644514083862\n",
      "epoch:8, batch: 1615,  loss: 1.7489134073257446\n",
      "epoch:8, batch: 1616,  loss: 1.2001433372497559\n",
      "epoch:8, batch: 1617,  loss: 0.8787510395050049\n",
      "epoch:8, batch: 1618,  loss: 0.8120242953300476\n",
      "epoch:8, batch: 1619,  loss: 1.0330073833465576\n",
      "epoch:8, batch: 1620,  loss: 1.0443570613861084\n",
      "epoch:8, batch: 1621,  loss: 1.0639371871948242\n",
      "epoch:8, batch: 1622,  loss: 0.9192066788673401\n",
      "epoch:8, batch: 1623,  loss: 0.9400009512901306\n",
      "epoch:8, batch: 1624,  loss: 0.9690755605697632\n",
      "epoch:8, batch: 1625,  loss: 1.082535743713379\n",
      "epoch:8, batch: 1626,  loss: 1.014580488204956\n",
      "epoch:8, batch: 1627,  loss: 1.278261661529541\n",
      "epoch:8, batch: 1628,  loss: 1.2262279987335205\n",
      "epoch:8, batch: 1629,  loss: 0.7545385360717773\n",
      "epoch:8, batch: 1630,  loss: 1.031927227973938\n",
      "epoch:8, batch: 1631,  loss: 1.0298523902893066\n",
      "epoch:8, batch: 1632,  loss: 1.1793267726898193\n",
      "epoch:8, batch: 1633,  loss: 1.1500988006591797\n",
      "epoch:8, batch: 1634,  loss: 0.7565053105354309\n",
      "epoch:8, batch: 1635,  loss: 1.0304898023605347\n",
      "epoch:8, batch: 1636,  loss: 1.117068886756897\n",
      "epoch:8, batch: 1637,  loss: 1.027571439743042\n",
      "epoch:8, batch: 1638,  loss: 0.8807786107063293\n",
      "epoch:8, batch: 1639,  loss: 0.7356081604957581\n",
      "epoch:8, batch: 1640,  loss: 1.0442655086517334\n",
      "epoch:8, batch: 1641,  loss: 0.703517735004425\n",
      "epoch:8, batch: 1642,  loss: 0.935875415802002\n",
      "epoch:8, batch: 1643,  loss: 1.1750881671905518\n",
      "epoch:8, batch: 1644,  loss: 0.7582783102989197\n",
      "epoch:8, batch: 1645,  loss: 1.0655320882797241\n",
      "epoch:8, batch: 1646,  loss: 1.3741204738616943\n",
      "epoch:8, batch: 1647,  loss: 1.0354024171829224\n",
      "epoch:8, batch: 1648,  loss: 1.153634786605835\n",
      "epoch:8, batch: 1649,  loss: 1.1007903814315796\n",
      "epoch:8, batch: 1650,  loss: 0.825325608253479\n",
      "epoch:8, batch: 1651,  loss: 0.6708089709281921\n",
      "epoch:8, batch: 1652,  loss: 1.0044502019882202\n",
      "epoch:8, batch: 1653,  loss: 0.6548935174942017\n",
      "epoch:8, batch: 1654,  loss: 0.950087308883667\n",
      "epoch:8, batch: 1655,  loss: 1.4835619926452637\n",
      "epoch:8, batch: 1656,  loss: 1.2168488502502441\n",
      "epoch:8, batch: 1657,  loss: 0.9996729493141174\n",
      "epoch:8, batch: 1658,  loss: 0.9104774594306946\n",
      "epoch:8, batch: 1659,  loss: 0.9459000825881958\n",
      "epoch:8, batch: 1660,  loss: 0.9941571950912476\n",
      "epoch:8, batch: 1661,  loss: 1.0564615726470947\n",
      "epoch:8, batch: 1662,  loss: 1.0751444101333618\n",
      "epoch:8, batch: 1663,  loss: 0.9895990490913391\n",
      "epoch:8, batch: 1664,  loss: 0.8005616664886475\n",
      "epoch:8, batch: 1665,  loss: 0.9901466965675354\n",
      "epoch:8, batch: 1666,  loss: 1.08913254737854\n",
      "epoch:8, batch: 1667,  loss: 0.8299916982650757\n",
      "epoch:8, batch: 1668,  loss: 1.0354547500610352\n",
      "epoch:8, batch: 1669,  loss: 1.2627633810043335\n",
      "epoch:8, batch: 1670,  loss: 1.0283972024917603\n",
      "epoch:8, batch: 1671,  loss: 1.310265302658081\n",
      "epoch:8, batch: 1672,  loss: 0.9962936639785767\n",
      "epoch:8, batch: 1673,  loss: 1.0114307403564453\n",
      "epoch:8, batch: 1674,  loss: 0.9824659824371338\n",
      "epoch:8, batch: 1675,  loss: 0.9153473973274231\n",
      "epoch:8, batch: 1676,  loss: 1.342758059501648\n",
      "epoch:8, batch: 1677,  loss: 0.948847234249115\n",
      "epoch:8, batch: 1678,  loss: 1.3255741596221924\n",
      "epoch:8, batch: 1679,  loss: 1.0905817747116089\n",
      "epoch:8, batch: 1680,  loss: 0.9181106090545654\n",
      "epoch:8, batch: 1681,  loss: 1.031019926071167\n",
      "epoch:8, batch: 1682,  loss: 1.2400798797607422\n",
      "epoch:8, batch: 1683,  loss: 1.0549545288085938\n",
      "epoch:8, batch: 1684,  loss: 1.288870930671692\n",
      "epoch:8, batch: 1685,  loss: 1.0453152656555176\n",
      "epoch:8, batch: 1686,  loss: 0.8425965309143066\n",
      "epoch:8, batch: 1687,  loss: 1.1385446786880493\n",
      "epoch:8, batch: 1688,  loss: 1.041915774345398\n",
      "epoch:8, batch: 1689,  loss: 1.2283165454864502\n",
      "epoch:8, batch: 1690,  loss: 1.1962547302246094\n",
      "epoch:8, batch: 1691,  loss: 1.0106157064437866\n",
      "epoch:8, batch: 1692,  loss: 0.9347574710845947\n",
      "epoch:8, batch: 1693,  loss: 1.2558157444000244\n",
      "epoch:8, batch: 1694,  loss: 1.0414880514144897\n",
      "epoch:8, batch: 1695,  loss: 0.9477514028549194\n",
      "epoch:8, batch: 1696,  loss: 1.1587618589401245\n",
      "epoch:8, batch: 1697,  loss: 1.2844324111938477\n",
      "epoch:8, batch: 1698,  loss: 0.9287579655647278\n",
      "epoch:8, batch: 1699,  loss: 1.4375038146972656\n",
      "epoch:8, batch: 1700,  loss: 0.7024223804473877\n",
      "epoch:8, batch: 1701,  loss: 0.7772130370140076\n",
      "epoch:8, batch: 1702,  loss: 0.7942495942115784\n",
      "epoch:8, batch: 1703,  loss: 0.9109628796577454\n",
      "epoch:8, batch: 1704,  loss: 1.0700137615203857\n",
      "epoch:8, batch: 1705,  loss: 1.0485237836837769\n",
      "epoch:8, batch: 1706,  loss: 0.8789498805999756\n",
      "epoch:8, batch: 1707,  loss: 0.9529226422309875\n",
      "epoch:8, batch: 1708,  loss: 0.6748522520065308\n",
      "epoch:8, batch: 1709,  loss: 1.1722815036773682\n",
      "epoch:8, batch: 1710,  loss: 1.0903323888778687\n",
      "epoch:8, batch: 1711,  loss: 0.8665862679481506\n",
      "epoch:8, batch: 1712,  loss: 1.4584202766418457\n",
      "epoch:8, batch: 1713,  loss: 0.7977589964866638\n",
      "epoch:8, batch: 1714,  loss: 1.1724112033843994\n",
      "epoch:8, batch: 1715,  loss: 0.8379488587379456\n",
      "epoch:8, batch: 1716,  loss: 0.9744527339935303\n",
      "epoch:8, batch: 1717,  loss: 0.9650183916091919\n",
      "epoch:8, batch: 1718,  loss: 0.7438754439353943\n",
      "epoch:8, batch: 1719,  loss: 1.1990807056427002\n",
      "epoch:8, batch: 1720,  loss: 1.1668164730072021\n",
      "epoch:8, batch: 1721,  loss: 0.652038037776947\n",
      "epoch:8, batch: 1722,  loss: 0.983803927898407\n",
      "epoch:8, batch: 1723,  loss: 0.8670297861099243\n",
      "epoch:8, batch: 1724,  loss: 0.9435269236564636\n",
      "epoch:8, batch: 1725,  loss: 0.8587702512741089\n",
      "epoch:8, batch: 1726,  loss: 1.0709917545318604\n",
      "epoch:8, batch: 1727,  loss: 0.8670908212661743\n",
      "epoch:8, batch: 1728,  loss: 0.7897782921791077\n",
      "epoch:8, batch: 1729,  loss: 0.9653377532958984\n",
      "epoch:8, batch: 1730,  loss: 0.8413352966308594\n",
      "epoch:8, batch: 1731,  loss: 0.9307218194007874\n",
      "epoch:8, batch: 1732,  loss: 0.9999393224716187\n",
      "epoch:8, batch: 1733,  loss: 0.9659024477005005\n",
      "epoch:8, batch: 1734,  loss: 1.2300559282302856\n",
      "epoch:8, batch: 1735,  loss: 0.7345927357673645\n",
      "epoch:8, batch: 1736,  loss: 1.1452922821044922\n",
      "epoch:8, batch: 1737,  loss: 1.0670201778411865\n",
      "epoch:8, batch: 1738,  loss: 0.993695080280304\n",
      "epoch:8, batch: 1739,  loss: 0.9103243350982666\n",
      "epoch:8, batch: 1740,  loss: 1.2732402086257935\n",
      "epoch:8, batch: 1741,  loss: 1.1385189294815063\n",
      "epoch:8, batch: 1742,  loss: 1.2463487386703491\n",
      "epoch:8, batch: 1743,  loss: 1.1526148319244385\n",
      "epoch:8, batch: 1744,  loss: 1.01172935962677\n",
      "epoch:8, batch: 1745,  loss: 0.8015475273132324\n",
      "epoch:8, batch: 1746,  loss: 0.8778547644615173\n",
      "epoch:8, batch: 1747,  loss: 1.0482760667800903\n",
      "epoch:8, batch: 1748,  loss: 1.0342718362808228\n",
      "epoch:8, batch: 1749,  loss: 0.9368261098861694\n",
      "epoch:8, batch: 1750,  loss: 1.1145946979522705\n",
      "epoch:8, batch: 1751,  loss: 1.010939598083496\n",
      "epoch:8, batch: 1752,  loss: 1.0602927207946777\n",
      "epoch:8, batch: 1753,  loss: 1.0262422561645508\n",
      "epoch:8, batch: 1754,  loss: 1.15381920337677\n",
      "epoch:8, batch: 1755,  loss: 0.8686543703079224\n",
      "epoch:8, batch: 1756,  loss: 0.9221183061599731\n",
      "epoch:8, batch: 1757,  loss: 0.9066789746284485\n",
      "epoch:8, batch: 1758,  loss: 0.9452019929885864\n",
      "epoch:8, batch: 1759,  loss: 1.3127351999282837\n",
      "epoch:8, batch: 1760,  loss: 0.9428565502166748\n",
      "epoch:8, batch: 1761,  loss: 0.7943282723426819\n",
      "epoch:8, batch: 1762,  loss: 0.9348697662353516\n",
      "epoch:8, batch: 1763,  loss: 1.009658932685852\n",
      "epoch:9, batch: 1,  loss: 1.0302549600601196\n",
      "epoch:9, batch: 2,  loss: 1.6434378623962402\n",
      "epoch:9, batch: 3,  loss: 1.4500651359558105\n",
      "epoch:9, batch: 4,  loss: 1.1231845617294312\n",
      "epoch:9, batch: 5,  loss: 1.0759496688842773\n",
      "epoch:9, batch: 6,  loss: 0.8484654426574707\n",
      "epoch:9, batch: 7,  loss: 1.3925907611846924\n",
      "epoch:9, batch: 8,  loss: 0.9383926391601562\n",
      "epoch:9, batch: 9,  loss: 0.7258254289627075\n",
      "epoch:9, batch: 10,  loss: 0.8171432018280029\n",
      "epoch:9, batch: 11,  loss: 1.0391265153884888\n",
      "epoch:9, batch: 12,  loss: 1.2822102308273315\n",
      "epoch:9, batch: 13,  loss: 1.1144996881484985\n",
      "epoch:9, batch: 14,  loss: 0.964916467666626\n",
      "epoch:9, batch: 15,  loss: 0.9064525961875916\n",
      "epoch:9, batch: 16,  loss: 0.7738538384437561\n",
      "epoch:9, batch: 17,  loss: 1.2457491159439087\n",
      "epoch:9, batch: 18,  loss: 1.0130003690719604\n",
      "epoch:9, batch: 19,  loss: 1.243879795074463\n",
      "epoch:9, batch: 20,  loss: 1.19159734249115\n",
      "epoch:9, batch: 21,  loss: 1.133463978767395\n",
      "epoch:9, batch: 22,  loss: 1.0239861011505127\n",
      "epoch:9, batch: 23,  loss: 0.9600098133087158\n",
      "epoch:9, batch: 24,  loss: 0.9493317008018494\n",
      "epoch:9, batch: 25,  loss: 0.979606032371521\n",
      "epoch:9, batch: 26,  loss: 0.8063130974769592\n",
      "epoch:9, batch: 27,  loss: 0.9670091867446899\n",
      "epoch:9, batch: 28,  loss: 0.7904943227767944\n",
      "epoch:9, batch: 29,  loss: 1.118198037147522\n",
      "epoch:9, batch: 30,  loss: 1.0485897064208984\n",
      "epoch:9, batch: 31,  loss: 1.1920337677001953\n",
      "epoch:9, batch: 32,  loss: 1.1892064809799194\n",
      "epoch:9, batch: 33,  loss: 1.3043262958526611\n",
      "epoch:9, batch: 34,  loss: 1.3289262056350708\n",
      "epoch:9, batch: 35,  loss: 1.4294682741165161\n",
      "epoch:9, batch: 36,  loss: 0.9629415273666382\n",
      "epoch:9, batch: 37,  loss: 1.0365201234817505\n",
      "epoch:9, batch: 38,  loss: 0.9646002650260925\n",
      "epoch:9, batch: 39,  loss: 0.9668629765510559\n",
      "epoch:9, batch: 40,  loss: 1.1258794069290161\n",
      "epoch:9, batch: 41,  loss: 0.6449112296104431\n",
      "epoch:9, batch: 42,  loss: 1.3130213022232056\n",
      "epoch:9, batch: 43,  loss: 0.9902255535125732\n",
      "epoch:9, batch: 44,  loss: 1.3975340127944946\n",
      "epoch:9, batch: 45,  loss: 1.0975092649459839\n",
      "epoch:9, batch: 46,  loss: 0.9223889112472534\n",
      "epoch:9, batch: 47,  loss: 0.8069433569908142\n",
      "epoch:9, batch: 48,  loss: 1.3336825370788574\n",
      "epoch:9, batch: 49,  loss: 0.9761093854904175\n",
      "epoch:9, batch: 50,  loss: 0.8745145797729492\n",
      "epoch:9, batch: 51,  loss: 1.0289584398269653\n",
      "epoch:9, batch: 52,  loss: 0.9100204110145569\n",
      "epoch:9, batch: 53,  loss: 0.9735541343688965\n",
      "epoch:9, batch: 54,  loss: 1.0824962854385376\n",
      "epoch:9, batch: 55,  loss: 1.1087770462036133\n",
      "epoch:9, batch: 56,  loss: 0.9465871453285217\n",
      "epoch:9, batch: 57,  loss: 0.9351531267166138\n",
      "epoch:9, batch: 58,  loss: 0.6462243795394897\n",
      "epoch:9, batch: 59,  loss: 1.020235538482666\n",
      "epoch:9, batch: 60,  loss: 1.042255163192749\n",
      "epoch:9, batch: 61,  loss: 0.8651540875434875\n",
      "epoch:9, batch: 62,  loss: 1.010833978652954\n",
      "epoch:9, batch: 63,  loss: 1.0988736152648926\n",
      "epoch:9, batch: 64,  loss: 0.7764381766319275\n",
      "epoch:9, batch: 65,  loss: 1.2006168365478516\n",
      "epoch:9, batch: 66,  loss: 0.9779385328292847\n",
      "epoch:9, batch: 67,  loss: 1.1080060005187988\n",
      "epoch:9, batch: 68,  loss: 0.911404013633728\n",
      "epoch:9, batch: 69,  loss: 1.1135790348052979\n",
      "epoch:9, batch: 70,  loss: 1.0887314081192017\n",
      "epoch:9, batch: 71,  loss: 0.8212387561798096\n",
      "epoch:9, batch: 72,  loss: 1.0214478969573975\n",
      "epoch:9, batch: 73,  loss: 1.3911428451538086\n",
      "epoch:9, batch: 74,  loss: 1.0845788717269897\n",
      "epoch:9, batch: 75,  loss: 0.8135362267494202\n",
      "epoch:9, batch: 76,  loss: 0.9068049192428589\n",
      "epoch:9, batch: 77,  loss: 0.9587619304656982\n",
      "epoch:9, batch: 78,  loss: 1.0288015604019165\n",
      "epoch:9, batch: 79,  loss: 1.057397484779358\n",
      "epoch:9, batch: 80,  loss: 0.9166988134384155\n",
      "epoch:9, batch: 81,  loss: 0.8992133736610413\n",
      "epoch:9, batch: 82,  loss: 1.1490352153778076\n",
      "epoch:9, batch: 83,  loss: 0.7181723117828369\n",
      "epoch:9, batch: 84,  loss: 0.8232782483100891\n",
      "epoch:9, batch: 85,  loss: 1.0971025228500366\n",
      "epoch:9, batch: 86,  loss: 0.8938044309616089\n",
      "epoch:9, batch: 87,  loss: 0.9902070164680481\n",
      "epoch:9, batch: 88,  loss: 0.8014253973960876\n",
      "epoch:9, batch: 89,  loss: 0.8500612378120422\n",
      "epoch:9, batch: 90,  loss: 1.1044646501541138\n",
      "epoch:9, batch: 91,  loss: 1.0649425983428955\n",
      "epoch:9, batch: 92,  loss: 0.933735191822052\n",
      "epoch:9, batch: 93,  loss: 1.2275906801223755\n",
      "epoch:9, batch: 94,  loss: 0.9148992300033569\n",
      "epoch:9, batch: 95,  loss: 1.319276213645935\n",
      "epoch:9, batch: 96,  loss: 1.1408244371414185\n",
      "epoch:9, batch: 97,  loss: 0.9422781467437744\n",
      "epoch:9, batch: 98,  loss: 0.9016220569610596\n",
      "epoch:9, batch: 99,  loss: 0.886948823928833\n",
      "epoch:9, batch: 100,  loss: 1.071547031402588\n",
      "epoch:9, batch: 101,  loss: 1.3313586711883545\n",
      "epoch:9, batch: 102,  loss: 0.8799484968185425\n",
      "epoch:9, batch: 103,  loss: 1.0161323547363281\n",
      "epoch:9, batch: 104,  loss: 0.806647539138794\n",
      "epoch:9, batch: 105,  loss: 1.1558583974838257\n",
      "epoch:9, batch: 106,  loss: 1.2237883806228638\n",
      "epoch:9, batch: 107,  loss: 0.953722357749939\n",
      "epoch:9, batch: 108,  loss: 0.9102807641029358\n",
      "epoch:9, batch: 109,  loss: 0.9273715615272522\n",
      "epoch:9, batch: 110,  loss: 0.9654790759086609\n",
      "epoch:9, batch: 111,  loss: 1.0774587392807007\n",
      "epoch:9, batch: 112,  loss: 1.2217419147491455\n",
      "epoch:9, batch: 113,  loss: 0.8516091704368591\n",
      "epoch:9, batch: 114,  loss: 1.2134730815887451\n",
      "epoch:9, batch: 115,  loss: 1.0112547874450684\n",
      "epoch:9, batch: 116,  loss: 1.1060515642166138\n",
      "epoch:9, batch: 117,  loss: 0.9364754557609558\n",
      "epoch:9, batch: 118,  loss: 1.203572392463684\n",
      "epoch:9, batch: 119,  loss: 1.1890816688537598\n",
      "epoch:9, batch: 120,  loss: 0.9556317925453186\n",
      "epoch:9, batch: 121,  loss: 1.0449239015579224\n",
      "epoch:9, batch: 122,  loss: 0.8441301584243774\n",
      "epoch:9, batch: 123,  loss: 0.9508510231971741\n",
      "epoch:9, batch: 124,  loss: 0.8828356266021729\n",
      "epoch:9, batch: 125,  loss: 1.082059383392334\n",
      "epoch:9, batch: 126,  loss: 1.213236927986145\n",
      "epoch:9, batch: 127,  loss: 1.002249002456665\n",
      "epoch:9, batch: 128,  loss: 0.9778006076812744\n",
      "epoch:9, batch: 129,  loss: 1.3158631324768066\n",
      "epoch:9, batch: 130,  loss: 0.7724102735519409\n",
      "epoch:9, batch: 131,  loss: 1.3473451137542725\n",
      "epoch:9, batch: 132,  loss: 0.9753431677818298\n",
      "epoch:9, batch: 133,  loss: 1.2154666185379028\n",
      "epoch:9, batch: 134,  loss: 0.8171374201774597\n",
      "epoch:9, batch: 135,  loss: 1.2501376867294312\n",
      "epoch:9, batch: 136,  loss: 0.6318688988685608\n",
      "epoch:9, batch: 137,  loss: 0.9976665377616882\n",
      "epoch:9, batch: 138,  loss: 0.9036636352539062\n",
      "epoch:9, batch: 139,  loss: 1.0554081201553345\n",
      "epoch:9, batch: 140,  loss: 1.0707237720489502\n",
      "epoch:9, batch: 141,  loss: 0.9006454944610596\n",
      "epoch:9, batch: 142,  loss: 1.2618093490600586\n",
      "epoch:9, batch: 143,  loss: 0.593466579914093\n",
      "epoch:9, batch: 144,  loss: 1.0474073886871338\n",
      "epoch:9, batch: 145,  loss: 1.3228400945663452\n",
      "epoch:9, batch: 146,  loss: 0.8628557920455933\n",
      "epoch:9, batch: 147,  loss: 0.8590260744094849\n",
      "epoch:9, batch: 148,  loss: 1.2264958620071411\n",
      "epoch:9, batch: 149,  loss: 0.8125043511390686\n",
      "epoch:9, batch: 150,  loss: 1.111259937286377\n",
      "epoch:9, batch: 151,  loss: 1.1898051500320435\n",
      "epoch:9, batch: 152,  loss: 0.8667651414871216\n",
      "epoch:9, batch: 153,  loss: 1.014448881149292\n",
      "epoch:9, batch: 154,  loss: 1.2193523645401\n",
      "epoch:9, batch: 155,  loss: 1.1604377031326294\n",
      "epoch:9, batch: 156,  loss: 1.3125429153442383\n",
      "epoch:9, batch: 157,  loss: 0.8227099180221558\n",
      "epoch:9, batch: 158,  loss: 0.9146579504013062\n",
      "epoch:9, batch: 159,  loss: 0.7603063583374023\n",
      "epoch:9, batch: 160,  loss: 0.88773512840271\n",
      "epoch:9, batch: 161,  loss: 0.9473652243614197\n",
      "epoch:9, batch: 162,  loss: 1.1010997295379639\n",
      "epoch:9, batch: 163,  loss: 1.123018503189087\n",
      "epoch:9, batch: 164,  loss: 0.8795837759971619\n",
      "epoch:9, batch: 165,  loss: 1.2961716651916504\n",
      "epoch:9, batch: 166,  loss: 1.2333076000213623\n",
      "epoch:9, batch: 167,  loss: 1.1825939416885376\n",
      "epoch:9, batch: 168,  loss: 1.059131145477295\n",
      "epoch:9, batch: 169,  loss: 1.0085046291351318\n",
      "epoch:9, batch: 170,  loss: 0.7780197858810425\n",
      "epoch:9, batch: 171,  loss: 0.9044841527938843\n",
      "epoch:9, batch: 172,  loss: 1.118222713470459\n",
      "epoch:9, batch: 173,  loss: 1.1626191139221191\n",
      "epoch:9, batch: 174,  loss: 1.0753183364868164\n",
      "epoch:9, batch: 175,  loss: 0.9230055212974548\n",
      "epoch:9, batch: 176,  loss: 1.23281991481781\n",
      "epoch:9, batch: 177,  loss: 1.3518173694610596\n",
      "epoch:9, batch: 178,  loss: 1.1096590757369995\n",
      "epoch:9, batch: 179,  loss: 1.1381943225860596\n",
      "epoch:9, batch: 180,  loss: 1.2733207941055298\n",
      "epoch:9, batch: 181,  loss: 1.0446220636367798\n",
      "epoch:9, batch: 182,  loss: 0.9378515481948853\n",
      "epoch:9, batch: 183,  loss: 0.9691292643547058\n",
      "epoch:9, batch: 184,  loss: 1.2101136445999146\n",
      "epoch:9, batch: 185,  loss: 1.0018441677093506\n",
      "epoch:9, batch: 186,  loss: 1.2774040699005127\n",
      "epoch:9, batch: 187,  loss: 1.0090553760528564\n",
      "epoch:9, batch: 188,  loss: 0.8662154078483582\n",
      "epoch:9, batch: 189,  loss: 1.0827287435531616\n",
      "epoch:9, batch: 190,  loss: 0.7603657841682434\n",
      "epoch:9, batch: 191,  loss: 0.9489109516143799\n",
      "epoch:9, batch: 192,  loss: 1.0231214761734009\n",
      "epoch:9, batch: 193,  loss: 1.0414676666259766\n",
      "epoch:9, batch: 194,  loss: 1.4872586727142334\n",
      "epoch:9, batch: 195,  loss: 1.330659031867981\n",
      "epoch:9, batch: 196,  loss: 1.299849271774292\n",
      "epoch:9, batch: 197,  loss: 1.1242371797561646\n",
      "epoch:9, batch: 198,  loss: 1.0642242431640625\n",
      "epoch:9, batch: 199,  loss: 1.116966724395752\n",
      "epoch:9, batch: 200,  loss: 0.9185275435447693\n",
      "epoch:9, batch: 201,  loss: 1.5604981184005737\n",
      "epoch:9, batch: 202,  loss: 1.0054636001586914\n",
      "epoch:9, batch: 203,  loss: 1.3732643127441406\n",
      "epoch:9, batch: 204,  loss: 1.0445231199264526\n",
      "epoch:9, batch: 205,  loss: 1.195034146308899\n",
      "epoch:9, batch: 206,  loss: 0.8238917589187622\n",
      "epoch:9, batch: 207,  loss: 0.9347570538520813\n",
      "epoch:9, batch: 208,  loss: 1.3328135013580322\n",
      "epoch:9, batch: 209,  loss: 1.0799658298492432\n",
      "epoch:9, batch: 210,  loss: 0.9031657576560974\n",
      "epoch:9, batch: 211,  loss: 0.8466830849647522\n",
      "epoch:9, batch: 212,  loss: 0.7421669960021973\n",
      "epoch:9, batch: 213,  loss: 0.7769757509231567\n",
      "epoch:9, batch: 214,  loss: 1.2354012727737427\n",
      "epoch:9, batch: 215,  loss: 1.0347282886505127\n",
      "epoch:9, batch: 216,  loss: 0.9131845235824585\n",
      "epoch:9, batch: 217,  loss: 1.121852159500122\n",
      "epoch:9, batch: 218,  loss: 1.0466639995574951\n",
      "epoch:9, batch: 219,  loss: 0.9035992622375488\n",
      "epoch:9, batch: 220,  loss: 1.15776789188385\n",
      "epoch:9, batch: 221,  loss: 0.7878485918045044\n",
      "epoch:9, batch: 222,  loss: 0.8630422353744507\n",
      "epoch:9, batch: 223,  loss: 1.1091057062149048\n",
      "epoch:9, batch: 224,  loss: 1.346217393875122\n",
      "epoch:9, batch: 225,  loss: 0.9143983125686646\n",
      "epoch:9, batch: 226,  loss: 0.8056398034095764\n",
      "epoch:9, batch: 227,  loss: 1.1435883045196533\n",
      "epoch:9, batch: 228,  loss: 0.8095989227294922\n",
      "epoch:9, batch: 229,  loss: 1.1115576028823853\n",
      "epoch:9, batch: 230,  loss: 0.6873221397399902\n",
      "epoch:9, batch: 231,  loss: 0.9519457817077637\n",
      "epoch:9, batch: 232,  loss: 1.2982738018035889\n",
      "epoch:9, batch: 233,  loss: 1.0353306531906128\n",
      "epoch:9, batch: 234,  loss: 0.9630880951881409\n",
      "epoch:9, batch: 235,  loss: 1.117104172706604\n",
      "epoch:9, batch: 236,  loss: 0.9276069402694702\n",
      "epoch:9, batch: 237,  loss: 0.8030762672424316\n",
      "epoch:9, batch: 238,  loss: 0.933148205280304\n",
      "epoch:9, batch: 239,  loss: 1.1901664733886719\n",
      "epoch:9, batch: 240,  loss: 0.8448089361190796\n",
      "epoch:9, batch: 241,  loss: 1.0765087604522705\n",
      "epoch:9, batch: 242,  loss: 0.9927814602851868\n",
      "epoch:9, batch: 243,  loss: 0.8679726719856262\n",
      "epoch:9, batch: 244,  loss: 1.0838972330093384\n",
      "epoch:9, batch: 245,  loss: 1.1860371828079224\n",
      "epoch:9, batch: 246,  loss: 0.920394241809845\n",
      "epoch:9, batch: 247,  loss: 1.035913348197937\n",
      "epoch:9, batch: 248,  loss: 1.1290255784988403\n",
      "epoch:9, batch: 249,  loss: 0.7504380345344543\n",
      "epoch:9, batch: 250,  loss: 0.8122051954269409\n",
      "epoch:9, batch: 251,  loss: 0.9966354966163635\n",
      "epoch:9, batch: 252,  loss: 1.347049355506897\n",
      "epoch:9, batch: 253,  loss: 0.9561727643013\n",
      "epoch:9, batch: 254,  loss: 0.8155024647712708\n",
      "epoch:9, batch: 255,  loss: 1.2184213399887085\n",
      "epoch:9, batch: 256,  loss: 0.8166253566741943\n",
      "epoch:9, batch: 257,  loss: 1.110512137413025\n",
      "epoch:9, batch: 258,  loss: 0.9226459860801697\n",
      "epoch:9, batch: 259,  loss: 1.0133386850357056\n",
      "epoch:9, batch: 260,  loss: 1.326674461364746\n",
      "epoch:9, batch: 261,  loss: 1.1730804443359375\n",
      "epoch:9, batch: 262,  loss: 0.9078254103660583\n",
      "epoch:9, batch: 263,  loss: 1.0481618642807007\n",
      "epoch:9, batch: 264,  loss: 1.1779872179031372\n",
      "epoch:9, batch: 265,  loss: 0.812228262424469\n",
      "epoch:9, batch: 266,  loss: 1.0950144529342651\n",
      "epoch:9, batch: 267,  loss: 0.8968302607536316\n",
      "epoch:9, batch: 268,  loss: 0.7164211273193359\n",
      "epoch:9, batch: 269,  loss: 0.9160922765731812\n",
      "epoch:9, batch: 270,  loss: 0.8272445201873779\n",
      "epoch:9, batch: 271,  loss: 1.0842411518096924\n",
      "epoch:9, batch: 272,  loss: 1.3440626859664917\n",
      "epoch:9, batch: 273,  loss: 1.0412108898162842\n",
      "epoch:9, batch: 274,  loss: 1.04608154296875\n",
      "epoch:9, batch: 275,  loss: 0.9353748559951782\n",
      "epoch:9, batch: 276,  loss: 0.9712131023406982\n",
      "epoch:9, batch: 277,  loss: 1.188201904296875\n",
      "epoch:9, batch: 278,  loss: 1.1911484003067017\n",
      "epoch:9, batch: 279,  loss: 1.1489847898483276\n",
      "epoch:9, batch: 280,  loss: 0.9416075348854065\n",
      "epoch:9, batch: 281,  loss: 1.3334681987762451\n",
      "epoch:9, batch: 282,  loss: 1.129498839378357\n",
      "epoch:9, batch: 283,  loss: 1.493385672569275\n",
      "epoch:9, batch: 284,  loss: 0.783341109752655\n",
      "epoch:9, batch: 285,  loss: 0.9070310592651367\n",
      "epoch:9, batch: 286,  loss: 0.9794503450393677\n",
      "epoch:9, batch: 287,  loss: 1.2515207529067993\n",
      "epoch:9, batch: 288,  loss: 0.9239800572395325\n",
      "epoch:9, batch: 289,  loss: 0.9954861402511597\n",
      "epoch:9, batch: 290,  loss: 1.228833556175232\n",
      "epoch:9, batch: 291,  loss: 0.9203967452049255\n",
      "epoch:9, batch: 292,  loss: 1.3463531732559204\n",
      "epoch:9, batch: 293,  loss: 0.9749017357826233\n",
      "epoch:9, batch: 294,  loss: 1.0460567474365234\n",
      "epoch:9, batch: 295,  loss: 1.0308774709701538\n",
      "epoch:9, batch: 296,  loss: 1.0837609767913818\n",
      "epoch:9, batch: 297,  loss: 0.9452086687088013\n",
      "epoch:9, batch: 298,  loss: 0.9119348526000977\n",
      "epoch:9, batch: 299,  loss: 0.8786592483520508\n",
      "epoch:9, batch: 300,  loss: 0.882689356803894\n",
      "epoch:9, batch: 301,  loss: 1.0562682151794434\n",
      "epoch:9, batch: 302,  loss: 0.8001570701599121\n",
      "epoch:9, batch: 303,  loss: 0.9436577558517456\n",
      "epoch:9, batch: 304,  loss: 1.3750044107437134\n",
      "epoch:9, batch: 305,  loss: 0.7179005742073059\n",
      "epoch:9, batch: 306,  loss: 1.012452244758606\n",
      "epoch:9, batch: 307,  loss: 0.6064794659614563\n",
      "epoch:9, batch: 308,  loss: 0.9235341548919678\n",
      "epoch:9, batch: 309,  loss: 1.0079747438430786\n",
      "epoch:9, batch: 310,  loss: 1.3316845893859863\n",
      "epoch:9, batch: 311,  loss: 1.3679879903793335\n",
      "epoch:9, batch: 312,  loss: 0.9927449226379395\n",
      "epoch:9, batch: 313,  loss: 1.4424916505813599\n",
      "epoch:9, batch: 314,  loss: 1.1204904317855835\n",
      "epoch:9, batch: 315,  loss: 0.9738895893096924\n",
      "epoch:9, batch: 316,  loss: 1.291596531867981\n",
      "epoch:9, batch: 317,  loss: 1.105974555015564\n",
      "epoch:9, batch: 318,  loss: 1.1735167503356934\n",
      "epoch:9, batch: 319,  loss: 1.0775851011276245\n",
      "epoch:9, batch: 320,  loss: 0.9950771331787109\n",
      "epoch:9, batch: 321,  loss: 1.100719928741455\n",
      "epoch:9, batch: 322,  loss: 0.9382092952728271\n",
      "epoch:9, batch: 323,  loss: 0.8347896933555603\n",
      "epoch:9, batch: 324,  loss: 0.9231292605400085\n",
      "epoch:9, batch: 325,  loss: 0.9730565547943115\n",
      "epoch:9, batch: 326,  loss: 0.7255080938339233\n",
      "epoch:9, batch: 327,  loss: 0.9402678608894348\n",
      "epoch:9, batch: 328,  loss: 0.9570237398147583\n",
      "epoch:9, batch: 329,  loss: 0.8701654076576233\n",
      "epoch:9, batch: 330,  loss: 1.0989305973052979\n",
      "epoch:9, batch: 331,  loss: 0.9108378291130066\n",
      "epoch:9, batch: 332,  loss: 0.9733691811561584\n",
      "epoch:9, batch: 333,  loss: 1.3051936626434326\n",
      "epoch:9, batch: 334,  loss: 1.159921646118164\n",
      "epoch:9, batch: 335,  loss: 1.0867351293563843\n",
      "epoch:9, batch: 336,  loss: 0.8059126734733582\n",
      "epoch:9, batch: 337,  loss: 1.0575706958770752\n",
      "epoch:9, batch: 338,  loss: 1.2190849781036377\n",
      "epoch:9, batch: 339,  loss: 0.8862698674201965\n",
      "epoch:9, batch: 340,  loss: 1.0765538215637207\n",
      "epoch:9, batch: 341,  loss: 1.3141423463821411\n",
      "epoch:9, batch: 342,  loss: 1.218427300453186\n",
      "epoch:9, batch: 343,  loss: 0.9116713404655457\n",
      "epoch:9, batch: 344,  loss: 0.7817588448524475\n",
      "epoch:9, batch: 345,  loss: 1.182837963104248\n",
      "epoch:9, batch: 346,  loss: 1.0600159168243408\n",
      "epoch:9, batch: 347,  loss: 0.7566198110580444\n",
      "epoch:9, batch: 348,  loss: 1.3532065153121948\n",
      "epoch:9, batch: 349,  loss: 0.7763480544090271\n",
      "epoch:9, batch: 350,  loss: 0.9956001043319702\n",
      "epoch:9, batch: 351,  loss: 0.9028089046478271\n",
      "epoch:9, batch: 352,  loss: 1.3886432647705078\n",
      "epoch:9, batch: 353,  loss: 0.8688480854034424\n",
      "epoch:9, batch: 354,  loss: 1.0413583517074585\n",
      "epoch:9, batch: 355,  loss: 1.1741493940353394\n",
      "epoch:9, batch: 356,  loss: 1.158474326133728\n",
      "epoch:9, batch: 357,  loss: 1.1167794466018677\n",
      "epoch:9, batch: 358,  loss: 1.2880899906158447\n",
      "epoch:9, batch: 359,  loss: 1.3558297157287598\n",
      "epoch:9, batch: 360,  loss: 1.1085302829742432\n",
      "epoch:9, batch: 361,  loss: 1.038225769996643\n",
      "epoch:9, batch: 362,  loss: 0.8944429755210876\n",
      "epoch:9, batch: 363,  loss: 0.9453452229499817\n",
      "epoch:9, batch: 364,  loss: 0.962080717086792\n",
      "epoch:9, batch: 365,  loss: 0.8418881893157959\n",
      "epoch:9, batch: 366,  loss: 1.1890498399734497\n",
      "epoch:9, batch: 367,  loss: 1.0981525182724\n",
      "epoch:9, batch: 368,  loss: 1.048338532447815\n",
      "epoch:9, batch: 369,  loss: 1.067729115486145\n",
      "epoch:9, batch: 370,  loss: 1.0332459211349487\n",
      "epoch:9, batch: 371,  loss: 0.9064885377883911\n",
      "epoch:9, batch: 372,  loss: 1.2861112356185913\n",
      "epoch:9, batch: 373,  loss: 1.342405080795288\n",
      "epoch:9, batch: 374,  loss: 1.0572781562805176\n",
      "epoch:9, batch: 375,  loss: 0.9554548263549805\n",
      "epoch:9, batch: 376,  loss: 1.3256632089614868\n",
      "epoch:9, batch: 377,  loss: 0.7099645733833313\n",
      "epoch:9, batch: 378,  loss: 0.6784916520118713\n",
      "epoch:9, batch: 379,  loss: 0.9663347601890564\n",
      "epoch:9, batch: 380,  loss: 0.858464241027832\n",
      "epoch:9, batch: 381,  loss: 1.0726563930511475\n",
      "epoch:9, batch: 382,  loss: 0.7997710704803467\n",
      "epoch:9, batch: 383,  loss: 0.9658350348472595\n",
      "epoch:9, batch: 384,  loss: 1.13674795627594\n",
      "epoch:9, batch: 385,  loss: 1.1253468990325928\n",
      "epoch:9, batch: 386,  loss: 0.9827292561531067\n",
      "epoch:9, batch: 387,  loss: 1.2635425329208374\n",
      "epoch:9, batch: 388,  loss: 0.9450710415840149\n",
      "epoch:9, batch: 389,  loss: 0.931986391544342\n",
      "epoch:9, batch: 390,  loss: 0.8393298983573914\n",
      "epoch:9, batch: 391,  loss: 1.0471467971801758\n",
      "epoch:9, batch: 392,  loss: 0.8509783148765564\n",
      "epoch:9, batch: 393,  loss: 0.9592130780220032\n",
      "epoch:9, batch: 394,  loss: 1.0603336095809937\n",
      "epoch:9, batch: 395,  loss: 1.0078697204589844\n",
      "epoch:9, batch: 396,  loss: 1.0516321659088135\n",
      "epoch:9, batch: 397,  loss: 1.0342565774917603\n",
      "epoch:9, batch: 398,  loss: 0.8426408171653748\n",
      "epoch:9, batch: 399,  loss: 1.1189312934875488\n",
      "epoch:9, batch: 400,  loss: 1.3104959726333618\n",
      "epoch:9, batch: 401,  loss: 0.8276901841163635\n",
      "epoch:9, batch: 402,  loss: 0.9291132688522339\n",
      "epoch:9, batch: 403,  loss: 1.186779260635376\n",
      "epoch:9, batch: 404,  loss: 1.2885255813598633\n",
      "epoch:9, batch: 405,  loss: 1.0253814458847046\n",
      "epoch:9, batch: 406,  loss: 1.1507389545440674\n",
      "epoch:9, batch: 407,  loss: 1.0047764778137207\n",
      "epoch:9, batch: 408,  loss: 1.0746238231658936\n",
      "epoch:9, batch: 409,  loss: 0.6547480225563049\n",
      "epoch:9, batch: 410,  loss: 0.8324342966079712\n",
      "epoch:9, batch: 411,  loss: 1.0427429676055908\n",
      "epoch:9, batch: 412,  loss: 1.0474153757095337\n",
      "epoch:9, batch: 413,  loss: 1.1038293838500977\n",
      "epoch:9, batch: 414,  loss: 1.2454601526260376\n",
      "epoch:9, batch: 415,  loss: 1.1245448589324951\n",
      "epoch:9, batch: 416,  loss: 0.936456024646759\n",
      "epoch:9, batch: 417,  loss: 1.1678881645202637\n",
      "epoch:9, batch: 418,  loss: 1.1733367443084717\n",
      "epoch:9, batch: 419,  loss: 0.8073374032974243\n",
      "epoch:9, batch: 420,  loss: 0.9916085004806519\n",
      "epoch:9, batch: 421,  loss: 0.9857616424560547\n",
      "epoch:9, batch: 422,  loss: 1.0786875486373901\n",
      "epoch:9, batch: 423,  loss: 0.8328268527984619\n",
      "epoch:9, batch: 424,  loss: 1.2329180240631104\n",
      "epoch:9, batch: 425,  loss: 1.232192873954773\n",
      "epoch:9, batch: 426,  loss: 1.164182424545288\n",
      "epoch:9, batch: 427,  loss: 1.3807190656661987\n",
      "epoch:9, batch: 428,  loss: 1.0055134296417236\n",
      "epoch:9, batch: 429,  loss: 0.9562867879867554\n",
      "epoch:9, batch: 430,  loss: 1.0758061408996582\n",
      "epoch:9, batch: 431,  loss: 0.6619254350662231\n",
      "epoch:9, batch: 432,  loss: 1.105750322341919\n",
      "epoch:9, batch: 433,  loss: 1.1988645792007446\n",
      "epoch:9, batch: 434,  loss: 0.8082239031791687\n",
      "epoch:9, batch: 435,  loss: 1.097394347190857\n",
      "epoch:9, batch: 436,  loss: 0.8455893993377686\n",
      "epoch:9, batch: 437,  loss: 1.2318168878555298\n",
      "epoch:9, batch: 438,  loss: 1.0411427021026611\n",
      "epoch:9, batch: 439,  loss: 0.9077205657958984\n",
      "epoch:9, batch: 440,  loss: 1.1109211444854736\n",
      "epoch:9, batch: 441,  loss: 1.4909954071044922\n",
      "epoch:9, batch: 442,  loss: 0.8841432332992554\n",
      "epoch:9, batch: 443,  loss: 0.9448715448379517\n",
      "epoch:9, batch: 444,  loss: 0.9753837585449219\n",
      "epoch:9, batch: 445,  loss: 1.2967572212219238\n",
      "epoch:9, batch: 446,  loss: 1.0519908666610718\n",
      "epoch:9, batch: 447,  loss: 0.952663004398346\n",
      "epoch:9, batch: 448,  loss: 1.442789912223816\n",
      "epoch:9, batch: 449,  loss: 1.295729398727417\n",
      "epoch:9, batch: 450,  loss: 1.055007815361023\n",
      "epoch:9, batch: 451,  loss: 0.668898344039917\n",
      "epoch:9, batch: 452,  loss: 0.7464562058448792\n",
      "epoch:9, batch: 453,  loss: 1.05784273147583\n",
      "epoch:9, batch: 454,  loss: 0.9027709364891052\n",
      "epoch:9, batch: 455,  loss: 0.8484603762626648\n",
      "epoch:9, batch: 456,  loss: 1.6164785623550415\n",
      "epoch:9, batch: 457,  loss: 1.3442474603652954\n",
      "epoch:9, batch: 458,  loss: 1.0249273777008057\n",
      "epoch:9, batch: 459,  loss: 0.8516242504119873\n",
      "epoch:9, batch: 460,  loss: 0.9345664978027344\n",
      "epoch:9, batch: 461,  loss: 0.7451584935188293\n",
      "epoch:9, batch: 462,  loss: 1.3617016077041626\n",
      "epoch:9, batch: 463,  loss: 1.2737277746200562\n",
      "epoch:9, batch: 464,  loss: 1.1293644905090332\n",
      "epoch:9, batch: 465,  loss: 1.0613658428192139\n",
      "epoch:9, batch: 466,  loss: 1.2259938716888428\n",
      "epoch:9, batch: 467,  loss: 0.889106273651123\n",
      "epoch:9, batch: 468,  loss: 1.0615475177764893\n",
      "epoch:9, batch: 469,  loss: 1.0927298069000244\n",
      "epoch:9, batch: 470,  loss: 1.157135248184204\n",
      "epoch:9, batch: 471,  loss: 0.8938076496124268\n",
      "epoch:9, batch: 472,  loss: 1.4699852466583252\n",
      "epoch:9, batch: 473,  loss: 1.0043327808380127\n",
      "epoch:9, batch: 474,  loss: 0.91312575340271\n",
      "epoch:9, batch: 475,  loss: 1.1521987915039062\n",
      "epoch:9, batch: 476,  loss: 1.346841812133789\n",
      "epoch:9, batch: 477,  loss: 1.222893476486206\n",
      "epoch:9, batch: 478,  loss: 1.055689811706543\n",
      "epoch:9, batch: 479,  loss: 1.0286095142364502\n",
      "epoch:9, batch: 480,  loss: 0.9919298887252808\n",
      "epoch:9, batch: 481,  loss: 1.0475999116897583\n",
      "epoch:9, batch: 482,  loss: 1.0123610496520996\n",
      "epoch:9, batch: 483,  loss: 1.2690308094024658\n",
      "epoch:9, batch: 484,  loss: 0.9416401982307434\n",
      "epoch:9, batch: 485,  loss: 0.7343893051147461\n",
      "epoch:9, batch: 486,  loss: 1.150105357170105\n",
      "epoch:9, batch: 487,  loss: 0.7851921319961548\n",
      "epoch:9, batch: 488,  loss: 1.2854598760604858\n",
      "epoch:9, batch: 489,  loss: 0.9718390703201294\n",
      "epoch:9, batch: 490,  loss: 0.8283671140670776\n",
      "epoch:9, batch: 491,  loss: 1.1568982601165771\n",
      "epoch:9, batch: 492,  loss: 0.9645203351974487\n",
      "epoch:9, batch: 493,  loss: 1.08948814868927\n",
      "epoch:9, batch: 494,  loss: 1.0746492147445679\n",
      "epoch:9, batch: 495,  loss: 0.8660596609115601\n",
      "epoch:9, batch: 496,  loss: 1.3659296035766602\n",
      "epoch:9, batch: 497,  loss: 0.9294140934944153\n",
      "epoch:9, batch: 498,  loss: 0.9007970690727234\n",
      "epoch:9, batch: 499,  loss: 1.2805094718933105\n",
      "epoch:9, batch: 500,  loss: 0.8844968676567078\n",
      "epoch:9, batch: 501,  loss: 0.9649695754051208\n",
      "epoch:9, batch: 502,  loss: 1.1149861812591553\n",
      "epoch:9, batch: 503,  loss: 1.028274416923523\n",
      "epoch:9, batch: 504,  loss: 0.7457879781723022\n",
      "epoch:9, batch: 505,  loss: 1.0498428344726562\n",
      "epoch:9, batch: 506,  loss: 0.7389597296714783\n",
      "epoch:9, batch: 507,  loss: 1.1616878509521484\n",
      "epoch:9, batch: 508,  loss: 0.8219473361968994\n",
      "epoch:9, batch: 509,  loss: 1.1085666418075562\n",
      "epoch:9, batch: 510,  loss: 0.9288586974143982\n",
      "epoch:9, batch: 511,  loss: 1.3101518154144287\n",
      "epoch:9, batch: 512,  loss: 1.0733537673950195\n",
      "epoch:9, batch: 513,  loss: 0.9699823260307312\n",
      "epoch:9, batch: 514,  loss: 1.1987202167510986\n",
      "epoch:9, batch: 515,  loss: 0.931852400302887\n",
      "epoch:9, batch: 516,  loss: 1.1021286249160767\n",
      "epoch:9, batch: 517,  loss: 0.9757230281829834\n",
      "epoch:9, batch: 518,  loss: 1.1271345615386963\n",
      "epoch:9, batch: 519,  loss: 1.0215108394622803\n",
      "epoch:9, batch: 520,  loss: 0.8620240092277527\n",
      "epoch:9, batch: 521,  loss: 1.0936886072158813\n",
      "epoch:9, batch: 522,  loss: 1.2690796852111816\n",
      "epoch:9, batch: 523,  loss: 0.8959609866142273\n",
      "epoch:9, batch: 524,  loss: 0.8573862910270691\n",
      "epoch:9, batch: 525,  loss: 1.2634968757629395\n",
      "epoch:9, batch: 526,  loss: 1.184493064880371\n",
      "epoch:9, batch: 527,  loss: 0.9839671850204468\n",
      "epoch:9, batch: 528,  loss: 1.039409875869751\n",
      "epoch:9, batch: 529,  loss: 1.3149197101593018\n",
      "epoch:9, batch: 530,  loss: 1.1500816345214844\n",
      "epoch:9, batch: 531,  loss: 1.055881142616272\n",
      "epoch:9, batch: 532,  loss: 0.8490874767303467\n",
      "epoch:9, batch: 533,  loss: 0.868015706539154\n",
      "epoch:9, batch: 534,  loss: 0.647983729839325\n",
      "epoch:9, batch: 535,  loss: 0.7998713850975037\n",
      "epoch:9, batch: 536,  loss: 1.0492773056030273\n",
      "epoch:9, batch: 537,  loss: 0.9955068230628967\n",
      "epoch:9, batch: 538,  loss: 0.9163491725921631\n",
      "epoch:9, batch: 539,  loss: 0.9976024627685547\n",
      "epoch:9, batch: 540,  loss: 1.1625556945800781\n",
      "epoch:9, batch: 541,  loss: 0.940894365310669\n",
      "epoch:9, batch: 542,  loss: 0.95164555311203\n",
      "epoch:9, batch: 543,  loss: 1.0238231420516968\n",
      "epoch:9, batch: 544,  loss: 1.1144312620162964\n",
      "epoch:9, batch: 545,  loss: 1.189712405204773\n",
      "epoch:9, batch: 546,  loss: 0.9911075234413147\n",
      "epoch:9, batch: 547,  loss: 1.361118197441101\n",
      "epoch:9, batch: 548,  loss: 1.2484562397003174\n",
      "epoch:9, batch: 549,  loss: 0.8578703999519348\n",
      "epoch:9, batch: 550,  loss: 1.116868019104004\n",
      "epoch:9, batch: 551,  loss: 0.8935810923576355\n",
      "epoch:9, batch: 552,  loss: 1.0898765325546265\n",
      "epoch:9, batch: 553,  loss: 0.7586290836334229\n",
      "epoch:9, batch: 554,  loss: 1.0452414751052856\n",
      "epoch:9, batch: 555,  loss: 1.5015594959259033\n",
      "epoch:9, batch: 556,  loss: 0.7569555640220642\n",
      "epoch:9, batch: 557,  loss: 0.9208148717880249\n",
      "epoch:9, batch: 558,  loss: 1.0075061321258545\n",
      "epoch:9, batch: 559,  loss: 0.7959327697753906\n",
      "epoch:9, batch: 560,  loss: 0.6506568193435669\n",
      "epoch:9, batch: 561,  loss: 1.1409003734588623\n",
      "epoch:9, batch: 562,  loss: 0.9146283864974976\n",
      "epoch:9, batch: 563,  loss: 0.9167120456695557\n",
      "epoch:9, batch: 564,  loss: 1.0997169017791748\n",
      "epoch:9, batch: 565,  loss: 1.2075952291488647\n",
      "epoch:9, batch: 566,  loss: 0.7511255741119385\n",
      "epoch:9, batch: 567,  loss: 0.8480905890464783\n",
      "epoch:9, batch: 568,  loss: 1.201530933380127\n",
      "epoch:9, batch: 569,  loss: 1.221398115158081\n",
      "epoch:9, batch: 570,  loss: 0.8647531270980835\n",
      "epoch:9, batch: 571,  loss: 1.2314839363098145\n",
      "epoch:9, batch: 572,  loss: 1.0707664489746094\n",
      "epoch:9, batch: 573,  loss: 0.8977568745613098\n",
      "epoch:9, batch: 574,  loss: 1.0382026433944702\n",
      "epoch:9, batch: 575,  loss: 1.0224082469940186\n",
      "epoch:9, batch: 576,  loss: 1.0774105787277222\n",
      "epoch:9, batch: 577,  loss: 1.0095783472061157\n",
      "epoch:9, batch: 578,  loss: 0.8599331974983215\n",
      "epoch:9, batch: 579,  loss: 1.2499552965164185\n",
      "epoch:9, batch: 580,  loss: 1.3961153030395508\n",
      "epoch:9, batch: 581,  loss: 0.9979069828987122\n",
      "epoch:9, batch: 582,  loss: 0.8187030553817749\n",
      "epoch:9, batch: 583,  loss: 1.2538362741470337\n",
      "epoch:9, batch: 584,  loss: 1.2135326862335205\n",
      "epoch:9, batch: 585,  loss: 0.9111922383308411\n",
      "epoch:9, batch: 586,  loss: 1.453247308731079\n",
      "epoch:9, batch: 587,  loss: 1.0163601636886597\n",
      "epoch:9, batch: 588,  loss: 1.1467818021774292\n",
      "epoch:9, batch: 589,  loss: 0.9960961937904358\n",
      "epoch:9, batch: 590,  loss: 0.9961665272712708\n",
      "epoch:9, batch: 591,  loss: 1.022125005722046\n",
      "epoch:9, batch: 592,  loss: 0.7825993299484253\n",
      "epoch:9, batch: 593,  loss: 1.2182942628860474\n",
      "epoch:9, batch: 594,  loss: 1.0498430728912354\n",
      "epoch:9, batch: 595,  loss: 0.8813220858573914\n",
      "epoch:9, batch: 596,  loss: 0.8589393496513367\n",
      "epoch:9, batch: 597,  loss: 1.5174171924591064\n",
      "epoch:9, batch: 598,  loss: 0.9002111554145813\n",
      "epoch:9, batch: 599,  loss: 1.0714319944381714\n",
      "epoch:9, batch: 600,  loss: 0.9161074757575989\n",
      "epoch:9, batch: 601,  loss: 1.4965425729751587\n",
      "epoch:9, batch: 602,  loss: 1.2104548215866089\n",
      "epoch:9, batch: 603,  loss: 0.6760083436965942\n",
      "epoch:9, batch: 604,  loss: 0.9359437227249146\n",
      "epoch:9, batch: 605,  loss: 1.3512117862701416\n",
      "epoch:9, batch: 606,  loss: 0.8748459815979004\n",
      "epoch:9, batch: 607,  loss: 1.0548820495605469\n",
      "epoch:9, batch: 608,  loss: 0.926676869392395\n",
      "epoch:9, batch: 609,  loss: 1.003600001335144\n",
      "epoch:9, batch: 610,  loss: 1.3244930505752563\n",
      "epoch:9, batch: 611,  loss: 0.9527544975280762\n",
      "epoch:9, batch: 612,  loss: 1.1714378595352173\n",
      "epoch:9, batch: 613,  loss: 0.7560915946960449\n",
      "epoch:9, batch: 614,  loss: 1.0146703720092773\n",
      "epoch:9, batch: 615,  loss: 0.8773793578147888\n",
      "epoch:9, batch: 616,  loss: 0.9489762783050537\n",
      "epoch:9, batch: 617,  loss: 1.3006422519683838\n",
      "epoch:9, batch: 618,  loss: 1.189210295677185\n",
      "epoch:9, batch: 619,  loss: 1.1502389907836914\n",
      "epoch:9, batch: 620,  loss: 0.8001977205276489\n",
      "epoch:9, batch: 621,  loss: 1.358163833618164\n",
      "epoch:9, batch: 622,  loss: 1.1313540935516357\n",
      "epoch:9, batch: 623,  loss: 1.0088539123535156\n",
      "epoch:9, batch: 624,  loss: 0.94301837682724\n",
      "epoch:9, batch: 625,  loss: 0.7493914365768433\n",
      "epoch:9, batch: 626,  loss: 0.9436467289924622\n",
      "epoch:9, batch: 627,  loss: 0.9989036917686462\n",
      "epoch:9, batch: 628,  loss: 1.1308225393295288\n",
      "epoch:9, batch: 629,  loss: 1.324178695678711\n",
      "epoch:9, batch: 630,  loss: 1.0648174285888672\n",
      "epoch:9, batch: 631,  loss: 1.0155622959136963\n",
      "epoch:9, batch: 632,  loss: 0.8879112601280212\n",
      "epoch:9, batch: 633,  loss: 1.1088550090789795\n",
      "epoch:9, batch: 634,  loss: 1.232112169265747\n",
      "epoch:9, batch: 635,  loss: 0.9079816937446594\n",
      "epoch:9, batch: 636,  loss: 0.9478368163108826\n",
      "epoch:9, batch: 637,  loss: 1.0472068786621094\n",
      "epoch:9, batch: 638,  loss: 1.1853471994400024\n",
      "epoch:9, batch: 639,  loss: 1.0568420886993408\n",
      "epoch:9, batch: 640,  loss: 1.0329501628875732\n",
      "epoch:9, batch: 641,  loss: 1.1285673379898071\n",
      "epoch:9, batch: 642,  loss: 1.1311166286468506\n",
      "epoch:9, batch: 643,  loss: 1.1305079460144043\n",
      "epoch:9, batch: 644,  loss: 0.9611284732818604\n",
      "epoch:9, batch: 645,  loss: 1.057801604270935\n",
      "epoch:9, batch: 646,  loss: 1.171405553817749\n",
      "epoch:9, batch: 647,  loss: 0.9707751274108887\n",
      "epoch:9, batch: 648,  loss: 1.035208821296692\n",
      "epoch:9, batch: 649,  loss: 0.871760904788971\n",
      "epoch:9, batch: 650,  loss: 1.1552643775939941\n",
      "epoch:9, batch: 651,  loss: 1.0427496433258057\n",
      "epoch:9, batch: 652,  loss: 0.8057385683059692\n",
      "epoch:9, batch: 653,  loss: 1.0351762771606445\n",
      "epoch:9, batch: 654,  loss: 0.9568699598312378\n",
      "epoch:9, batch: 655,  loss: 1.199837565422058\n",
      "epoch:9, batch: 656,  loss: 1.0227165222167969\n",
      "epoch:9, batch: 657,  loss: 1.1208785772323608\n",
      "epoch:9, batch: 658,  loss: 0.7390435338020325\n",
      "epoch:9, batch: 659,  loss: 0.8253713250160217\n",
      "epoch:9, batch: 660,  loss: 1.2653203010559082\n",
      "epoch:9, batch: 661,  loss: 1.0622318983078003\n",
      "epoch:9, batch: 662,  loss: 1.2472666501998901\n",
      "epoch:9, batch: 663,  loss: 1.1765724420547485\n",
      "epoch:9, batch: 664,  loss: 1.121768593788147\n",
      "epoch:9, batch: 665,  loss: 1.0586249828338623\n",
      "epoch:9, batch: 666,  loss: 1.309005856513977\n",
      "epoch:9, batch: 667,  loss: 1.331563949584961\n",
      "epoch:9, batch: 668,  loss: 1.1007968187332153\n",
      "epoch:9, batch: 669,  loss: 1.1600664854049683\n",
      "epoch:9, batch: 670,  loss: 1.2598323822021484\n",
      "epoch:9, batch: 671,  loss: 0.9019218683242798\n",
      "epoch:9, batch: 672,  loss: 0.8541644215583801\n",
      "epoch:9, batch: 673,  loss: 1.0948809385299683\n",
      "epoch:9, batch: 674,  loss: 1.1256890296936035\n",
      "epoch:9, batch: 675,  loss: 0.8585923910140991\n",
      "epoch:9, batch: 676,  loss: 1.389646291732788\n",
      "epoch:9, batch: 677,  loss: 0.8934246897697449\n",
      "epoch:9, batch: 678,  loss: 0.8393717408180237\n",
      "epoch:9, batch: 679,  loss: 1.202026128768921\n",
      "epoch:9, batch: 680,  loss: 1.0997687578201294\n",
      "epoch:9, batch: 681,  loss: 0.9736289978027344\n",
      "epoch:9, batch: 682,  loss: 1.4928462505340576\n",
      "epoch:9, batch: 683,  loss: 0.7552965879440308\n",
      "epoch:9, batch: 684,  loss: 1.2860304117202759\n",
      "epoch:9, batch: 685,  loss: 0.846972644329071\n",
      "epoch:9, batch: 686,  loss: 0.7164146900177002\n",
      "epoch:9, batch: 687,  loss: 1.3793739080429077\n",
      "epoch:9, batch: 688,  loss: 1.1110727787017822\n",
      "epoch:9, batch: 689,  loss: 1.128732442855835\n",
      "epoch:9, batch: 690,  loss: 1.0967254638671875\n",
      "epoch:9, batch: 691,  loss: 1.1900699138641357\n",
      "epoch:9, batch: 692,  loss: 0.9912697076797485\n",
      "epoch:9, batch: 693,  loss: 0.6869977116584778\n",
      "epoch:9, batch: 694,  loss: 1.2425657510757446\n",
      "epoch:9, batch: 695,  loss: 0.827634334564209\n",
      "epoch:9, batch: 696,  loss: 1.0136231184005737\n",
      "epoch:9, batch: 697,  loss: 0.8504401445388794\n",
      "epoch:9, batch: 698,  loss: 0.9066765904426575\n",
      "epoch:9, batch: 699,  loss: 1.0943357944488525\n",
      "epoch:9, batch: 700,  loss: 1.025255799293518\n",
      "epoch:9, batch: 701,  loss: 0.9560427665710449\n",
      "epoch:9, batch: 702,  loss: 1.068428635597229\n",
      "epoch:9, batch: 703,  loss: 1.2910786867141724\n",
      "epoch:9, batch: 704,  loss: 1.2619775533676147\n",
      "epoch:9, batch: 705,  loss: 1.271203637123108\n",
      "epoch:9, batch: 706,  loss: 0.9260839819908142\n",
      "epoch:9, batch: 707,  loss: 1.0201835632324219\n",
      "epoch:9, batch: 708,  loss: 1.0305489301681519\n",
      "epoch:9, batch: 709,  loss: 1.0470370054244995\n",
      "epoch:9, batch: 710,  loss: 0.8905432224273682\n",
      "epoch:9, batch: 711,  loss: 0.7368026375770569\n",
      "epoch:9, batch: 712,  loss: 0.7662264108657837\n",
      "epoch:9, batch: 713,  loss: 0.9946870803833008\n",
      "epoch:9, batch: 714,  loss: 0.9404940605163574\n",
      "epoch:9, batch: 715,  loss: 1.258347749710083\n",
      "epoch:9, batch: 716,  loss: 1.1025381088256836\n",
      "epoch:9, batch: 717,  loss: 0.8397864103317261\n",
      "epoch:9, batch: 718,  loss: 1.0321898460388184\n",
      "epoch:9, batch: 719,  loss: 1.0249309539794922\n",
      "epoch:9, batch: 720,  loss: 0.8381100296974182\n",
      "epoch:9, batch: 721,  loss: 0.9354304075241089\n",
      "epoch:9, batch: 722,  loss: 1.1357942819595337\n",
      "epoch:9, batch: 723,  loss: 1.3349344730377197\n",
      "epoch:9, batch: 724,  loss: 0.9776301383972168\n",
      "epoch:9, batch: 725,  loss: 0.9527004957199097\n",
      "epoch:9, batch: 726,  loss: 1.2101389169692993\n",
      "epoch:9, batch: 727,  loss: 0.9423239827156067\n",
      "epoch:9, batch: 728,  loss: 1.1735754013061523\n",
      "epoch:9, batch: 729,  loss: 0.8085647225379944\n",
      "epoch:9, batch: 730,  loss: 0.8239085078239441\n",
      "epoch:9, batch: 731,  loss: 0.9323745369911194\n",
      "epoch:9, batch: 732,  loss: 1.1003237962722778\n",
      "epoch:9, batch: 733,  loss: 1.1636576652526855\n",
      "epoch:9, batch: 734,  loss: 1.1155108213424683\n",
      "epoch:9, batch: 735,  loss: 1.5219595432281494\n",
      "epoch:9, batch: 736,  loss: 1.0387698411941528\n",
      "epoch:9, batch: 737,  loss: 0.8062115907669067\n",
      "epoch:9, batch: 738,  loss: 0.9174211621284485\n",
      "epoch:9, batch: 739,  loss: 1.0780978202819824\n",
      "epoch:9, batch: 740,  loss: 1.1073909997940063\n",
      "epoch:9, batch: 741,  loss: 1.3123805522918701\n",
      "epoch:9, batch: 742,  loss: 0.9069875478744507\n",
      "epoch:9, batch: 743,  loss: 1.2820478677749634\n",
      "epoch:9, batch: 744,  loss: 1.2711727619171143\n",
      "epoch:9, batch: 745,  loss: 1.0854853391647339\n",
      "epoch:9, batch: 746,  loss: 0.7286955118179321\n",
      "epoch:9, batch: 747,  loss: 1.2341148853302002\n",
      "epoch:9, batch: 748,  loss: 1.008228063583374\n",
      "epoch:9, batch: 749,  loss: 0.7214007377624512\n",
      "epoch:9, batch: 750,  loss: 1.4239223003387451\n",
      "epoch:9, batch: 751,  loss: 0.8966213464736938\n",
      "epoch:9, batch: 752,  loss: 1.1485803127288818\n",
      "epoch:9, batch: 753,  loss: 1.2343461513519287\n",
      "epoch:9, batch: 754,  loss: 1.0792863368988037\n",
      "epoch:9, batch: 755,  loss: 0.9021896719932556\n",
      "epoch:9, batch: 756,  loss: 1.2577648162841797\n",
      "epoch:9, batch: 757,  loss: 0.8869389295578003\n",
      "epoch:9, batch: 758,  loss: 1.2376827001571655\n",
      "epoch:9, batch: 759,  loss: 1.2117698192596436\n",
      "epoch:9, batch: 760,  loss: 1.1899833679199219\n",
      "epoch:9, batch: 761,  loss: 0.8979871273040771\n",
      "epoch:9, batch: 762,  loss: 1.0034575462341309\n",
      "epoch:9, batch: 763,  loss: 1.0131748914718628\n",
      "epoch:9, batch: 764,  loss: 1.110906720161438\n",
      "epoch:9, batch: 765,  loss: 0.8125899434089661\n",
      "epoch:9, batch: 766,  loss: 0.9273332953453064\n",
      "epoch:9, batch: 767,  loss: 0.7462130784988403\n",
      "epoch:9, batch: 768,  loss: 1.0483750104904175\n",
      "epoch:9, batch: 769,  loss: 0.7243916988372803\n",
      "epoch:9, batch: 770,  loss: 0.9583834409713745\n",
      "epoch:9, batch: 771,  loss: 1.0071088075637817\n",
      "epoch:9, batch: 772,  loss: 0.8700098395347595\n",
      "epoch:9, batch: 773,  loss: 0.9294885396957397\n",
      "epoch:9, batch: 774,  loss: 0.9862965941429138\n",
      "epoch:9, batch: 775,  loss: 1.1377516984939575\n",
      "epoch:9, batch: 776,  loss: 1.0302621126174927\n",
      "epoch:9, batch: 777,  loss: 1.0546371936798096\n",
      "epoch:9, batch: 778,  loss: 1.173029899597168\n",
      "epoch:9, batch: 779,  loss: 1.1610803604125977\n",
      "epoch:9, batch: 780,  loss: 1.0174686908721924\n",
      "epoch:9, batch: 781,  loss: 0.8737363815307617\n",
      "epoch:9, batch: 782,  loss: 0.7023380994796753\n",
      "epoch:9, batch: 783,  loss: 0.9067978262901306\n",
      "epoch:9, batch: 784,  loss: 1.0784960985183716\n",
      "epoch:9, batch: 785,  loss: 1.1852887868881226\n",
      "epoch:9, batch: 786,  loss: 1.1455011367797852\n",
      "epoch:9, batch: 787,  loss: 0.8134815096855164\n",
      "epoch:9, batch: 788,  loss: 0.7232996821403503\n",
      "epoch:9, batch: 789,  loss: 1.432346224784851\n",
      "epoch:9, batch: 790,  loss: 0.8045721054077148\n",
      "epoch:9, batch: 791,  loss: 1.085121512413025\n",
      "epoch:9, batch: 792,  loss: 0.7558152675628662\n",
      "epoch:9, batch: 793,  loss: 1.1807750463485718\n",
      "epoch:9, batch: 794,  loss: 0.8718488216400146\n",
      "epoch:9, batch: 795,  loss: 1.0381814241409302\n",
      "epoch:9, batch: 796,  loss: 0.9634099006652832\n",
      "epoch:9, batch: 797,  loss: 0.8228406310081482\n",
      "epoch:9, batch: 798,  loss: 0.8808845281600952\n",
      "epoch:9, batch: 799,  loss: 0.9637347459793091\n",
      "epoch:9, batch: 800,  loss: 0.8575284481048584\n",
      "epoch:9, batch: 801,  loss: 1.1112511157989502\n",
      "epoch:9, batch: 802,  loss: 1.1058294773101807\n",
      "epoch:9, batch: 803,  loss: 0.7292860150337219\n",
      "epoch:9, batch: 804,  loss: 1.0499542951583862\n",
      "epoch:9, batch: 805,  loss: 1.0082722902297974\n",
      "epoch:9, batch: 806,  loss: 0.9249479174613953\n",
      "epoch:9, batch: 807,  loss: 0.9352668523788452\n",
      "epoch:9, batch: 808,  loss: 1.0286840200424194\n",
      "epoch:9, batch: 809,  loss: 0.8993147611618042\n",
      "epoch:9, batch: 810,  loss: 1.1285663843154907\n",
      "epoch:9, batch: 811,  loss: 0.8574790954589844\n",
      "epoch:9, batch: 812,  loss: 1.3452621698379517\n",
      "epoch:9, batch: 813,  loss: 1.0719228982925415\n",
      "epoch:9, batch: 814,  loss: 1.0203266143798828\n",
      "epoch:9, batch: 815,  loss: 0.893524169921875\n",
      "epoch:9, batch: 816,  loss: 0.8160924911499023\n",
      "epoch:9, batch: 817,  loss: 1.0505155324935913\n",
      "epoch:9, batch: 818,  loss: 1.4135615825653076\n",
      "epoch:9, batch: 819,  loss: 1.0848606824874878\n",
      "epoch:9, batch: 820,  loss: 0.9595433473587036\n",
      "epoch:9, batch: 821,  loss: 1.4223787784576416\n",
      "epoch:9, batch: 822,  loss: 0.8792992830276489\n",
      "epoch:9, batch: 823,  loss: 0.9441483020782471\n",
      "epoch:9, batch: 824,  loss: 1.6146970987319946\n",
      "epoch:9, batch: 825,  loss: 0.686009407043457\n",
      "epoch:9, batch: 826,  loss: 0.8965663313865662\n",
      "epoch:9, batch: 827,  loss: 1.0955753326416016\n",
      "epoch:9, batch: 828,  loss: 1.030055046081543\n",
      "epoch:9, batch: 829,  loss: 1.0306421518325806\n",
      "epoch:9, batch: 830,  loss: 0.866075336933136\n",
      "epoch:9, batch: 831,  loss: 1.1318206787109375\n",
      "epoch:9, batch: 832,  loss: 1.273743987083435\n",
      "epoch:9, batch: 833,  loss: 0.8889460563659668\n",
      "epoch:9, batch: 834,  loss: 1.2238051891326904\n",
      "epoch:9, batch: 835,  loss: 1.0501984357833862\n",
      "epoch:9, batch: 836,  loss: 1.0336296558380127\n",
      "epoch:9, batch: 837,  loss: 1.3333274126052856\n",
      "epoch:9, batch: 838,  loss: 1.0372213125228882\n",
      "epoch:9, batch: 839,  loss: 1.1671569347381592\n",
      "epoch:9, batch: 840,  loss: 0.7727329134941101\n",
      "epoch:9, batch: 841,  loss: 0.8978353142738342\n",
      "epoch:9, batch: 842,  loss: 1.0279062986373901\n",
      "epoch:9, batch: 843,  loss: 0.8257530927658081\n",
      "epoch:9, batch: 844,  loss: 0.9494032263755798\n",
      "epoch:9, batch: 845,  loss: 0.9475749731063843\n",
      "epoch:9, batch: 846,  loss: 1.0303272008895874\n",
      "epoch:9, batch: 847,  loss: 0.6899746656417847\n",
      "epoch:9, batch: 848,  loss: 1.1907074451446533\n",
      "epoch:9, batch: 849,  loss: 0.7219560146331787\n",
      "epoch:9, batch: 850,  loss: 0.9340854287147522\n",
      "epoch:9, batch: 851,  loss: 1.2325124740600586\n",
      "epoch:9, batch: 852,  loss: 0.7361602187156677\n",
      "epoch:9, batch: 853,  loss: 0.8267778158187866\n",
      "epoch:9, batch: 854,  loss: 1.1919206380844116\n",
      "epoch:9, batch: 855,  loss: 1.0091114044189453\n",
      "epoch:9, batch: 856,  loss: 0.7992687821388245\n",
      "epoch:9, batch: 857,  loss: 0.7660419344902039\n",
      "epoch:9, batch: 858,  loss: 1.0137752294540405\n",
      "epoch:9, batch: 859,  loss: 0.9325293898582458\n",
      "epoch:9, batch: 860,  loss: 1.0223256349563599\n",
      "epoch:9, batch: 861,  loss: 1.0342955589294434\n",
      "epoch:9, batch: 862,  loss: 1.124070644378662\n",
      "epoch:9, batch: 863,  loss: 1.1530267000198364\n",
      "epoch:9, batch: 864,  loss: 0.8810385465621948\n",
      "epoch:9, batch: 865,  loss: 0.927126944065094\n",
      "epoch:9, batch: 866,  loss: 0.8911198377609253\n",
      "epoch:9, batch: 867,  loss: 1.057509183883667\n",
      "epoch:9, batch: 868,  loss: 0.8585638403892517\n",
      "epoch:9, batch: 869,  loss: 1.1081756353378296\n",
      "epoch:9, batch: 870,  loss: 0.8561208844184875\n",
      "epoch:9, batch: 871,  loss: 1.1057240962982178\n",
      "epoch:9, batch: 872,  loss: 0.8792502284049988\n",
      "epoch:9, batch: 873,  loss: 1.4632434844970703\n",
      "epoch:9, batch: 874,  loss: 0.8618056178092957\n",
      "epoch:9, batch: 875,  loss: 1.249016284942627\n",
      "epoch:9, batch: 876,  loss: 1.0332298278808594\n",
      "epoch:9, batch: 877,  loss: 0.9436346292495728\n",
      "epoch:9, batch: 878,  loss: 0.7363269925117493\n",
      "epoch:9, batch: 879,  loss: 1.1269676685333252\n",
      "epoch:9, batch: 880,  loss: 1.091794729232788\n",
      "epoch:9, batch: 881,  loss: 1.2000501155853271\n",
      "epoch:9, batch: 882,  loss: 1.0706498622894287\n",
      "epoch:9, batch: 883,  loss: 1.1791729927062988\n",
      "epoch:9, batch: 884,  loss: 1.1091188192367554\n",
      "epoch:9, batch: 885,  loss: 0.9155306816101074\n",
      "epoch:9, batch: 886,  loss: 0.7527466416358948\n",
      "epoch:9, batch: 887,  loss: 0.9538430571556091\n",
      "epoch:9, batch: 888,  loss: 0.9198156595230103\n",
      "epoch:9, batch: 889,  loss: 1.0793812274932861\n",
      "epoch:9, batch: 890,  loss: 1.3293663263320923\n",
      "epoch:9, batch: 891,  loss: 1.1190235614776611\n",
      "epoch:9, batch: 892,  loss: 0.9503639340400696\n",
      "epoch:9, batch: 893,  loss: 1.0009969472885132\n",
      "epoch:9, batch: 894,  loss: 0.7850359678268433\n",
      "epoch:9, batch: 895,  loss: 1.0205150842666626\n",
      "epoch:9, batch: 896,  loss: 1.0468560457229614\n",
      "epoch:9, batch: 897,  loss: 0.7298758625984192\n",
      "epoch:9, batch: 898,  loss: 1.1107813119888306\n",
      "epoch:9, batch: 899,  loss: 0.7273774743080139\n",
      "epoch:9, batch: 900,  loss: 0.9699393510818481\n",
      "epoch:9, batch: 901,  loss: 1.1250580549240112\n",
      "epoch:9, batch: 902,  loss: 1.0533469915390015\n",
      "epoch:9, batch: 903,  loss: 0.8381115198135376\n",
      "epoch:9, batch: 904,  loss: 0.9745446443557739\n",
      "epoch:9, batch: 905,  loss: 1.0876750946044922\n",
      "epoch:9, batch: 906,  loss: 1.1396228075027466\n",
      "epoch:9, batch: 907,  loss: 1.0431053638458252\n",
      "epoch:9, batch: 908,  loss: 0.9199277758598328\n",
      "epoch:9, batch: 909,  loss: 1.617620587348938\n",
      "epoch:9, batch: 910,  loss: 0.8631640672683716\n",
      "epoch:9, batch: 911,  loss: 0.9528553485870361\n",
      "epoch:9, batch: 912,  loss: 0.9942995309829712\n",
      "epoch:9, batch: 913,  loss: 0.7450255155563354\n",
      "epoch:9, batch: 914,  loss: 0.8078731298446655\n",
      "epoch:9, batch: 915,  loss: 0.9670419096946716\n",
      "epoch:9, batch: 916,  loss: 1.411360263824463\n",
      "epoch:9, batch: 917,  loss: 0.9895254373550415\n",
      "epoch:9, batch: 918,  loss: 0.9417396783828735\n",
      "epoch:9, batch: 919,  loss: 0.8436151742935181\n",
      "epoch:9, batch: 920,  loss: 1.0176278352737427\n",
      "epoch:9, batch: 921,  loss: 0.8768576979637146\n",
      "epoch:9, batch: 922,  loss: 0.9081918597221375\n",
      "epoch:9, batch: 923,  loss: 1.0421658754348755\n",
      "epoch:9, batch: 924,  loss: 1.0502699613571167\n",
      "epoch:9, batch: 925,  loss: 1.0272504091262817\n",
      "epoch:9, batch: 926,  loss: 1.1040403842926025\n",
      "epoch:9, batch: 927,  loss: 0.9153926968574524\n",
      "epoch:9, batch: 928,  loss: 1.2190485000610352\n",
      "epoch:9, batch: 929,  loss: 0.9569160342216492\n",
      "epoch:9, batch: 930,  loss: 0.7811515927314758\n",
      "epoch:9, batch: 931,  loss: 1.1381622552871704\n",
      "epoch:9, batch: 932,  loss: 1.0498441457748413\n",
      "epoch:9, batch: 933,  loss: 1.1748138666152954\n",
      "epoch:9, batch: 934,  loss: 0.6544781923294067\n",
      "epoch:9, batch: 935,  loss: 1.275162935256958\n",
      "epoch:9, batch: 936,  loss: 1.3311983346939087\n",
      "epoch:9, batch: 937,  loss: 1.1760461330413818\n",
      "epoch:9, batch: 938,  loss: 1.1228845119476318\n",
      "epoch:9, batch: 939,  loss: 0.8636404871940613\n",
      "epoch:9, batch: 940,  loss: 1.3921420574188232\n",
      "epoch:9, batch: 941,  loss: 1.1435970067977905\n",
      "epoch:9, batch: 942,  loss: 1.0094308853149414\n",
      "epoch:9, batch: 943,  loss: 1.0013923645019531\n",
      "epoch:9, batch: 944,  loss: 1.0461798906326294\n",
      "epoch:9, batch: 945,  loss: 0.8665637969970703\n",
      "epoch:9, batch: 946,  loss: 0.9608678221702576\n",
      "epoch:9, batch: 947,  loss: 0.9971502423286438\n",
      "epoch:9, batch: 948,  loss: 1.1417293548583984\n",
      "epoch:9, batch: 949,  loss: 0.8921613693237305\n",
      "epoch:9, batch: 950,  loss: 0.9527119994163513\n",
      "epoch:9, batch: 951,  loss: 0.8086262345314026\n",
      "epoch:9, batch: 952,  loss: 1.0167220830917358\n",
      "epoch:9, batch: 953,  loss: 1.0227667093276978\n",
      "epoch:9, batch: 954,  loss: 1.0054823160171509\n",
      "epoch:9, batch: 955,  loss: 0.9751214981079102\n",
      "epoch:9, batch: 956,  loss: 0.9562572240829468\n",
      "epoch:9, batch: 957,  loss: 0.8393523693084717\n",
      "epoch:9, batch: 958,  loss: 0.8801775574684143\n",
      "epoch:9, batch: 959,  loss: 0.8705681562423706\n",
      "epoch:9, batch: 960,  loss: 0.9775065183639526\n",
      "epoch:9, batch: 961,  loss: 0.7351966500282288\n",
      "epoch:9, batch: 962,  loss: 1.2034130096435547\n",
      "epoch:9, batch: 963,  loss: 1.0123072862625122\n",
      "epoch:9, batch: 964,  loss: 1.011486291885376\n",
      "epoch:9, batch: 965,  loss: 0.8927397727966309\n",
      "epoch:9, batch: 966,  loss: 0.8392002582550049\n",
      "epoch:9, batch: 967,  loss: 0.9965536594390869\n",
      "epoch:9, batch: 968,  loss: 1.0418968200683594\n",
      "epoch:9, batch: 969,  loss: 0.687252938747406\n",
      "epoch:9, batch: 970,  loss: 0.9575595259666443\n",
      "epoch:9, batch: 971,  loss: 0.9374361634254456\n",
      "epoch:9, batch: 972,  loss: 1.0324641466140747\n",
      "epoch:9, batch: 973,  loss: 0.8217287659645081\n",
      "epoch:9, batch: 974,  loss: 0.853460967540741\n",
      "epoch:9, batch: 975,  loss: 0.9918838739395142\n",
      "epoch:9, batch: 976,  loss: 1.0997323989868164\n",
      "epoch:9, batch: 977,  loss: 0.8862454295158386\n",
      "epoch:9, batch: 978,  loss: 1.0401266813278198\n",
      "epoch:9, batch: 979,  loss: 0.8020254373550415\n",
      "epoch:9, batch: 980,  loss: 0.8443209528923035\n",
      "epoch:9, batch: 981,  loss: 0.7786762118339539\n",
      "epoch:9, batch: 982,  loss: 1.1580060720443726\n",
      "epoch:9, batch: 983,  loss: 0.9582133889198303\n",
      "epoch:9, batch: 984,  loss: 1.278464436531067\n",
      "epoch:9, batch: 985,  loss: 1.1635226011276245\n",
      "epoch:9, batch: 986,  loss: 0.6405349969863892\n",
      "epoch:9, batch: 987,  loss: 1.0380704402923584\n",
      "epoch:9, batch: 988,  loss: 0.8856522440910339\n",
      "epoch:9, batch: 989,  loss: 1.0967403650283813\n",
      "epoch:9, batch: 990,  loss: 1.0811599493026733\n",
      "epoch:9, batch: 991,  loss: 0.7081199884414673\n",
      "epoch:9, batch: 992,  loss: 0.942488968372345\n",
      "epoch:9, batch: 993,  loss: 1.0985780954360962\n",
      "epoch:9, batch: 994,  loss: 0.7380834221839905\n",
      "epoch:9, batch: 995,  loss: 0.7735799551010132\n",
      "epoch:9, batch: 996,  loss: 0.9600442051887512\n",
      "epoch:9, batch: 997,  loss: 1.1546367406845093\n",
      "epoch:9, batch: 998,  loss: 0.9323492050170898\n",
      "epoch:9, batch: 999,  loss: 0.8827829957008362\n",
      "epoch:9, batch: 1000,  loss: 1.2632784843444824\n",
      "epoch:9, batch: 1001,  loss: 1.124163269996643\n",
      "epoch:9, batch: 1002,  loss: 1.1238455772399902\n",
      "epoch:9, batch: 1003,  loss: 1.022889494895935\n",
      "epoch:9, batch: 1004,  loss: 0.8718266487121582\n",
      "epoch:9, batch: 1005,  loss: 0.7275726199150085\n",
      "epoch:9, batch: 1006,  loss: 0.8796219825744629\n",
      "epoch:9, batch: 1007,  loss: 0.9233630299568176\n",
      "epoch:9, batch: 1008,  loss: 0.9495943188667297\n",
      "epoch:9, batch: 1009,  loss: 1.0835458040237427\n",
      "epoch:9, batch: 1010,  loss: 0.7521330714225769\n",
      "epoch:9, batch: 1011,  loss: 0.9893972873687744\n",
      "epoch:9, batch: 1012,  loss: 1.0371367931365967\n",
      "epoch:9, batch: 1013,  loss: 0.9035358428955078\n",
      "epoch:9, batch: 1014,  loss: 0.9404730200767517\n",
      "epoch:9, batch: 1015,  loss: 0.9555589556694031\n",
      "epoch:9, batch: 1016,  loss: 0.9235691428184509\n",
      "epoch:9, batch: 1017,  loss: 1.0525280237197876\n",
      "epoch:9, batch: 1018,  loss: 0.9233099222183228\n",
      "epoch:9, batch: 1019,  loss: 1.1831480264663696\n",
      "epoch:9, batch: 1020,  loss: 1.2197840213775635\n",
      "epoch:9, batch: 1021,  loss: 0.9411066770553589\n",
      "epoch:9, batch: 1022,  loss: 0.8038988709449768\n",
      "epoch:9, batch: 1023,  loss: 1.3137565851211548\n",
      "epoch:9, batch: 1024,  loss: 0.8713802099227905\n",
      "epoch:9, batch: 1025,  loss: 0.983214795589447\n",
      "epoch:9, batch: 1026,  loss: 0.913395345211029\n",
      "epoch:9, batch: 1027,  loss: 0.847511351108551\n",
      "epoch:9, batch: 1028,  loss: 1.0319170951843262\n",
      "epoch:9, batch: 1029,  loss: 1.0939764976501465\n",
      "epoch:9, batch: 1030,  loss: 1.039273977279663\n",
      "epoch:9, batch: 1031,  loss: 1.0222041606903076\n",
      "epoch:9, batch: 1032,  loss: 0.9557366371154785\n",
      "epoch:9, batch: 1033,  loss: 0.7644293308258057\n",
      "epoch:9, batch: 1034,  loss: 1.0238008499145508\n",
      "epoch:9, batch: 1035,  loss: 0.9413108229637146\n",
      "epoch:9, batch: 1036,  loss: 0.9673804044723511\n",
      "epoch:9, batch: 1037,  loss: 1.223717212677002\n",
      "epoch:9, batch: 1038,  loss: 0.8096421957015991\n",
      "epoch:9, batch: 1039,  loss: 0.817176342010498\n",
      "epoch:9, batch: 1040,  loss: 1.2188506126403809\n",
      "epoch:9, batch: 1041,  loss: 0.945206344127655\n",
      "epoch:9, batch: 1042,  loss: 1.179548740386963\n",
      "epoch:9, batch: 1043,  loss: 1.0174521207809448\n",
      "epoch:9, batch: 1044,  loss: 0.9560465812683105\n",
      "epoch:9, batch: 1045,  loss: 0.9483513236045837\n",
      "epoch:9, batch: 1046,  loss: 0.918712317943573\n",
      "epoch:9, batch: 1047,  loss: 0.8442878723144531\n",
      "epoch:9, batch: 1048,  loss: 0.8975761532783508\n",
      "epoch:9, batch: 1049,  loss: 1.1888858079910278\n",
      "epoch:9, batch: 1050,  loss: 1.5166336297988892\n",
      "epoch:9, batch: 1051,  loss: 0.9731115102767944\n",
      "epoch:9, batch: 1052,  loss: 0.957576334476471\n",
      "epoch:9, batch: 1053,  loss: 0.7291409373283386\n",
      "epoch:9, batch: 1054,  loss: 1.0029925107955933\n",
      "epoch:9, batch: 1055,  loss: 0.7379723787307739\n",
      "epoch:9, batch: 1056,  loss: 1.3018640279769897\n",
      "epoch:9, batch: 1057,  loss: 0.8416131138801575\n",
      "epoch:9, batch: 1058,  loss: 0.8172000646591187\n",
      "epoch:9, batch: 1059,  loss: 0.9401921033859253\n",
      "epoch:9, batch: 1060,  loss: 0.7655094861984253\n",
      "epoch:9, batch: 1061,  loss: 0.8720730543136597\n",
      "epoch:9, batch: 1062,  loss: 1.069506049156189\n",
      "epoch:9, batch: 1063,  loss: 1.0785809755325317\n",
      "epoch:9, batch: 1064,  loss: 1.1935213804244995\n",
      "epoch:9, batch: 1065,  loss: 0.8466370701789856\n",
      "epoch:9, batch: 1066,  loss: 0.926407516002655\n",
      "epoch:9, batch: 1067,  loss: 0.6585878729820251\n",
      "epoch:9, batch: 1068,  loss: 0.8885330557823181\n",
      "epoch:9, batch: 1069,  loss: 1.0621461868286133\n",
      "epoch:9, batch: 1070,  loss: 0.9287580847740173\n",
      "epoch:9, batch: 1071,  loss: 0.9436984658241272\n",
      "epoch:9, batch: 1072,  loss: 0.970491886138916\n",
      "epoch:9, batch: 1073,  loss: 1.129191279411316\n",
      "epoch:9, batch: 1074,  loss: 0.8556472063064575\n",
      "epoch:9, batch: 1075,  loss: 1.0463144779205322\n",
      "epoch:9, batch: 1076,  loss: 1.0275464057922363\n",
      "epoch:9, batch: 1077,  loss: 0.8323748707771301\n",
      "epoch:9, batch: 1078,  loss: 1.1338050365447998\n",
      "epoch:9, batch: 1079,  loss: 0.9223501682281494\n",
      "epoch:9, batch: 1080,  loss: 1.195456862449646\n",
      "epoch:9, batch: 1081,  loss: 1.0932247638702393\n",
      "epoch:9, batch: 1082,  loss: 0.9836320281028748\n",
      "epoch:9, batch: 1083,  loss: 1.0574215650558472\n",
      "epoch:9, batch: 1084,  loss: 1.0675252676010132\n",
      "epoch:9, batch: 1085,  loss: 1.4085078239440918\n",
      "epoch:9, batch: 1086,  loss: 1.0420573949813843\n",
      "epoch:9, batch: 1087,  loss: 0.8449673056602478\n",
      "epoch:9, batch: 1088,  loss: 1.473393201828003\n",
      "epoch:9, batch: 1089,  loss: 0.9934192299842834\n",
      "epoch:9, batch: 1090,  loss: 0.8454292416572571\n",
      "epoch:9, batch: 1091,  loss: 0.8216671347618103\n",
      "epoch:9, batch: 1092,  loss: 1.0169334411621094\n",
      "epoch:9, batch: 1093,  loss: 1.2724241018295288\n",
      "epoch:9, batch: 1094,  loss: 1.0232652425765991\n",
      "epoch:9, batch: 1095,  loss: 1.2302570343017578\n",
      "epoch:9, batch: 1096,  loss: 1.1768734455108643\n",
      "epoch:9, batch: 1097,  loss: 0.9329726099967957\n",
      "epoch:9, batch: 1098,  loss: 1.3389796018600464\n",
      "epoch:9, batch: 1099,  loss: 1.3016691207885742\n",
      "epoch:9, batch: 1100,  loss: 1.0629992485046387\n",
      "epoch:9, batch: 1101,  loss: 0.8803033828735352\n",
      "epoch:9, batch: 1102,  loss: 0.9996224045753479\n",
      "epoch:9, batch: 1103,  loss: 1.203399419784546\n",
      "epoch:9, batch: 1104,  loss: 0.9390665888786316\n",
      "epoch:9, batch: 1105,  loss: 0.8139875531196594\n",
      "epoch:9, batch: 1106,  loss: 0.7201992273330688\n",
      "epoch:9, batch: 1107,  loss: 0.9747418165206909\n",
      "epoch:9, batch: 1108,  loss: 0.9984829425811768\n",
      "epoch:9, batch: 1109,  loss: 1.1804007291793823\n",
      "epoch:9, batch: 1110,  loss: 0.9365138411521912\n",
      "epoch:9, batch: 1111,  loss: 0.9341384172439575\n",
      "epoch:9, batch: 1112,  loss: 0.8995585441589355\n",
      "epoch:9, batch: 1113,  loss: 0.9708912968635559\n",
      "epoch:9, batch: 1114,  loss: 1.0167690515518188\n",
      "epoch:9, batch: 1115,  loss: 0.7313638925552368\n",
      "epoch:9, batch: 1116,  loss: 1.155924916267395\n",
      "epoch:9, batch: 1117,  loss: 1.0391353368759155\n",
      "epoch:9, batch: 1118,  loss: 0.8849867582321167\n",
      "epoch:9, batch: 1119,  loss: 1.1344103813171387\n",
      "epoch:9, batch: 1120,  loss: 0.8780031204223633\n",
      "epoch:9, batch: 1121,  loss: 0.9916127920150757\n",
      "epoch:9, batch: 1122,  loss: 1.1848491430282593\n",
      "epoch:9, batch: 1123,  loss: 0.7832031846046448\n",
      "epoch:9, batch: 1124,  loss: 1.1495484113693237\n",
      "epoch:9, batch: 1125,  loss: 0.9636175632476807\n",
      "epoch:9, batch: 1126,  loss: 1.0665042400360107\n",
      "epoch:9, batch: 1127,  loss: 1.1044104099273682\n",
      "epoch:9, batch: 1128,  loss: 0.8951873779296875\n",
      "epoch:9, batch: 1129,  loss: 0.917819082736969\n",
      "epoch:9, batch: 1130,  loss: 0.9680368900299072\n",
      "epoch:9, batch: 1131,  loss: 1.199489951133728\n",
      "epoch:9, batch: 1132,  loss: 0.9417595267295837\n",
      "epoch:9, batch: 1133,  loss: 0.9063917398452759\n",
      "epoch:9, batch: 1134,  loss: 0.8978116512298584\n",
      "epoch:9, batch: 1135,  loss: 0.8146132230758667\n",
      "epoch:9, batch: 1136,  loss: 1.3218988180160522\n",
      "epoch:9, batch: 1137,  loss: 0.9907146096229553\n",
      "epoch:9, batch: 1138,  loss: 1.0127896070480347\n",
      "epoch:9, batch: 1139,  loss: 0.8990527987480164\n",
      "epoch:9, batch: 1140,  loss: 1.182879090309143\n",
      "epoch:9, batch: 1141,  loss: 1.0992000102996826\n",
      "epoch:9, batch: 1142,  loss: 0.9402866363525391\n",
      "epoch:9, batch: 1143,  loss: 0.9641771912574768\n",
      "epoch:9, batch: 1144,  loss: 1.1428663730621338\n",
      "epoch:9, batch: 1145,  loss: 1.1234145164489746\n",
      "epoch:9, batch: 1146,  loss: 1.3476718664169312\n",
      "epoch:9, batch: 1147,  loss: 0.7737007737159729\n",
      "epoch:9, batch: 1148,  loss: 0.9653741121292114\n",
      "epoch:9, batch: 1149,  loss: 1.024343490600586\n",
      "epoch:9, batch: 1150,  loss: 0.9520322680473328\n",
      "epoch:9, batch: 1151,  loss: 0.8329635858535767\n",
      "epoch:9, batch: 1152,  loss: 0.8519148826599121\n",
      "epoch:9, batch: 1153,  loss: 0.925765335559845\n",
      "epoch:9, batch: 1154,  loss: 1.1809765100479126\n",
      "epoch:9, batch: 1155,  loss: 1.1182153224945068\n",
      "epoch:9, batch: 1156,  loss: 1.1747665405273438\n",
      "epoch:9, batch: 1157,  loss: 1.1283349990844727\n",
      "epoch:9, batch: 1158,  loss: 0.6904545426368713\n",
      "epoch:9, batch: 1159,  loss: 1.2949496507644653\n",
      "epoch:9, batch: 1160,  loss: 0.8387015461921692\n",
      "epoch:9, batch: 1161,  loss: 1.2794864177703857\n",
      "epoch:9, batch: 1162,  loss: 0.8227572441101074\n",
      "epoch:9, batch: 1163,  loss: 0.9340761303901672\n",
      "epoch:9, batch: 1164,  loss: 0.8485879898071289\n",
      "epoch:9, batch: 1165,  loss: 0.9877040982246399\n",
      "epoch:9, batch: 1166,  loss: 1.0145996809005737\n",
      "epoch:9, batch: 1167,  loss: 1.0565505027770996\n",
      "epoch:9, batch: 1168,  loss: 1.1677353382110596\n",
      "epoch:9, batch: 1169,  loss: 0.9782297015190125\n",
      "epoch:9, batch: 1170,  loss: 0.8550742864608765\n",
      "epoch:9, batch: 1171,  loss: 1.3898931741714478\n",
      "epoch:9, batch: 1172,  loss: 1.1335140466690063\n",
      "epoch:9, batch: 1173,  loss: 1.174851655960083\n",
      "epoch:9, batch: 1174,  loss: 0.9550742506980896\n",
      "epoch:9, batch: 1175,  loss: 1.2946827411651611\n",
      "epoch:9, batch: 1176,  loss: 0.9862481951713562\n",
      "epoch:9, batch: 1177,  loss: 0.7773255109786987\n",
      "epoch:9, batch: 1178,  loss: 0.9666588306427002\n",
      "epoch:9, batch: 1179,  loss: 0.9888829588890076\n",
      "epoch:9, batch: 1180,  loss: 1.0108212232589722\n",
      "epoch:9, batch: 1181,  loss: 1.127812385559082\n",
      "epoch:9, batch: 1182,  loss: 0.9333608746528625\n",
      "epoch:9, batch: 1183,  loss: 1.1597058773040771\n",
      "epoch:9, batch: 1184,  loss: 1.0876734256744385\n",
      "epoch:9, batch: 1185,  loss: 0.7893587350845337\n",
      "epoch:9, batch: 1186,  loss: 1.0409431457519531\n",
      "epoch:9, batch: 1187,  loss: 1.1063258647918701\n",
      "epoch:9, batch: 1188,  loss: 0.8981215953826904\n",
      "epoch:9, batch: 1189,  loss: 1.2282392978668213\n",
      "epoch:9, batch: 1190,  loss: 0.9693474173545837\n",
      "epoch:9, batch: 1191,  loss: 0.9114151000976562\n",
      "epoch:9, batch: 1192,  loss: 0.9220443367958069\n",
      "epoch:9, batch: 1193,  loss: 1.2513039112091064\n",
      "epoch:9, batch: 1194,  loss: 0.9882901906967163\n",
      "epoch:9, batch: 1195,  loss: 0.9441957473754883\n",
      "epoch:9, batch: 1196,  loss: 0.802510678768158\n",
      "epoch:9, batch: 1197,  loss: 0.7413260340690613\n",
      "epoch:9, batch: 1198,  loss: 0.8403387665748596\n",
      "epoch:9, batch: 1199,  loss: 1.0417677164077759\n",
      "epoch:9, batch: 1200,  loss: 1.4291483163833618\n",
      "epoch:9, batch: 1201,  loss: 0.8019077181816101\n",
      "epoch:9, batch: 1202,  loss: 0.935602605342865\n",
      "epoch:9, batch: 1203,  loss: 1.1075254678726196\n",
      "epoch:9, batch: 1204,  loss: 0.8771227598190308\n",
      "epoch:9, batch: 1205,  loss: 1.2371422052383423\n",
      "epoch:9, batch: 1206,  loss: 0.7182981967926025\n",
      "epoch:9, batch: 1207,  loss: 0.8769245147705078\n",
      "epoch:9, batch: 1208,  loss: 1.1235451698303223\n",
      "epoch:9, batch: 1209,  loss: 0.9272194504737854\n",
      "epoch:9, batch: 1210,  loss: 1.0472060441970825\n",
      "epoch:9, batch: 1211,  loss: 0.6979495286941528\n",
      "epoch:9, batch: 1212,  loss: 1.0124213695526123\n",
      "epoch:9, batch: 1213,  loss: 1.1076914072036743\n",
      "epoch:9, batch: 1214,  loss: 1.0539274215698242\n",
      "epoch:9, batch: 1215,  loss: 1.0174534320831299\n",
      "epoch:9, batch: 1216,  loss: 1.1326813697814941\n",
      "epoch:9, batch: 1217,  loss: 0.8096526861190796\n",
      "epoch:9, batch: 1218,  loss: 0.982043445110321\n",
      "epoch:9, batch: 1219,  loss: 0.9212852120399475\n",
      "epoch:9, batch: 1220,  loss: 1.3723042011260986\n",
      "epoch:9, batch: 1221,  loss: 1.0820491313934326\n",
      "epoch:9, batch: 1222,  loss: 0.9615654349327087\n",
      "epoch:9, batch: 1223,  loss: 1.00284743309021\n",
      "epoch:9, batch: 1224,  loss: 1.0444821119308472\n",
      "epoch:9, batch: 1225,  loss: 0.9878513813018799\n",
      "epoch:9, batch: 1226,  loss: 0.9865252375602722\n",
      "epoch:9, batch: 1227,  loss: 0.9013728499412537\n",
      "epoch:9, batch: 1228,  loss: 1.0847716331481934\n",
      "epoch:9, batch: 1229,  loss: 1.0531480312347412\n",
      "epoch:9, batch: 1230,  loss: 0.8088080883026123\n",
      "epoch:9, batch: 1231,  loss: 0.959368884563446\n",
      "epoch:9, batch: 1232,  loss: 1.0358397960662842\n",
      "epoch:9, batch: 1233,  loss: 1.2272953987121582\n",
      "epoch:9, batch: 1234,  loss: 0.7058324813842773\n",
      "epoch:9, batch: 1235,  loss: 1.3200855255126953\n",
      "epoch:9, batch: 1236,  loss: 0.7950661182403564\n",
      "epoch:9, batch: 1237,  loss: 1.3347148895263672\n",
      "epoch:9, batch: 1238,  loss: 1.0965873003005981\n",
      "epoch:9, batch: 1239,  loss: 0.9309974312782288\n",
      "epoch:9, batch: 1240,  loss: 1.0057061910629272\n",
      "epoch:9, batch: 1241,  loss: 1.1629482507705688\n",
      "epoch:9, batch: 1242,  loss: 1.0692200660705566\n",
      "epoch:9, batch: 1243,  loss: 1.3266054391860962\n",
      "epoch:9, batch: 1244,  loss: 1.1384943723678589\n",
      "epoch:9, batch: 1245,  loss: 1.0676462650299072\n",
      "epoch:9, batch: 1246,  loss: 1.0558124780654907\n",
      "epoch:9, batch: 1247,  loss: 1.0101255178451538\n",
      "epoch:9, batch: 1248,  loss: 0.9767977595329285\n",
      "epoch:9, batch: 1249,  loss: 1.4163707494735718\n",
      "epoch:9, batch: 1250,  loss: 1.2230138778686523\n",
      "epoch:9, batch: 1251,  loss: 0.7827794551849365\n",
      "epoch:9, batch: 1252,  loss: 1.2763639688491821\n",
      "epoch:9, batch: 1253,  loss: 1.0340782403945923\n",
      "epoch:9, batch: 1254,  loss: 0.9941154718399048\n",
      "epoch:9, batch: 1255,  loss: 0.9821417927742004\n",
      "epoch:9, batch: 1256,  loss: 0.8868815898895264\n",
      "epoch:9, batch: 1257,  loss: 0.7446904182434082\n",
      "epoch:9, batch: 1258,  loss: 0.901426374912262\n",
      "epoch:9, batch: 1259,  loss: 1.0677647590637207\n",
      "epoch:9, batch: 1260,  loss: 0.6893434524536133\n",
      "epoch:9, batch: 1261,  loss: 0.9050147533416748\n",
      "epoch:9, batch: 1262,  loss: 0.8991165161132812\n",
      "epoch:9, batch: 1263,  loss: 0.8965147733688354\n",
      "epoch:9, batch: 1264,  loss: 0.9323027729988098\n",
      "epoch:9, batch: 1265,  loss: 0.92816561460495\n",
      "epoch:9, batch: 1266,  loss: 1.054872989654541\n",
      "epoch:9, batch: 1267,  loss: 1.5766080617904663\n",
      "epoch:9, batch: 1268,  loss: 1.01031494140625\n",
      "epoch:9, batch: 1269,  loss: 0.9579293727874756\n",
      "epoch:9, batch: 1270,  loss: 1.0962928533554077\n",
      "epoch:9, batch: 1271,  loss: 1.035765290260315\n",
      "epoch:9, batch: 1272,  loss: 0.7777674198150635\n",
      "epoch:9, batch: 1273,  loss: 0.9890168905258179\n",
      "epoch:9, batch: 1274,  loss: 0.9495328068733215\n",
      "epoch:9, batch: 1275,  loss: 0.8538536429405212\n",
      "epoch:9, batch: 1276,  loss: 0.9564668536186218\n",
      "epoch:9, batch: 1277,  loss: 1.1550331115722656\n",
      "epoch:9, batch: 1278,  loss: 0.9210692644119263\n",
      "epoch:9, batch: 1279,  loss: 0.9230217933654785\n",
      "epoch:9, batch: 1280,  loss: 1.1721007823944092\n",
      "epoch:9, batch: 1281,  loss: 0.9455486536026001\n",
      "epoch:9, batch: 1282,  loss: 1.1744266748428345\n",
      "epoch:9, batch: 1283,  loss: 0.77508145570755\n",
      "epoch:9, batch: 1284,  loss: 1.207292914390564\n",
      "epoch:9, batch: 1285,  loss: 1.0668076276779175\n",
      "epoch:9, batch: 1286,  loss: 1.201945424079895\n",
      "epoch:9, batch: 1287,  loss: 0.9016683101654053\n",
      "epoch:9, batch: 1288,  loss: 1.2365463972091675\n",
      "epoch:9, batch: 1289,  loss: 1.0554211139678955\n",
      "epoch:9, batch: 1290,  loss: 0.979644238948822\n",
      "epoch:9, batch: 1291,  loss: 1.0003008842468262\n",
      "epoch:9, batch: 1292,  loss: 1.0690988302230835\n",
      "epoch:9, batch: 1293,  loss: 1.1562367677688599\n",
      "epoch:9, batch: 1294,  loss: 0.7498607635498047\n",
      "epoch:9, batch: 1295,  loss: 1.0504462718963623\n",
      "epoch:9, batch: 1296,  loss: 0.9346332550048828\n",
      "epoch:9, batch: 1297,  loss: 1.1140490770339966\n",
      "epoch:9, batch: 1298,  loss: 1.337669014930725\n",
      "epoch:9, batch: 1299,  loss: 1.2490613460540771\n",
      "epoch:9, batch: 1300,  loss: 0.9513668417930603\n",
      "epoch:9, batch: 1301,  loss: 0.795025110244751\n",
      "epoch:9, batch: 1302,  loss: 1.1685426235198975\n",
      "epoch:9, batch: 1303,  loss: 0.7647792100906372\n",
      "epoch:9, batch: 1304,  loss: 0.8458390831947327\n",
      "epoch:9, batch: 1305,  loss: 0.8772863149642944\n",
      "epoch:9, batch: 1306,  loss: 1.0835349559783936\n",
      "epoch:9, batch: 1307,  loss: 0.7112662196159363\n",
      "epoch:9, batch: 1308,  loss: 1.135792851448059\n",
      "epoch:9, batch: 1309,  loss: 0.8642532825469971\n",
      "epoch:9, batch: 1310,  loss: 1.1128895282745361\n",
      "epoch:9, batch: 1311,  loss: 0.7695317268371582\n",
      "epoch:9, batch: 1312,  loss: 1.0882017612457275\n",
      "epoch:9, batch: 1313,  loss: 0.9194945096969604\n",
      "epoch:9, batch: 1314,  loss: 0.6373905539512634\n",
      "epoch:9, batch: 1315,  loss: 1.0026512145996094\n",
      "epoch:9, batch: 1316,  loss: 1.3289393186569214\n",
      "epoch:9, batch: 1317,  loss: 1.0522825717926025\n",
      "epoch:9, batch: 1318,  loss: 0.8728373646736145\n",
      "epoch:9, batch: 1319,  loss: 1.0465967655181885\n",
      "epoch:9, batch: 1320,  loss: 0.8419198393821716\n",
      "epoch:9, batch: 1321,  loss: 1.205869197845459\n",
      "epoch:9, batch: 1322,  loss: 1.0574764013290405\n",
      "epoch:9, batch: 1323,  loss: 1.0089131593704224\n",
      "epoch:9, batch: 1324,  loss: 1.0983428955078125\n",
      "epoch:9, batch: 1325,  loss: 0.9732922315597534\n",
      "epoch:9, batch: 1326,  loss: 1.2006665468215942\n",
      "epoch:9, batch: 1327,  loss: 1.1936322450637817\n",
      "epoch:9, batch: 1328,  loss: 0.7439574003219604\n",
      "epoch:9, batch: 1329,  loss: 0.9259299039840698\n",
      "epoch:9, batch: 1330,  loss: 1.0356212854385376\n",
      "epoch:9, batch: 1331,  loss: 0.8729984760284424\n",
      "epoch:9, batch: 1332,  loss: 0.9408442974090576\n",
      "epoch:9, batch: 1333,  loss: 1.1186832189559937\n",
      "epoch:9, batch: 1334,  loss: 1.052162766456604\n",
      "epoch:9, batch: 1335,  loss: 1.005800724029541\n",
      "epoch:9, batch: 1336,  loss: 0.7469130158424377\n",
      "epoch:9, batch: 1337,  loss: 0.8812297582626343\n",
      "epoch:9, batch: 1338,  loss: 0.9190769195556641\n",
      "epoch:9, batch: 1339,  loss: 1.1484851837158203\n",
      "epoch:9, batch: 1340,  loss: 1.0340121984481812\n",
      "epoch:9, batch: 1341,  loss: 1.4258155822753906\n",
      "epoch:9, batch: 1342,  loss: 0.8947250247001648\n",
      "epoch:9, batch: 1343,  loss: 0.8554626703262329\n",
      "epoch:9, batch: 1344,  loss: 0.8163095116615295\n",
      "epoch:9, batch: 1345,  loss: 1.3234539031982422\n",
      "epoch:9, batch: 1346,  loss: 0.894544780254364\n",
      "epoch:9, batch: 1347,  loss: 1.3477721214294434\n",
      "epoch:9, batch: 1348,  loss: 0.950360119342804\n",
      "epoch:9, batch: 1349,  loss: 0.9920490980148315\n",
      "epoch:9, batch: 1350,  loss: 1.111662745475769\n",
      "epoch:9, batch: 1351,  loss: 0.9735714197158813\n",
      "epoch:9, batch: 1352,  loss: 1.0548014640808105\n",
      "epoch:9, batch: 1353,  loss: 0.927142322063446\n",
      "epoch:9, batch: 1354,  loss: 0.9730231165885925\n",
      "epoch:9, batch: 1355,  loss: 1.2124537229537964\n",
      "epoch:9, batch: 1356,  loss: 0.9415267705917358\n",
      "epoch:9, batch: 1357,  loss: 1.0550546646118164\n",
      "epoch:9, batch: 1358,  loss: 1.295682430267334\n",
      "epoch:9, batch: 1359,  loss: 1.3239285945892334\n",
      "epoch:9, batch: 1360,  loss: 1.0857734680175781\n",
      "epoch:9, batch: 1361,  loss: 1.3490277528762817\n",
      "epoch:9, batch: 1362,  loss: 1.1226508617401123\n",
      "epoch:9, batch: 1363,  loss: 0.9669828414916992\n",
      "epoch:9, batch: 1364,  loss: 0.9179714322090149\n",
      "epoch:9, batch: 1365,  loss: 0.9505259990692139\n",
      "epoch:9, batch: 1366,  loss: 1.0728774070739746\n",
      "epoch:9, batch: 1367,  loss: 1.2880032062530518\n",
      "epoch:9, batch: 1368,  loss: 1.1191513538360596\n",
      "epoch:9, batch: 1369,  loss: 1.3728872537612915\n",
      "epoch:9, batch: 1370,  loss: 0.9688407182693481\n",
      "epoch:9, batch: 1371,  loss: 0.880158543586731\n",
      "epoch:9, batch: 1372,  loss: 1.1011122465133667\n",
      "epoch:9, batch: 1373,  loss: 1.3481884002685547\n",
      "epoch:9, batch: 1374,  loss: 0.891258716583252\n",
      "epoch:9, batch: 1375,  loss: 1.2065860033035278\n",
      "epoch:9, batch: 1376,  loss: 1.0472455024719238\n",
      "epoch:9, batch: 1377,  loss: 1.162391185760498\n",
      "epoch:9, batch: 1378,  loss: 1.0028327703475952\n",
      "epoch:9, batch: 1379,  loss: 0.9464913606643677\n",
      "epoch:9, batch: 1380,  loss: 0.8704932332038879\n",
      "epoch:9, batch: 1381,  loss: 0.8221398591995239\n",
      "epoch:9, batch: 1382,  loss: 0.983068585395813\n",
      "epoch:9, batch: 1383,  loss: 1.124911904335022\n",
      "epoch:9, batch: 1384,  loss: 0.8903056383132935\n",
      "epoch:9, batch: 1385,  loss: 1.4109902381896973\n",
      "epoch:9, batch: 1386,  loss: 1.0104435682296753\n",
      "epoch:9, batch: 1387,  loss: 0.9247657656669617\n",
      "epoch:9, batch: 1388,  loss: 0.9869596362113953\n",
      "epoch:9, batch: 1389,  loss: 0.7653500437736511\n",
      "epoch:9, batch: 1390,  loss: 1.39419424533844\n",
      "epoch:9, batch: 1391,  loss: 1.1214011907577515\n",
      "epoch:9, batch: 1392,  loss: 1.0096569061279297\n",
      "epoch:9, batch: 1393,  loss: 1.1788499355316162\n",
      "epoch:9, batch: 1394,  loss: 0.7522756457328796\n",
      "epoch:9, batch: 1395,  loss: 0.9161512851715088\n",
      "epoch:9, batch: 1396,  loss: 1.144298791885376\n",
      "epoch:9, batch: 1397,  loss: 0.95020991563797\n",
      "epoch:9, batch: 1398,  loss: 1.201772689819336\n",
      "epoch:9, batch: 1399,  loss: 1.4026583433151245\n",
      "epoch:9, batch: 1400,  loss: 0.9403440356254578\n",
      "epoch:9, batch: 1401,  loss: 0.7857995629310608\n",
      "epoch:9, batch: 1402,  loss: 0.942405641078949\n",
      "epoch:9, batch: 1403,  loss: 0.9235415458679199\n",
      "epoch:9, batch: 1404,  loss: 0.9297487735748291\n",
      "epoch:9, batch: 1405,  loss: 0.9294441342353821\n",
      "epoch:9, batch: 1406,  loss: 1.1581045389175415\n",
      "epoch:9, batch: 1407,  loss: 0.9310720562934875\n",
      "epoch:9, batch: 1408,  loss: 1.1658682823181152\n",
      "epoch:9, batch: 1409,  loss: 0.8971899747848511\n",
      "epoch:9, batch: 1410,  loss: 1.095337986946106\n",
      "epoch:9, batch: 1411,  loss: 1.3226914405822754\n",
      "epoch:9, batch: 1412,  loss: 0.7219812273979187\n",
      "epoch:9, batch: 1413,  loss: 1.0834964513778687\n",
      "epoch:9, batch: 1414,  loss: 1.16947603225708\n",
      "epoch:9, batch: 1415,  loss: 1.3609181642532349\n",
      "epoch:9, batch: 1416,  loss: 1.0825999975204468\n",
      "epoch:9, batch: 1417,  loss: 0.9839036464691162\n",
      "epoch:9, batch: 1418,  loss: 0.9905797243118286\n",
      "epoch:9, batch: 1419,  loss: 0.8347066640853882\n",
      "epoch:9, batch: 1420,  loss: 0.8672296404838562\n",
      "epoch:9, batch: 1421,  loss: 1.0275245904922485\n",
      "epoch:9, batch: 1422,  loss: 1.0598931312561035\n",
      "epoch:9, batch: 1423,  loss: 0.884221613407135\n",
      "epoch:9, batch: 1424,  loss: 0.8618066310882568\n",
      "epoch:9, batch: 1425,  loss: 0.9466304779052734\n",
      "epoch:9, batch: 1426,  loss: 0.9323391318321228\n",
      "epoch:9, batch: 1427,  loss: 1.2149142026901245\n",
      "epoch:9, batch: 1428,  loss: 1.1880841255187988\n",
      "epoch:9, batch: 1429,  loss: 0.9216097593307495\n",
      "epoch:9, batch: 1430,  loss: 1.1779474020004272\n",
      "epoch:9, batch: 1431,  loss: 1.2293100357055664\n",
      "epoch:9, batch: 1432,  loss: 0.8826812505722046\n",
      "epoch:9, batch: 1433,  loss: 1.0948681831359863\n",
      "epoch:9, batch: 1434,  loss: 1.1833522319793701\n",
      "epoch:9, batch: 1435,  loss: 1.2013646364212036\n",
      "epoch:9, batch: 1436,  loss: 0.9189831614494324\n",
      "epoch:9, batch: 1437,  loss: 0.9945403337478638\n",
      "epoch:9, batch: 1438,  loss: 0.9857797622680664\n",
      "epoch:9, batch: 1439,  loss: 0.9775333404541016\n",
      "epoch:9, batch: 1440,  loss: 0.9072375893592834\n",
      "epoch:9, batch: 1441,  loss: 0.8474792242050171\n",
      "epoch:9, batch: 1442,  loss: 1.1184066534042358\n",
      "epoch:9, batch: 1443,  loss: 0.9340726733207703\n",
      "epoch:9, batch: 1444,  loss: 0.8389378786087036\n",
      "epoch:9, batch: 1445,  loss: 1.0289045572280884\n",
      "epoch:9, batch: 1446,  loss: 1.104767084121704\n",
      "epoch:9, batch: 1447,  loss: 1.0349788665771484\n",
      "epoch:9, batch: 1448,  loss: 1.2506484985351562\n",
      "epoch:9, batch: 1449,  loss: 0.7393723726272583\n",
      "epoch:9, batch: 1450,  loss: 1.1900776624679565\n",
      "epoch:9, batch: 1451,  loss: 1.0579767227172852\n",
      "epoch:9, batch: 1452,  loss: 1.1734055280685425\n",
      "epoch:9, batch: 1453,  loss: 0.9875857830047607\n",
      "epoch:9, batch: 1454,  loss: 0.9378382563591003\n",
      "epoch:9, batch: 1455,  loss: 1.382240891456604\n",
      "epoch:9, batch: 1456,  loss: 1.1930643320083618\n",
      "epoch:9, batch: 1457,  loss: 1.0326738357543945\n",
      "epoch:9, batch: 1458,  loss: 0.839647650718689\n",
      "epoch:9, batch: 1459,  loss: 1.0829744338989258\n",
      "epoch:9, batch: 1460,  loss: 1.2396643161773682\n",
      "epoch:9, batch: 1461,  loss: 1.0873302221298218\n",
      "epoch:9, batch: 1462,  loss: 0.7450513243675232\n",
      "epoch:9, batch: 1463,  loss: 0.9385079741477966\n",
      "epoch:9, batch: 1464,  loss: 0.8243412971496582\n",
      "epoch:9, batch: 1465,  loss: 0.9013558030128479\n",
      "epoch:9, batch: 1466,  loss: 1.0467283725738525\n",
      "epoch:9, batch: 1467,  loss: 1.1857057809829712\n",
      "epoch:9, batch: 1468,  loss: 0.8484854698181152\n",
      "epoch:9, batch: 1469,  loss: 0.7993802428245544\n",
      "epoch:9, batch: 1470,  loss: 1.4536573886871338\n",
      "epoch:9, batch: 1471,  loss: 1.003741979598999\n",
      "epoch:9, batch: 1472,  loss: 1.0249806642532349\n",
      "epoch:9, batch: 1473,  loss: 1.0613973140716553\n",
      "epoch:9, batch: 1474,  loss: 1.2104569673538208\n",
      "epoch:9, batch: 1475,  loss: 0.8574938178062439\n",
      "epoch:9, batch: 1476,  loss: 1.2596735954284668\n",
      "epoch:9, batch: 1477,  loss: 1.0115951299667358\n",
      "epoch:9, batch: 1478,  loss: 0.9206810593605042\n",
      "epoch:9, batch: 1479,  loss: 1.0976414680480957\n",
      "epoch:9, batch: 1480,  loss: 1.068822979927063\n",
      "epoch:9, batch: 1481,  loss: 1.2449361085891724\n",
      "epoch:9, batch: 1482,  loss: 0.9814804792404175\n",
      "epoch:9, batch: 1483,  loss: 1.117213249206543\n",
      "epoch:9, batch: 1484,  loss: 1.1754289865493774\n",
      "epoch:9, batch: 1485,  loss: 1.0398980379104614\n",
      "epoch:9, batch: 1486,  loss: 0.8278900980949402\n",
      "epoch:9, batch: 1487,  loss: 0.8731788396835327\n",
      "epoch:9, batch: 1488,  loss: 1.052546501159668\n",
      "epoch:9, batch: 1489,  loss: 0.9846650958061218\n",
      "epoch:9, batch: 1490,  loss: 1.030167818069458\n",
      "epoch:9, batch: 1491,  loss: 0.7907801866531372\n",
      "epoch:9, batch: 1492,  loss: 0.8940989375114441\n",
      "epoch:9, batch: 1493,  loss: 0.9802734851837158\n",
      "epoch:9, batch: 1494,  loss: 1.2565600872039795\n",
      "epoch:9, batch: 1495,  loss: 0.9491751194000244\n",
      "epoch:9, batch: 1496,  loss: 1.0468194484710693\n",
      "epoch:9, batch: 1497,  loss: 1.1836026906967163\n",
      "epoch:9, batch: 1498,  loss: 0.7726119160652161\n",
      "epoch:9, batch: 1499,  loss: 0.7822223901748657\n",
      "epoch:9, batch: 1500,  loss: 0.9345778226852417\n",
      "epoch:9, batch: 1501,  loss: 0.8148003816604614\n",
      "epoch:9, batch: 1502,  loss: 1.0765451192855835\n",
      "epoch:9, batch: 1503,  loss: 0.8205534815788269\n",
      "epoch:9, batch: 1504,  loss: 1.1255367994308472\n",
      "epoch:9, batch: 1505,  loss: 0.8609606027603149\n",
      "epoch:9, batch: 1506,  loss: 0.943388044834137\n",
      "epoch:9, batch: 1507,  loss: 1.1571189165115356\n",
      "epoch:9, batch: 1508,  loss: 1.1804003715515137\n",
      "epoch:9, batch: 1509,  loss: 0.9706904292106628\n",
      "epoch:9, batch: 1510,  loss: 0.8477173447608948\n",
      "epoch:9, batch: 1511,  loss: 0.7506589293479919\n",
      "epoch:9, batch: 1512,  loss: 0.8759129047393799\n",
      "epoch:9, batch: 1513,  loss: 0.987101674079895\n",
      "epoch:9, batch: 1514,  loss: 1.1697396039962769\n",
      "epoch:9, batch: 1515,  loss: 0.9723302721977234\n",
      "epoch:9, batch: 1516,  loss: 1.1116998195648193\n",
      "epoch:9, batch: 1517,  loss: 0.9269620776176453\n",
      "epoch:9, batch: 1518,  loss: 1.1862517595291138\n",
      "epoch:9, batch: 1519,  loss: 1.0098395347595215\n",
      "epoch:9, batch: 1520,  loss: 1.0339857339859009\n",
      "epoch:9, batch: 1521,  loss: 0.9452996253967285\n",
      "epoch:9, batch: 1522,  loss: 0.910527229309082\n",
      "epoch:9, batch: 1523,  loss: 1.0818690061569214\n",
      "epoch:9, batch: 1524,  loss: 0.8798989057540894\n",
      "epoch:9, batch: 1525,  loss: 0.9967001080513\n",
      "epoch:9, batch: 1526,  loss: 1.14468252658844\n",
      "epoch:9, batch: 1527,  loss: 1.051019310951233\n",
      "epoch:9, batch: 1528,  loss: 0.849774181842804\n",
      "epoch:9, batch: 1529,  loss: 1.0810991525650024\n",
      "epoch:9, batch: 1530,  loss: 0.8712245225906372\n",
      "epoch:9, batch: 1531,  loss: 1.1650326251983643\n",
      "epoch:9, batch: 1532,  loss: 1.1083213090896606\n",
      "epoch:9, batch: 1533,  loss: 1.3848185539245605\n",
      "epoch:9, batch: 1534,  loss: 0.8631548285484314\n",
      "epoch:9, batch: 1535,  loss: 1.2221404314041138\n",
      "epoch:9, batch: 1536,  loss: 1.1998909711837769\n",
      "epoch:9, batch: 1537,  loss: 0.9685957431793213\n",
      "epoch:9, batch: 1538,  loss: 1.445858120918274\n",
      "epoch:9, batch: 1539,  loss: 0.6929017305374146\n",
      "epoch:9, batch: 1540,  loss: 0.9450877904891968\n",
      "epoch:9, batch: 1541,  loss: 0.8475207686424255\n",
      "epoch:9, batch: 1542,  loss: 1.0369285345077515\n",
      "epoch:9, batch: 1543,  loss: 0.9918287992477417\n",
      "epoch:9, batch: 1544,  loss: 0.9949300289154053\n",
      "epoch:9, batch: 1545,  loss: 1.1392652988433838\n",
      "epoch:9, batch: 1546,  loss: 1.027029037475586\n",
      "epoch:9, batch: 1547,  loss: 1.1632498502731323\n",
      "epoch:9, batch: 1548,  loss: 1.2131201028823853\n",
      "epoch:9, batch: 1549,  loss: 0.9488075971603394\n",
      "epoch:9, batch: 1550,  loss: 1.3646669387817383\n",
      "epoch:9, batch: 1551,  loss: 0.8695666790008545\n",
      "epoch:9, batch: 1552,  loss: 1.1089181900024414\n",
      "epoch:9, batch: 1553,  loss: 1.139925479888916\n",
      "epoch:9, batch: 1554,  loss: 1.0929759740829468\n",
      "epoch:9, batch: 1555,  loss: 1.0599515438079834\n",
      "epoch:9, batch: 1556,  loss: 1.002034306526184\n",
      "epoch:9, batch: 1557,  loss: 0.985673725605011\n",
      "epoch:9, batch: 1558,  loss: 1.099215030670166\n",
      "epoch:9, batch: 1559,  loss: 0.9241100549697876\n",
      "epoch:9, batch: 1560,  loss: 1.101426362991333\n",
      "epoch:9, batch: 1561,  loss: 1.1871973276138306\n",
      "epoch:9, batch: 1562,  loss: 0.9649927616119385\n",
      "epoch:9, batch: 1563,  loss: 1.1394952535629272\n",
      "epoch:9, batch: 1564,  loss: 1.0873111486434937\n",
      "epoch:9, batch: 1565,  loss: 1.3397960662841797\n",
      "epoch:9, batch: 1566,  loss: 1.0079264640808105\n",
      "epoch:9, batch: 1567,  loss: 0.8681974411010742\n",
      "epoch:9, batch: 1568,  loss: 0.9186075925827026\n",
      "epoch:9, batch: 1569,  loss: 0.9766668677330017\n",
      "epoch:9, batch: 1570,  loss: 0.8766767978668213\n",
      "epoch:9, batch: 1571,  loss: 0.8460788726806641\n",
      "epoch:9, batch: 1572,  loss: 0.810070276260376\n",
      "epoch:9, batch: 1573,  loss: 0.8171898722648621\n",
      "epoch:9, batch: 1574,  loss: 1.4641693830490112\n",
      "epoch:9, batch: 1575,  loss: 0.7234914302825928\n",
      "epoch:9, batch: 1576,  loss: 0.8271737694740295\n",
      "epoch:9, batch: 1577,  loss: 1.064241647720337\n",
      "epoch:9, batch: 1578,  loss: 1.329412579536438\n",
      "epoch:9, batch: 1579,  loss: 0.9699799418449402\n",
      "epoch:9, batch: 1580,  loss: 0.8750920295715332\n",
      "epoch:9, batch: 1581,  loss: 1.2345263957977295\n",
      "epoch:9, batch: 1582,  loss: 1.0527232885360718\n",
      "epoch:9, batch: 1583,  loss: 1.1107661724090576\n",
      "epoch:9, batch: 1584,  loss: 0.8128699064254761\n",
      "epoch:9, batch: 1585,  loss: 1.0874662399291992\n",
      "epoch:9, batch: 1586,  loss: 1.1411725282669067\n",
      "epoch:9, batch: 1587,  loss: 0.8404844999313354\n",
      "epoch:9, batch: 1588,  loss: 1.1020023822784424\n",
      "epoch:9, batch: 1589,  loss: 0.8736023306846619\n",
      "epoch:9, batch: 1590,  loss: 1.0860518217086792\n",
      "epoch:9, batch: 1591,  loss: 0.915204644203186\n",
      "epoch:9, batch: 1592,  loss: 0.6844637393951416\n",
      "epoch:9, batch: 1593,  loss: 0.9757330417633057\n",
      "epoch:9, batch: 1594,  loss: 0.8472932577133179\n",
      "epoch:9, batch: 1595,  loss: 1.0246235132217407\n",
      "epoch:9, batch: 1596,  loss: 1.1802756786346436\n",
      "epoch:9, batch: 1597,  loss: 0.902863085269928\n",
      "epoch:9, batch: 1598,  loss: 1.2180801630020142\n",
      "epoch:9, batch: 1599,  loss: 0.8227648138999939\n",
      "epoch:9, batch: 1600,  loss: 1.0989487171173096\n",
      "epoch:9, batch: 1601,  loss: 0.8755182027816772\n",
      "epoch:9, batch: 1602,  loss: 1.1172899007797241\n",
      "epoch:9, batch: 1603,  loss: 1.0472437143325806\n",
      "epoch:9, batch: 1604,  loss: 0.8947405219078064\n",
      "epoch:9, batch: 1605,  loss: 1.0357815027236938\n",
      "epoch:9, batch: 1606,  loss: 1.0600312948226929\n",
      "epoch:9, batch: 1607,  loss: 0.921558141708374\n",
      "epoch:9, batch: 1608,  loss: 1.0088715553283691\n",
      "epoch:9, batch: 1609,  loss: 1.1478379964828491\n",
      "epoch:9, batch: 1610,  loss: 0.7108509540557861\n",
      "epoch:9, batch: 1611,  loss: 0.9008544087409973\n",
      "epoch:9, batch: 1612,  loss: 1.007041096687317\n",
      "epoch:9, batch: 1613,  loss: 0.8709730505943298\n",
      "epoch:9, batch: 1614,  loss: 1.0314075946807861\n",
      "epoch:9, batch: 1615,  loss: 0.953037440776825\n",
      "epoch:9, batch: 1616,  loss: 1.0094329118728638\n",
      "epoch:9, batch: 1617,  loss: 1.0603954792022705\n",
      "epoch:9, batch: 1618,  loss: 1.6534266471862793\n",
      "epoch:9, batch: 1619,  loss: 1.1287856101989746\n",
      "epoch:9, batch: 1620,  loss: 1.104678988456726\n",
      "epoch:9, batch: 1621,  loss: 1.0017988681793213\n",
      "epoch:9, batch: 1622,  loss: 0.823733389377594\n",
      "epoch:9, batch: 1623,  loss: 0.7680644989013672\n",
      "epoch:9, batch: 1624,  loss: 0.9834691882133484\n",
      "epoch:9, batch: 1625,  loss: 1.0491602420806885\n",
      "epoch:9, batch: 1626,  loss: 1.140356183052063\n",
      "epoch:9, batch: 1627,  loss: 1.3842976093292236\n",
      "epoch:9, batch: 1628,  loss: 1.2510145902633667\n",
      "epoch:9, batch: 1629,  loss: 0.9914788007736206\n",
      "epoch:9, batch: 1630,  loss: 1.1546119451522827\n",
      "epoch:9, batch: 1631,  loss: 0.9816928505897522\n",
      "epoch:9, batch: 1632,  loss: 1.2684247493743896\n",
      "epoch:9, batch: 1633,  loss: 1.0752110481262207\n",
      "epoch:9, batch: 1634,  loss: 1.1550110578536987\n",
      "epoch:9, batch: 1635,  loss: 1.0843418836593628\n",
      "epoch:9, batch: 1636,  loss: 1.0636942386627197\n",
      "epoch:9, batch: 1637,  loss: 1.4127626419067383\n",
      "epoch:9, batch: 1638,  loss: 1.0335757732391357\n",
      "epoch:9, batch: 1639,  loss: 0.9185724258422852\n",
      "epoch:9, batch: 1640,  loss: 0.8728561997413635\n",
      "epoch:9, batch: 1641,  loss: 0.9860954284667969\n",
      "epoch:9, batch: 1642,  loss: 1.109631061553955\n",
      "epoch:9, batch: 1643,  loss: 0.7893490195274353\n",
      "epoch:9, batch: 1644,  loss: 1.0053212642669678\n",
      "epoch:9, batch: 1645,  loss: 0.9357444643974304\n",
      "epoch:9, batch: 1646,  loss: 1.0545567274093628\n",
      "epoch:9, batch: 1647,  loss: 1.2668721675872803\n",
      "epoch:9, batch: 1648,  loss: 1.174181580543518\n",
      "epoch:9, batch: 1649,  loss: 1.2060527801513672\n",
      "epoch:9, batch: 1650,  loss: 0.8676876425743103\n",
      "epoch:9, batch: 1651,  loss: 0.787582278251648\n",
      "epoch:9, batch: 1652,  loss: 0.9929757118225098\n",
      "epoch:9, batch: 1653,  loss: 1.1318343877792358\n",
      "epoch:9, batch: 1654,  loss: 1.1513160467147827\n",
      "epoch:9, batch: 1655,  loss: 0.81407231092453\n",
      "epoch:9, batch: 1656,  loss: 1.096929669380188\n",
      "epoch:9, batch: 1657,  loss: 1.0521745681762695\n",
      "epoch:9, batch: 1658,  loss: 1.1067817211151123\n",
      "epoch:9, batch: 1659,  loss: 0.9412855505943298\n",
      "epoch:9, batch: 1660,  loss: 0.8743700385093689\n",
      "epoch:9, batch: 1661,  loss: 0.981264591217041\n",
      "epoch:9, batch: 1662,  loss: 0.957231879234314\n",
      "epoch:9, batch: 1663,  loss: 1.1854304075241089\n",
      "epoch:9, batch: 1664,  loss: 0.9550402164459229\n",
      "epoch:9, batch: 1665,  loss: 0.9082411527633667\n",
      "epoch:9, batch: 1666,  loss: 0.7981254458427429\n",
      "epoch:9, batch: 1667,  loss: 1.1212811470031738\n",
      "epoch:9, batch: 1668,  loss: 1.017935037612915\n",
      "epoch:9, batch: 1669,  loss: 1.072195053100586\n",
      "epoch:9, batch: 1670,  loss: 0.815951943397522\n",
      "epoch:9, batch: 1671,  loss: 1.3385851383209229\n",
      "epoch:9, batch: 1672,  loss: 1.2289701700210571\n",
      "epoch:9, batch: 1673,  loss: 0.9773352146148682\n",
      "epoch:9, batch: 1674,  loss: 1.1803562641143799\n",
      "epoch:9, batch: 1675,  loss: 1.1294664144515991\n",
      "epoch:9, batch: 1676,  loss: 1.1226494312286377\n",
      "epoch:9, batch: 1677,  loss: 1.269282579421997\n",
      "epoch:9, batch: 1678,  loss: 0.9611569046974182\n",
      "epoch:9, batch: 1679,  loss: 0.8607397675514221\n",
      "epoch:9, batch: 1680,  loss: 0.7294740676879883\n",
      "epoch:9, batch: 1681,  loss: 0.9092778563499451\n",
      "epoch:9, batch: 1682,  loss: 0.7690451145172119\n",
      "epoch:9, batch: 1683,  loss: 1.01739501953125\n",
      "epoch:9, batch: 1684,  loss: 1.1853798627853394\n",
      "epoch:9, batch: 1685,  loss: 1.2667644023895264\n",
      "epoch:9, batch: 1686,  loss: 1.1309654712677002\n",
      "epoch:9, batch: 1687,  loss: 1.0201002359390259\n",
      "epoch:9, batch: 1688,  loss: 0.8634141087532043\n",
      "epoch:9, batch: 1689,  loss: 0.9852838516235352\n",
      "epoch:9, batch: 1690,  loss: 1.0847586393356323\n",
      "epoch:9, batch: 1691,  loss: 0.9400056600570679\n",
      "epoch:9, batch: 1692,  loss: 1.1287291049957275\n",
      "epoch:9, batch: 1693,  loss: 1.1870183944702148\n",
      "epoch:9, batch: 1694,  loss: 1.003416657447815\n",
      "epoch:9, batch: 1695,  loss: 0.8419826626777649\n",
      "epoch:9, batch: 1696,  loss: 0.8768590092658997\n",
      "epoch:9, batch: 1697,  loss: 0.9360358119010925\n",
      "epoch:9, batch: 1698,  loss: 1.1010957956314087\n",
      "epoch:9, batch: 1699,  loss: 0.9839258790016174\n",
      "epoch:9, batch: 1700,  loss: 0.875857949256897\n",
      "epoch:9, batch: 1701,  loss: 1.1764068603515625\n",
      "epoch:9, batch: 1702,  loss: 1.1751761436462402\n",
      "epoch:9, batch: 1703,  loss: 1.3950127363204956\n",
      "epoch:9, batch: 1704,  loss: 0.8585782647132874\n",
      "epoch:9, batch: 1705,  loss: 1.2394077777862549\n",
      "epoch:9, batch: 1706,  loss: 0.993428647518158\n",
      "epoch:9, batch: 1707,  loss: 1.1595160961151123\n",
      "epoch:9, batch: 1708,  loss: 1.1420804262161255\n",
      "epoch:9, batch: 1709,  loss: 1.1149015426635742\n",
      "epoch:9, batch: 1710,  loss: 0.6632622480392456\n",
      "epoch:9, batch: 1711,  loss: 0.7966113090515137\n",
      "epoch:9, batch: 1712,  loss: 0.9111031293869019\n",
      "epoch:9, batch: 1713,  loss: 1.1202988624572754\n",
      "epoch:9, batch: 1714,  loss: 1.0799471139907837\n",
      "epoch:9, batch: 1715,  loss: 1.1993939876556396\n",
      "epoch:9, batch: 1716,  loss: 0.8909716010093689\n",
      "epoch:9, batch: 1717,  loss: 1.2254722118377686\n",
      "epoch:9, batch: 1718,  loss: 0.883111298084259\n",
      "epoch:9, batch: 1719,  loss: 1.1098500490188599\n",
      "epoch:9, batch: 1720,  loss: 1.0833370685577393\n",
      "epoch:9, batch: 1721,  loss: 1.0187442302703857\n",
      "epoch:9, batch: 1722,  loss: 0.8030374646186829\n",
      "epoch:9, batch: 1723,  loss: 1.0479830503463745\n",
      "epoch:9, batch: 1724,  loss: 0.8928976058959961\n",
      "epoch:9, batch: 1725,  loss: 1.3606759309768677\n",
      "epoch:9, batch: 1726,  loss: 0.9908032417297363\n",
      "epoch:9, batch: 1727,  loss: 1.435645580291748\n",
      "epoch:9, batch: 1728,  loss: 0.8389002084732056\n",
      "epoch:9, batch: 1729,  loss: 0.9856677651405334\n",
      "epoch:9, batch: 1730,  loss: 1.175297498703003\n",
      "epoch:9, batch: 1731,  loss: 1.3547744750976562\n",
      "epoch:9, batch: 1732,  loss: 0.882519006729126\n",
      "epoch:9, batch: 1733,  loss: 1.062842845916748\n",
      "epoch:9, batch: 1734,  loss: 1.0483845472335815\n",
      "epoch:9, batch: 1735,  loss: 0.8165373206138611\n",
      "epoch:9, batch: 1736,  loss: 1.3695433139801025\n",
      "epoch:9, batch: 1737,  loss: 0.9284049868583679\n",
      "epoch:9, batch: 1738,  loss: 1.3499730825424194\n",
      "epoch:9, batch: 1739,  loss: 0.898102343082428\n",
      "epoch:9, batch: 1740,  loss: 1.056731939315796\n",
      "epoch:9, batch: 1741,  loss: 1.035178780555725\n",
      "epoch:9, batch: 1742,  loss: 0.8754804730415344\n",
      "epoch:9, batch: 1743,  loss: 1.0963826179504395\n",
      "epoch:9, batch: 1744,  loss: 0.9508435130119324\n",
      "epoch:9, batch: 1745,  loss: 1.105014681816101\n",
      "epoch:9, batch: 1746,  loss: 0.9136644601821899\n",
      "epoch:9, batch: 1747,  loss: 0.7649497985839844\n",
      "epoch:9, batch: 1748,  loss: 1.1186585426330566\n",
      "epoch:9, batch: 1749,  loss: 1.0099263191223145\n",
      "epoch:9, batch: 1750,  loss: 0.9000205397605896\n",
      "epoch:9, batch: 1751,  loss: 0.9252139329910278\n",
      "epoch:9, batch: 1752,  loss: 1.3919779062271118\n",
      "epoch:9, batch: 1753,  loss: 0.916246771812439\n",
      "epoch:9, batch: 1754,  loss: 0.811741828918457\n",
      "epoch:9, batch: 1755,  loss: 0.815387487411499\n",
      "epoch:9, batch: 1756,  loss: 1.0938490629196167\n",
      "epoch:9, batch: 1757,  loss: 1.093032717704773\n",
      "epoch:9, batch: 1758,  loss: 1.1010098457336426\n",
      "epoch:9, batch: 1759,  loss: 1.114905834197998\n",
      "epoch:9, batch: 1760,  loss: 1.339266300201416\n",
      "epoch:9, batch: 1761,  loss: 0.7635144591331482\n",
      "epoch:9, batch: 1762,  loss: 0.7724723815917969\n",
      "epoch:9, batch: 1763,  loss: 1.1534440517425537\n",
      "epoch:10, batch: 1,  loss: 0.9783426523208618\n",
      "epoch:10, batch: 2,  loss: 0.9905564188957214\n",
      "epoch:10, batch: 3,  loss: 0.6738950610160828\n",
      "epoch:10, batch: 4,  loss: 0.8726931810379028\n",
      "epoch:10, batch: 5,  loss: 1.0064265727996826\n",
      "epoch:10, batch: 6,  loss: 1.0899120569229126\n",
      "epoch:10, batch: 7,  loss: 0.8789033889770508\n",
      "epoch:10, batch: 8,  loss: 1.279839277267456\n",
      "epoch:10, batch: 9,  loss: 0.9803433418273926\n",
      "epoch:10, batch: 10,  loss: 0.8116715550422668\n",
      "epoch:10, batch: 11,  loss: 0.9992623329162598\n",
      "epoch:10, batch: 12,  loss: 1.2025485038757324\n",
      "epoch:10, batch: 13,  loss: 1.064437985420227\n",
      "epoch:10, batch: 14,  loss: 1.2084780931472778\n",
      "epoch:10, batch: 15,  loss: 1.2819669246673584\n",
      "epoch:10, batch: 16,  loss: 1.1324535608291626\n",
      "epoch:10, batch: 17,  loss: 0.9011018872261047\n",
      "epoch:10, batch: 18,  loss: 0.9697877168655396\n",
      "epoch:10, batch: 19,  loss: 0.8709849119186401\n",
      "epoch:10, batch: 20,  loss: 1.2347486019134521\n",
      "epoch:10, batch: 21,  loss: 1.007867693901062\n",
      "epoch:10, batch: 22,  loss: 0.8739251494407654\n",
      "epoch:10, batch: 23,  loss: 1.0216786861419678\n",
      "epoch:10, batch: 24,  loss: 0.9607119560241699\n",
      "epoch:10, batch: 25,  loss: 1.2165143489837646\n",
      "epoch:10, batch: 26,  loss: 0.9737817049026489\n",
      "epoch:10, batch: 27,  loss: 1.1111046075820923\n",
      "epoch:10, batch: 28,  loss: 0.9443590044975281\n",
      "epoch:10, batch: 29,  loss: 0.8996432423591614\n",
      "epoch:10, batch: 30,  loss: 1.0759034156799316\n",
      "epoch:10, batch: 31,  loss: 0.87007075548172\n",
      "epoch:10, batch: 32,  loss: 1.0949348211288452\n",
      "epoch:10, batch: 33,  loss: 1.0021544694900513\n",
      "epoch:10, batch: 34,  loss: 1.1237878799438477\n",
      "epoch:10, batch: 35,  loss: 1.083200454711914\n",
      "epoch:10, batch: 36,  loss: 0.9803045988082886\n",
      "epoch:10, batch: 37,  loss: 1.243481159210205\n",
      "epoch:10, batch: 38,  loss: 0.779518187046051\n",
      "epoch:10, batch: 39,  loss: 0.8016780018806458\n",
      "epoch:10, batch: 40,  loss: 1.376103162765503\n",
      "epoch:10, batch: 41,  loss: 1.0111632347106934\n",
      "epoch:10, batch: 42,  loss: 1.0408655405044556\n",
      "epoch:10, batch: 43,  loss: 1.168764352798462\n",
      "epoch:10, batch: 44,  loss: 0.8333988785743713\n",
      "epoch:10, batch: 45,  loss: 1.3983772993087769\n",
      "epoch:10, batch: 46,  loss: 0.9327254891395569\n",
      "epoch:10, batch: 47,  loss: 1.105056643486023\n",
      "epoch:10, batch: 48,  loss: 1.0037150382995605\n",
      "epoch:10, batch: 49,  loss: 1.168214201927185\n",
      "epoch:10, batch: 50,  loss: 0.9103679656982422\n",
      "epoch:10, batch: 51,  loss: 1.0469965934753418\n",
      "epoch:10, batch: 52,  loss: 0.7240598201751709\n",
      "epoch:10, batch: 53,  loss: 0.8725289106369019\n",
      "epoch:10, batch: 54,  loss: 1.249189019203186\n",
      "epoch:10, batch: 55,  loss: 0.8746972680091858\n",
      "epoch:10, batch: 56,  loss: 0.9833130836486816\n",
      "epoch:10, batch: 57,  loss: 0.8340575098991394\n",
      "epoch:10, batch: 58,  loss: 1.0686061382293701\n",
      "epoch:10, batch: 59,  loss: 0.7727658152580261\n",
      "epoch:10, batch: 60,  loss: 1.1714259386062622\n",
      "epoch:10, batch: 61,  loss: 0.7912500500679016\n",
      "epoch:10, batch: 62,  loss: 0.9624717235565186\n",
      "epoch:10, batch: 63,  loss: 1.0659714937210083\n",
      "epoch:10, batch: 64,  loss: 0.7775808572769165\n",
      "epoch:10, batch: 65,  loss: 0.8451650738716125\n",
      "epoch:10, batch: 66,  loss: 0.9266304969787598\n",
      "epoch:10, batch: 67,  loss: 0.9919453859329224\n",
      "epoch:10, batch: 68,  loss: 0.9214916229248047\n",
      "epoch:10, batch: 69,  loss: 1.0696793794631958\n",
      "epoch:10, batch: 70,  loss: 1.1967172622680664\n",
      "epoch:10, batch: 71,  loss: 0.9952271580696106\n",
      "epoch:10, batch: 72,  loss: 0.8219161033630371\n",
      "epoch:10, batch: 73,  loss: 0.8910853266716003\n",
      "epoch:10, batch: 74,  loss: 0.9938783645629883\n",
      "epoch:10, batch: 75,  loss: 1.070874810218811\n",
      "epoch:10, batch: 76,  loss: 1.0748283863067627\n",
      "epoch:10, batch: 77,  loss: 1.115038514137268\n",
      "epoch:10, batch: 78,  loss: 0.9414910674095154\n",
      "epoch:10, batch: 79,  loss: 1.0753744840621948\n",
      "epoch:10, batch: 80,  loss: 1.1798394918441772\n",
      "epoch:10, batch: 81,  loss: 1.2816150188446045\n",
      "epoch:10, batch: 82,  loss: 0.862446665763855\n",
      "epoch:10, batch: 83,  loss: 1.1217405796051025\n",
      "epoch:10, batch: 84,  loss: 0.9088690876960754\n",
      "epoch:10, batch: 85,  loss: 0.9169585704803467\n",
      "epoch:10, batch: 86,  loss: 1.1345289945602417\n",
      "epoch:10, batch: 87,  loss: 1.178857445716858\n",
      "epoch:10, batch: 88,  loss: 1.1401704549789429\n",
      "epoch:10, batch: 89,  loss: 0.8779303431510925\n",
      "epoch:10, batch: 90,  loss: 0.9946399331092834\n",
      "epoch:10, batch: 91,  loss: 1.1232092380523682\n",
      "epoch:10, batch: 92,  loss: 0.6694443821907043\n",
      "epoch:10, batch: 93,  loss: 1.1530463695526123\n",
      "epoch:10, batch: 94,  loss: 0.9267396926879883\n",
      "epoch:10, batch: 95,  loss: 0.9385678768157959\n",
      "epoch:10, batch: 96,  loss: 1.0240426063537598\n",
      "epoch:10, batch: 97,  loss: 1.260229468345642\n",
      "epoch:10, batch: 98,  loss: 0.9790666699409485\n",
      "epoch:10, batch: 99,  loss: 1.2215903997421265\n",
      "epoch:10, batch: 100,  loss: 1.0211070775985718\n",
      "epoch:10, batch: 101,  loss: 0.9866158962249756\n",
      "epoch:10, batch: 102,  loss: 1.357725739479065\n",
      "epoch:10, batch: 103,  loss: 1.1546032428741455\n",
      "epoch:10, batch: 104,  loss: 1.09531831741333\n",
      "epoch:10, batch: 105,  loss: 1.0528098344802856\n",
      "epoch:10, batch: 106,  loss: 0.9906217455863953\n",
      "epoch:10, batch: 107,  loss: 1.2834292650222778\n",
      "epoch:10, batch: 108,  loss: 0.9447822570800781\n",
      "epoch:10, batch: 109,  loss: 0.954910159111023\n",
      "epoch:10, batch: 110,  loss: 0.8999180793762207\n",
      "epoch:10, batch: 111,  loss: 0.9044787287712097\n",
      "epoch:10, batch: 112,  loss: 0.9769771099090576\n",
      "epoch:10, batch: 113,  loss: 0.8609386682510376\n",
      "epoch:10, batch: 114,  loss: 1.3293919563293457\n",
      "epoch:10, batch: 115,  loss: 0.7791662216186523\n",
      "epoch:10, batch: 116,  loss: 1.0808639526367188\n",
      "epoch:10, batch: 117,  loss: 1.325689435005188\n",
      "epoch:10, batch: 118,  loss: 0.9104151725769043\n",
      "epoch:10, batch: 119,  loss: 0.818592369556427\n",
      "epoch:10, batch: 120,  loss: 0.9805593490600586\n",
      "epoch:10, batch: 121,  loss: 1.1866563558578491\n",
      "epoch:10, batch: 122,  loss: 0.8179919123649597\n",
      "epoch:10, batch: 123,  loss: 1.0625489950180054\n",
      "epoch:10, batch: 124,  loss: 0.9909490942955017\n",
      "epoch:10, batch: 125,  loss: 0.9567940831184387\n",
      "epoch:10, batch: 126,  loss: 1.143059492111206\n",
      "epoch:10, batch: 127,  loss: 1.0307716131210327\n",
      "epoch:10, batch: 128,  loss: 1.0235697031021118\n",
      "epoch:10, batch: 129,  loss: 1.068109154701233\n",
      "epoch:10, batch: 130,  loss: 0.9291385412216187\n",
      "epoch:10, batch: 131,  loss: 1.0095751285552979\n",
      "epoch:10, batch: 132,  loss: 1.0713298320770264\n",
      "epoch:10, batch: 133,  loss: 1.003101110458374\n",
      "epoch:10, batch: 134,  loss: 0.984914243221283\n",
      "epoch:10, batch: 135,  loss: 0.974628746509552\n",
      "epoch:10, batch: 136,  loss: 0.7682763338088989\n",
      "epoch:10, batch: 137,  loss: 1.0321139097213745\n",
      "epoch:10, batch: 138,  loss: 0.9143189191818237\n",
      "epoch:10, batch: 139,  loss: 0.9594731330871582\n",
      "epoch:10, batch: 140,  loss: 1.076850175857544\n",
      "epoch:10, batch: 141,  loss: 0.8726537227630615\n",
      "epoch:10, batch: 142,  loss: 0.9568514227867126\n",
      "epoch:10, batch: 143,  loss: 1.0035972595214844\n",
      "epoch:10, batch: 144,  loss: 1.0628377199172974\n",
      "epoch:10, batch: 145,  loss: 1.0288901329040527\n",
      "epoch:10, batch: 146,  loss: 1.030057430267334\n",
      "epoch:10, batch: 147,  loss: 1.1951112747192383\n",
      "epoch:10, batch: 148,  loss: 0.9345775246620178\n",
      "epoch:10, batch: 149,  loss: 0.8362547755241394\n",
      "epoch:10, batch: 150,  loss: 1.002555251121521\n",
      "epoch:10, batch: 151,  loss: 0.9066903591156006\n",
      "epoch:10, batch: 152,  loss: 1.1593199968338013\n",
      "epoch:10, batch: 153,  loss: 1.3007783889770508\n",
      "epoch:10, batch: 154,  loss: 0.8247435688972473\n",
      "epoch:10, batch: 155,  loss: 0.9462630748748779\n",
      "epoch:10, batch: 156,  loss: 0.8458777070045471\n",
      "epoch:10, batch: 157,  loss: 0.9369678497314453\n",
      "epoch:10, batch: 158,  loss: 0.9357985854148865\n",
      "epoch:10, batch: 159,  loss: 0.9758148193359375\n",
      "epoch:10, batch: 160,  loss: 1.124401569366455\n",
      "epoch:10, batch: 161,  loss: 0.8890472054481506\n",
      "epoch:10, batch: 162,  loss: 0.9110791087150574\n",
      "epoch:10, batch: 163,  loss: 1.107134461402893\n",
      "epoch:10, batch: 164,  loss: 0.8063697814941406\n",
      "epoch:10, batch: 165,  loss: 0.8019688725471497\n",
      "epoch:10, batch: 166,  loss: 1.2448360919952393\n",
      "epoch:10, batch: 167,  loss: 0.8672970533370972\n",
      "epoch:10, batch: 168,  loss: 1.1460113525390625\n",
      "epoch:10, batch: 169,  loss: 1.042087435722351\n",
      "epoch:10, batch: 170,  loss: 1.0511671304702759\n",
      "epoch:10, batch: 171,  loss: 1.1531065702438354\n",
      "epoch:10, batch: 172,  loss: 1.2685734033584595\n",
      "epoch:10, batch: 173,  loss: 1.2388818264007568\n",
      "epoch:10, batch: 174,  loss: 1.1617504358291626\n",
      "epoch:10, batch: 175,  loss: 1.068887710571289\n",
      "epoch:10, batch: 176,  loss: 1.1583850383758545\n",
      "epoch:10, batch: 177,  loss: 1.0691742897033691\n",
      "epoch:10, batch: 178,  loss: 1.153482437133789\n",
      "epoch:10, batch: 179,  loss: 0.8942301273345947\n",
      "epoch:10, batch: 180,  loss: 0.9660472869873047\n",
      "epoch:10, batch: 181,  loss: 1.2709851264953613\n",
      "epoch:10, batch: 182,  loss: 1.3171433210372925\n",
      "epoch:10, batch: 183,  loss: 0.9094175696372986\n",
      "epoch:10, batch: 184,  loss: 1.0780214071273804\n",
      "epoch:10, batch: 185,  loss: 1.0991449356079102\n",
      "epoch:10, batch: 186,  loss: 1.0269883871078491\n",
      "epoch:10, batch: 187,  loss: 1.1994215250015259\n",
      "epoch:10, batch: 188,  loss: 0.9926899075508118\n",
      "epoch:10, batch: 189,  loss: 0.8637493848800659\n",
      "epoch:10, batch: 190,  loss: 1.0814661979675293\n",
      "epoch:10, batch: 191,  loss: 0.8710676431655884\n",
      "epoch:10, batch: 192,  loss: 1.2566074132919312\n",
      "epoch:10, batch: 193,  loss: 0.9745395183563232\n",
      "epoch:10, batch: 194,  loss: 0.915410041809082\n",
      "epoch:10, batch: 195,  loss: 0.9470281600952148\n",
      "epoch:10, batch: 196,  loss: 0.9827701449394226\n",
      "epoch:10, batch: 197,  loss: 0.9143355488777161\n",
      "epoch:10, batch: 198,  loss: 1.233391523361206\n",
      "epoch:10, batch: 199,  loss: 0.960690438747406\n",
      "epoch:10, batch: 200,  loss: 0.7847694754600525\n",
      "epoch:10, batch: 201,  loss: 1.1877561807632446\n",
      "epoch:10, batch: 202,  loss: 0.8568603992462158\n",
      "epoch:10, batch: 203,  loss: 1.0077838897705078\n",
      "epoch:10, batch: 204,  loss: 1.3406281471252441\n",
      "epoch:10, batch: 205,  loss: 0.8551069498062134\n",
      "epoch:10, batch: 206,  loss: 1.129819393157959\n",
      "epoch:10, batch: 207,  loss: 1.2582030296325684\n",
      "epoch:10, batch: 208,  loss: 1.0268152952194214\n",
      "epoch:10, batch: 209,  loss: 1.1395899057388306\n",
      "epoch:10, batch: 210,  loss: 0.8672784566879272\n",
      "epoch:10, batch: 211,  loss: 1.1041526794433594\n",
      "epoch:10, batch: 212,  loss: 1.0370877981185913\n",
      "epoch:10, batch: 213,  loss: 0.8394216299057007\n",
      "epoch:10, batch: 214,  loss: 0.8744935393333435\n",
      "epoch:10, batch: 215,  loss: 1.3260258436203003\n",
      "epoch:10, batch: 216,  loss: 0.9901252388954163\n",
      "epoch:10, batch: 217,  loss: 0.8343949913978577\n",
      "epoch:10, batch: 218,  loss: 0.9563884735107422\n",
      "epoch:10, batch: 219,  loss: 1.0049116611480713\n",
      "epoch:10, batch: 220,  loss: 0.7189062237739563\n",
      "epoch:10, batch: 221,  loss: 0.9086230397224426\n",
      "epoch:10, batch: 222,  loss: 1.2701793909072876\n",
      "epoch:10, batch: 223,  loss: 1.2494901418685913\n",
      "epoch:10, batch: 224,  loss: 1.0797656774520874\n",
      "epoch:10, batch: 225,  loss: 0.7772718667984009\n",
      "epoch:10, batch: 226,  loss: 0.973635733127594\n",
      "epoch:10, batch: 227,  loss: 1.4407708644866943\n",
      "epoch:10, batch: 228,  loss: 1.0552297830581665\n",
      "epoch:10, batch: 229,  loss: 0.9745562672615051\n",
      "epoch:10, batch: 230,  loss: 0.6977182030677795\n",
      "epoch:10, batch: 231,  loss: 1.3417514562606812\n",
      "epoch:10, batch: 232,  loss: 0.9478675723075867\n",
      "epoch:10, batch: 233,  loss: 0.7944367527961731\n",
      "epoch:10, batch: 234,  loss: 0.720590353012085\n",
      "epoch:10, batch: 235,  loss: 0.9859296679496765\n",
      "epoch:10, batch: 236,  loss: 1.1018279790878296\n",
      "epoch:10, batch: 237,  loss: 1.0770063400268555\n",
      "epoch:10, batch: 238,  loss: 1.0108205080032349\n",
      "epoch:10, batch: 239,  loss: 1.2276369333267212\n",
      "epoch:10, batch: 240,  loss: 0.9968677759170532\n",
      "epoch:10, batch: 241,  loss: 0.8126751780509949\n",
      "epoch:10, batch: 242,  loss: 0.9768909215927124\n",
      "epoch:10, batch: 243,  loss: 0.9454413056373596\n",
      "epoch:10, batch: 244,  loss: 0.8389192223548889\n",
      "epoch:10, batch: 245,  loss: 0.9477155804634094\n",
      "epoch:10, batch: 246,  loss: 1.0462075471878052\n",
      "epoch:10, batch: 247,  loss: 0.7238370776176453\n",
      "epoch:10, batch: 248,  loss: 0.9857519865036011\n",
      "epoch:10, batch: 249,  loss: 1.089677095413208\n",
      "epoch:10, batch: 250,  loss: 0.9584502577781677\n",
      "epoch:10, batch: 251,  loss: 0.9243783354759216\n",
      "epoch:10, batch: 252,  loss: 0.8468453288078308\n",
      "epoch:10, batch: 253,  loss: 1.0097455978393555\n",
      "epoch:10, batch: 254,  loss: 0.8131508231163025\n",
      "epoch:10, batch: 255,  loss: 1.022095799446106\n",
      "epoch:10, batch: 256,  loss: 0.9349733591079712\n",
      "epoch:10, batch: 257,  loss: 1.0711638927459717\n",
      "epoch:10, batch: 258,  loss: 1.0329192876815796\n",
      "epoch:10, batch: 259,  loss: 1.0320508480072021\n",
      "epoch:10, batch: 260,  loss: 1.226090431213379\n",
      "epoch:10, batch: 261,  loss: 0.9526936411857605\n",
      "epoch:10, batch: 262,  loss: 1.0097548961639404\n",
      "epoch:10, batch: 263,  loss: 0.9709223508834839\n",
      "epoch:10, batch: 264,  loss: 0.760922908782959\n",
      "epoch:10, batch: 265,  loss: 0.8510214686393738\n",
      "epoch:10, batch: 266,  loss: 1.0312272310256958\n",
      "epoch:10, batch: 267,  loss: 1.0298008918762207\n",
      "epoch:10, batch: 268,  loss: 1.1021363735198975\n",
      "epoch:10, batch: 269,  loss: 0.8251659870147705\n",
      "epoch:10, batch: 270,  loss: 0.9689081311225891\n",
      "epoch:10, batch: 271,  loss: 0.8106650114059448\n",
      "epoch:10, batch: 272,  loss: 1.0954968929290771\n",
      "epoch:10, batch: 273,  loss: 0.9806570410728455\n",
      "epoch:10, batch: 274,  loss: 0.8879507780075073\n",
      "epoch:10, batch: 275,  loss: 1.2501238584518433\n",
      "epoch:10, batch: 276,  loss: 1.0271446704864502\n",
      "epoch:10, batch: 277,  loss: 0.9741281867027283\n",
      "epoch:10, batch: 278,  loss: 0.9448408484458923\n",
      "epoch:10, batch: 279,  loss: 1.2175897359848022\n",
      "epoch:10, batch: 280,  loss: 1.0077296495437622\n",
      "epoch:10, batch: 281,  loss: 1.249908685684204\n",
      "epoch:10, batch: 282,  loss: 0.9054098129272461\n",
      "epoch:10, batch: 283,  loss: 1.0673341751098633\n",
      "epoch:10, batch: 284,  loss: 0.7979772090911865\n",
      "epoch:10, batch: 285,  loss: 1.0260565280914307\n",
      "epoch:10, batch: 286,  loss: 0.8527698516845703\n",
      "epoch:10, batch: 287,  loss: 1.424821138381958\n",
      "epoch:10, batch: 288,  loss: 1.0254284143447876\n",
      "epoch:10, batch: 289,  loss: 0.8153554201126099\n",
      "epoch:10, batch: 290,  loss: 1.1347520351409912\n",
      "epoch:10, batch: 291,  loss: 0.8270794749259949\n",
      "epoch:10, batch: 292,  loss: 0.9917804002761841\n",
      "epoch:10, batch: 293,  loss: 1.0121448040008545\n",
      "epoch:10, batch: 294,  loss: 0.9262638688087463\n",
      "epoch:10, batch: 295,  loss: 0.7774314880371094\n",
      "epoch:10, batch: 296,  loss: 0.7594095468521118\n",
      "epoch:10, batch: 297,  loss: 1.1704490184783936\n",
      "epoch:10, batch: 298,  loss: 1.374411702156067\n",
      "epoch:10, batch: 299,  loss: 1.163212776184082\n",
      "epoch:10, batch: 300,  loss: 1.0561352968215942\n",
      "epoch:10, batch: 301,  loss: 0.7975119948387146\n",
      "epoch:10, batch: 302,  loss: 1.222557544708252\n",
      "epoch:10, batch: 303,  loss: 0.9086356163024902\n",
      "epoch:10, batch: 304,  loss: 1.275109887123108\n",
      "epoch:10, batch: 305,  loss: 1.1006410121917725\n",
      "epoch:10, batch: 306,  loss: 1.2265095710754395\n",
      "epoch:10, batch: 307,  loss: 0.8381759524345398\n",
      "epoch:10, batch: 308,  loss: 0.8065710067749023\n",
      "epoch:10, batch: 309,  loss: 0.839939534664154\n",
      "epoch:10, batch: 310,  loss: 0.7392578721046448\n",
      "epoch:10, batch: 311,  loss: 1.060422420501709\n",
      "epoch:10, batch: 312,  loss: 0.8715764284133911\n",
      "epoch:10, batch: 313,  loss: 1.0938184261322021\n",
      "epoch:10, batch: 314,  loss: 0.9606978297233582\n",
      "epoch:10, batch: 315,  loss: 0.8029882907867432\n",
      "epoch:10, batch: 316,  loss: 1.2893608808517456\n",
      "epoch:10, batch: 317,  loss: 1.3337862491607666\n",
      "epoch:10, batch: 318,  loss: 0.9335877895355225\n",
      "epoch:10, batch: 319,  loss: 0.8878996968269348\n",
      "epoch:10, batch: 320,  loss: 0.9157623052597046\n",
      "epoch:10, batch: 321,  loss: 0.8142232894897461\n",
      "epoch:10, batch: 322,  loss: 0.9637339115142822\n",
      "epoch:10, batch: 323,  loss: 0.779714822769165\n",
      "epoch:10, batch: 324,  loss: 1.0972226858139038\n",
      "epoch:10, batch: 325,  loss: 1.17515230178833\n",
      "epoch:10, batch: 326,  loss: 1.1749118566513062\n",
      "epoch:10, batch: 327,  loss: 1.1364206075668335\n",
      "epoch:10, batch: 328,  loss: 0.9298861622810364\n",
      "epoch:10, batch: 329,  loss: 0.8448521494865417\n",
      "epoch:10, batch: 330,  loss: 0.7625850439071655\n",
      "epoch:10, batch: 331,  loss: 0.8239704966545105\n",
      "epoch:10, batch: 332,  loss: 1.2944839000701904\n",
      "epoch:10, batch: 333,  loss: 0.9746312499046326\n",
      "epoch:10, batch: 334,  loss: 0.8920694589614868\n",
      "epoch:10, batch: 335,  loss: 0.7681646347045898\n",
      "epoch:10, batch: 336,  loss: 1.2888473272323608\n",
      "epoch:10, batch: 337,  loss: 1.23674738407135\n",
      "epoch:10, batch: 338,  loss: 0.8292117118835449\n",
      "epoch:10, batch: 339,  loss: 1.0700669288635254\n",
      "epoch:10, batch: 340,  loss: 0.9392288327217102\n",
      "epoch:10, batch: 341,  loss: 1.1149804592132568\n",
      "epoch:10, batch: 342,  loss: 1.1893084049224854\n",
      "epoch:10, batch: 343,  loss: 0.7931144833564758\n",
      "epoch:10, batch: 344,  loss: 0.8227367997169495\n",
      "epoch:10, batch: 345,  loss: 0.8537648320198059\n",
      "epoch:10, batch: 346,  loss: 0.9314419031143188\n",
      "epoch:10, batch: 347,  loss: 0.9645996689796448\n",
      "epoch:10, batch: 348,  loss: 0.8515151739120483\n",
      "epoch:10, batch: 349,  loss: 1.1325805187225342\n",
      "epoch:10, batch: 350,  loss: 0.9495996832847595\n",
      "epoch:10, batch: 351,  loss: 0.8133900165557861\n",
      "epoch:10, batch: 352,  loss: 1.4366117715835571\n",
      "epoch:10, batch: 353,  loss: 0.8631109595298767\n",
      "epoch:10, batch: 354,  loss: 0.8840467929840088\n",
      "epoch:10, batch: 355,  loss: 0.9753227829933167\n",
      "epoch:10, batch: 356,  loss: 0.9614729285240173\n",
      "epoch:10, batch: 357,  loss: 1.0847620964050293\n",
      "epoch:10, batch: 358,  loss: 0.9594477415084839\n",
      "epoch:10, batch: 359,  loss: 0.8744617700576782\n",
      "epoch:10, batch: 360,  loss: 1.0352648496627808\n",
      "epoch:10, batch: 361,  loss: 1.049561858177185\n",
      "epoch:10, batch: 362,  loss: 0.9021069407463074\n",
      "epoch:10, batch: 363,  loss: 1.1943519115447998\n",
      "epoch:10, batch: 364,  loss: 1.0421139001846313\n",
      "epoch:10, batch: 365,  loss: 0.9148093461990356\n",
      "epoch:10, batch: 366,  loss: 1.0751217603683472\n",
      "epoch:10, batch: 367,  loss: 1.2343380451202393\n",
      "epoch:10, batch: 368,  loss: 0.8640441298484802\n",
      "epoch:10, batch: 369,  loss: 0.845740020275116\n",
      "epoch:10, batch: 370,  loss: 0.8309071660041809\n",
      "epoch:10, batch: 371,  loss: 1.05038583278656\n",
      "epoch:10, batch: 372,  loss: 0.8948849439620972\n",
      "epoch:10, batch: 373,  loss: 1.0671480894088745\n",
      "epoch:10, batch: 374,  loss: 0.7791891098022461\n",
      "epoch:10, batch: 375,  loss: 1.1426520347595215\n",
      "epoch:10, batch: 376,  loss: 1.29807710647583\n",
      "epoch:10, batch: 377,  loss: 0.9301313757896423\n",
      "epoch:10, batch: 378,  loss: 1.2661941051483154\n",
      "epoch:10, batch: 379,  loss: 1.2359566688537598\n",
      "epoch:10, batch: 380,  loss: 0.8727918863296509\n",
      "epoch:10, batch: 381,  loss: 0.8680358529090881\n",
      "epoch:10, batch: 382,  loss: 1.0394841432571411\n",
      "epoch:10, batch: 383,  loss: 0.9926517009735107\n",
      "epoch:10, batch: 384,  loss: 0.8827891945838928\n",
      "epoch:10, batch: 385,  loss: 1.032820224761963\n",
      "epoch:10, batch: 386,  loss: 0.9460989832878113\n",
      "epoch:10, batch: 387,  loss: 1.0633865594863892\n",
      "epoch:10, batch: 388,  loss: 1.1975713968276978\n",
      "epoch:10, batch: 389,  loss: 1.3199266195297241\n",
      "epoch:10, batch: 390,  loss: 0.9844667315483093\n",
      "epoch:10, batch: 391,  loss: 0.9250041246414185\n",
      "epoch:10, batch: 392,  loss: 1.054628610610962\n",
      "epoch:10, batch: 393,  loss: 0.881402313709259\n",
      "epoch:10, batch: 394,  loss: 1.2077471017837524\n",
      "epoch:10, batch: 395,  loss: 0.8499834537506104\n",
      "epoch:10, batch: 396,  loss: 0.9301531314849854\n",
      "epoch:10, batch: 397,  loss: 0.8672532439231873\n",
      "epoch:10, batch: 398,  loss: 0.9350355267524719\n",
      "epoch:10, batch: 399,  loss: 0.982330322265625\n",
      "epoch:10, batch: 400,  loss: 1.1891425848007202\n",
      "epoch:10, batch: 401,  loss: 1.0562924146652222\n",
      "epoch:10, batch: 402,  loss: 0.9742249846458435\n",
      "epoch:10, batch: 403,  loss: 0.7715973258018494\n",
      "epoch:10, batch: 404,  loss: 0.8685259819030762\n",
      "epoch:10, batch: 405,  loss: 0.9264768958091736\n",
      "epoch:10, batch: 406,  loss: 0.9469014406204224\n",
      "epoch:10, batch: 407,  loss: 0.9482528567314148\n",
      "epoch:10, batch: 408,  loss: 0.9077897667884827\n",
      "epoch:10, batch: 409,  loss: 0.7918211221694946\n",
      "epoch:10, batch: 410,  loss: 0.8490151762962341\n",
      "epoch:10, batch: 411,  loss: 0.832019031047821\n",
      "epoch:10, batch: 412,  loss: 1.1637264490127563\n",
      "epoch:10, batch: 413,  loss: 0.8733391761779785\n",
      "epoch:10, batch: 414,  loss: 0.8959994912147522\n",
      "epoch:10, batch: 415,  loss: 1.2130383253097534\n",
      "epoch:10, batch: 416,  loss: 0.8841379284858704\n",
      "epoch:10, batch: 417,  loss: 1.2194416522979736\n",
      "epoch:10, batch: 418,  loss: 1.0282422304153442\n",
      "epoch:10, batch: 419,  loss: 0.8170179128646851\n",
      "epoch:10, batch: 420,  loss: 1.046789288520813\n",
      "epoch:10, batch: 421,  loss: 0.8335795998573303\n",
      "epoch:10, batch: 422,  loss: 1.1499359607696533\n",
      "epoch:10, batch: 423,  loss: 0.7461422681808472\n",
      "epoch:10, batch: 424,  loss: 0.9664901494979858\n",
      "epoch:10, batch: 425,  loss: 1.196371078491211\n",
      "epoch:10, batch: 426,  loss: 0.9681209921836853\n",
      "epoch:10, batch: 427,  loss: 1.13402259349823\n",
      "epoch:10, batch: 428,  loss: 1.0064340829849243\n",
      "epoch:10, batch: 429,  loss: 1.1646156311035156\n",
      "epoch:10, batch: 430,  loss: 1.0666347742080688\n",
      "epoch:10, batch: 431,  loss: 1.0324084758758545\n",
      "epoch:10, batch: 432,  loss: 0.8392046093940735\n",
      "epoch:10, batch: 433,  loss: 0.8167133927345276\n",
      "epoch:10, batch: 434,  loss: 0.7879162430763245\n",
      "epoch:10, batch: 435,  loss: 0.9010080695152283\n",
      "epoch:10, batch: 436,  loss: 0.9176512956619263\n",
      "epoch:10, batch: 437,  loss: 1.149623155593872\n",
      "epoch:10, batch: 438,  loss: 1.11006498336792\n",
      "epoch:10, batch: 439,  loss: 1.325145959854126\n",
      "epoch:10, batch: 440,  loss: 1.3244032859802246\n",
      "epoch:10, batch: 441,  loss: 1.037044644355774\n",
      "epoch:10, batch: 442,  loss: 0.8514216542243958\n",
      "epoch:10, batch: 443,  loss: 0.8671826124191284\n",
      "epoch:10, batch: 444,  loss: 1.1474689245224\n",
      "epoch:10, batch: 445,  loss: 1.0132968425750732\n",
      "epoch:10, batch: 446,  loss: 0.8718156218528748\n",
      "epoch:10, batch: 447,  loss: 1.169555425643921\n",
      "epoch:10, batch: 448,  loss: 0.6870107650756836\n",
      "epoch:10, batch: 449,  loss: 1.1455078125\n",
      "epoch:10, batch: 450,  loss: 0.9473347663879395\n",
      "epoch:10, batch: 451,  loss: 1.1706316471099854\n",
      "epoch:10, batch: 452,  loss: 0.8768197298049927\n",
      "epoch:10, batch: 453,  loss: 0.8843863606452942\n",
      "epoch:10, batch: 454,  loss: 0.9415192008018494\n",
      "epoch:10, batch: 455,  loss: 0.8565918803215027\n",
      "epoch:10, batch: 456,  loss: 0.9581311941146851\n",
      "epoch:10, batch: 457,  loss: 0.8042904734611511\n",
      "epoch:10, batch: 458,  loss: 0.8822482228279114\n",
      "epoch:10, batch: 459,  loss: 0.9877830743789673\n",
      "epoch:10, batch: 460,  loss: 0.9245120286941528\n",
      "epoch:10, batch: 461,  loss: 0.9484684467315674\n",
      "epoch:10, batch: 462,  loss: 0.8697813153266907\n",
      "epoch:10, batch: 463,  loss: 0.8462123274803162\n",
      "epoch:10, batch: 464,  loss: 0.952471137046814\n",
      "epoch:10, batch: 465,  loss: 0.9919531941413879\n",
      "epoch:10, batch: 466,  loss: 0.738426923751831\n",
      "epoch:10, batch: 467,  loss: 1.068906307220459\n",
      "epoch:10, batch: 468,  loss: 0.9321765303611755\n",
      "epoch:10, batch: 469,  loss: 1.5359270572662354\n",
      "epoch:10, batch: 470,  loss: 1.1077065467834473\n",
      "epoch:10, batch: 471,  loss: 0.6515966653823853\n",
      "epoch:10, batch: 472,  loss: 0.9729530811309814\n",
      "epoch:10, batch: 473,  loss: 1.4340641498565674\n",
      "epoch:10, batch: 474,  loss: 0.8797804713249207\n",
      "epoch:10, batch: 475,  loss: 0.7953975200653076\n",
      "epoch:10, batch: 476,  loss: 0.8454612493515015\n",
      "epoch:10, batch: 477,  loss: 1.1256157159805298\n",
      "epoch:10, batch: 478,  loss: 0.9920669198036194\n",
      "epoch:10, batch: 479,  loss: 0.9276655912399292\n",
      "epoch:10, batch: 480,  loss: 1.3239147663116455\n",
      "epoch:10, batch: 481,  loss: 1.0864629745483398\n",
      "epoch:10, batch: 482,  loss: 1.290946125984192\n",
      "epoch:10, batch: 483,  loss: 0.8653632998466492\n",
      "epoch:10, batch: 484,  loss: 0.9560182690620422\n",
      "epoch:10, batch: 485,  loss: 0.8609856367111206\n",
      "epoch:10, batch: 486,  loss: 0.6759917140007019\n",
      "epoch:10, batch: 487,  loss: 0.8852291107177734\n",
      "epoch:10, batch: 488,  loss: 1.0080195665359497\n",
      "epoch:10, batch: 489,  loss: 1.1338708400726318\n",
      "epoch:10, batch: 490,  loss: 1.0688081979751587\n",
      "epoch:10, batch: 491,  loss: 1.2636247873306274\n",
      "epoch:10, batch: 492,  loss: 1.1890703439712524\n",
      "epoch:10, batch: 493,  loss: 1.2889773845672607\n",
      "epoch:10, batch: 494,  loss: 1.1740429401397705\n",
      "epoch:10, batch: 495,  loss: 1.0516926050186157\n",
      "epoch:10, batch: 496,  loss: 0.8250589966773987\n",
      "epoch:10, batch: 497,  loss: 0.7195144891738892\n",
      "epoch:10, batch: 498,  loss: 1.0786608457565308\n",
      "epoch:10, batch: 499,  loss: 0.9280209541320801\n",
      "epoch:10, batch: 500,  loss: 0.8045542240142822\n",
      "epoch:10, batch: 501,  loss: 0.9452164173126221\n",
      "epoch:10, batch: 502,  loss: 1.1474418640136719\n",
      "epoch:10, batch: 503,  loss: 1.4014718532562256\n",
      "epoch:10, batch: 504,  loss: 1.1500813961029053\n",
      "epoch:10, batch: 505,  loss: 1.0183459520339966\n",
      "epoch:10, batch: 506,  loss: 0.9034833908081055\n",
      "epoch:10, batch: 507,  loss: 1.0134923458099365\n",
      "epoch:10, batch: 508,  loss: 1.1389864683151245\n",
      "epoch:10, batch: 509,  loss: 0.9675591588020325\n",
      "epoch:10, batch: 510,  loss: 0.9841842651367188\n",
      "epoch:10, batch: 511,  loss: 0.9129522442817688\n",
      "epoch:10, batch: 512,  loss: 1.300099492073059\n",
      "epoch:10, batch: 513,  loss: 1.1743371486663818\n",
      "epoch:10, batch: 514,  loss: 0.8881567716598511\n",
      "epoch:10, batch: 515,  loss: 1.1269646883010864\n",
      "epoch:10, batch: 516,  loss: 1.0408427715301514\n",
      "epoch:10, batch: 517,  loss: 1.2634940147399902\n",
      "epoch:10, batch: 518,  loss: 1.2562763690948486\n",
      "epoch:10, batch: 519,  loss: 1.0707038640975952\n",
      "epoch:10, batch: 520,  loss: 1.1742440462112427\n",
      "epoch:10, batch: 521,  loss: 0.9630778431892395\n",
      "epoch:10, batch: 522,  loss: 1.2375677824020386\n",
      "epoch:10, batch: 523,  loss: 1.125113606452942\n",
      "epoch:10, batch: 524,  loss: 1.0395809412002563\n",
      "epoch:10, batch: 525,  loss: 0.9688288569450378\n",
      "epoch:10, batch: 526,  loss: 0.9828371405601501\n",
      "epoch:10, batch: 527,  loss: 1.280113935470581\n",
      "epoch:10, batch: 528,  loss: 1.177219271659851\n",
      "epoch:10, batch: 529,  loss: 0.8092000484466553\n",
      "epoch:10, batch: 530,  loss: 1.2036857604980469\n",
      "epoch:10, batch: 531,  loss: 0.9257388710975647\n",
      "epoch:10, batch: 532,  loss: 1.0669023990631104\n",
      "epoch:10, batch: 533,  loss: 1.1581239700317383\n",
      "epoch:10, batch: 534,  loss: 1.071303367614746\n",
      "epoch:10, batch: 535,  loss: 1.0050159692764282\n",
      "epoch:10, batch: 536,  loss: 0.8055590391159058\n",
      "epoch:10, batch: 537,  loss: 1.2251965999603271\n",
      "epoch:10, batch: 538,  loss: 0.8563845753669739\n",
      "epoch:10, batch: 539,  loss: 0.8209782242774963\n",
      "epoch:10, batch: 540,  loss: 1.025456428527832\n",
      "epoch:10, batch: 541,  loss: 1.0809913873672485\n",
      "epoch:10, batch: 542,  loss: 0.9288389086723328\n",
      "epoch:10, batch: 543,  loss: 0.8921501636505127\n",
      "epoch:10, batch: 544,  loss: 0.7282471656799316\n",
      "epoch:10, batch: 545,  loss: 0.6930794715881348\n",
      "epoch:10, batch: 546,  loss: 0.9617987871170044\n",
      "epoch:10, batch: 547,  loss: 1.1629842519760132\n",
      "epoch:10, batch: 548,  loss: 0.8982514142990112\n",
      "epoch:10, batch: 549,  loss: 1.1536998748779297\n",
      "epoch:10, batch: 550,  loss: 0.9240084290504456\n",
      "epoch:10, batch: 551,  loss: 0.7779382467269897\n",
      "epoch:10, batch: 552,  loss: 0.9744861125946045\n",
      "epoch:10, batch: 553,  loss: 1.0011829137802124\n",
      "epoch:10, batch: 554,  loss: 1.5938175916671753\n",
      "epoch:10, batch: 555,  loss: 1.034218192100525\n",
      "epoch:10, batch: 556,  loss: 0.9636894464492798\n",
      "epoch:10, batch: 557,  loss: 0.8996378779411316\n",
      "epoch:10, batch: 558,  loss: 0.9864569902420044\n",
      "epoch:10, batch: 559,  loss: 0.9909664988517761\n",
      "epoch:10, batch: 560,  loss: 1.001452088356018\n",
      "epoch:10, batch: 561,  loss: 1.0086448192596436\n",
      "epoch:10, batch: 562,  loss: 1.025565266609192\n",
      "epoch:10, batch: 563,  loss: 1.0265170335769653\n",
      "epoch:10, batch: 564,  loss: 0.9787681698799133\n",
      "epoch:10, batch: 565,  loss: 0.9248772263526917\n",
      "epoch:10, batch: 566,  loss: 1.2802073955535889\n",
      "epoch:10, batch: 567,  loss: 0.9882216453552246\n",
      "epoch:10, batch: 568,  loss: 1.1093443632125854\n",
      "epoch:10, batch: 569,  loss: 1.2201625108718872\n",
      "epoch:10, batch: 570,  loss: 0.9436147212982178\n",
      "epoch:10, batch: 571,  loss: 0.8039084672927856\n",
      "epoch:10, batch: 572,  loss: 0.9227584600448608\n",
      "epoch:10, batch: 573,  loss: 1.1803416013717651\n",
      "epoch:10, batch: 574,  loss: 1.028692603111267\n",
      "epoch:10, batch: 575,  loss: 0.8358932137489319\n",
      "epoch:10, batch: 576,  loss: 1.0705674886703491\n",
      "epoch:10, batch: 577,  loss: 0.9068002700805664\n",
      "epoch:10, batch: 578,  loss: 0.9441028237342834\n",
      "epoch:10, batch: 579,  loss: 0.9829367995262146\n",
      "epoch:10, batch: 580,  loss: 1.171065092086792\n",
      "epoch:10, batch: 581,  loss: 0.9634038805961609\n",
      "epoch:10, batch: 582,  loss: 0.7890511155128479\n",
      "epoch:10, batch: 583,  loss: 1.363879680633545\n",
      "epoch:10, batch: 584,  loss: 1.0587972402572632\n",
      "epoch:10, batch: 585,  loss: 1.0209314823150635\n",
      "epoch:10, batch: 586,  loss: 0.9401338696479797\n",
      "epoch:10, batch: 587,  loss: 0.9949613809585571\n",
      "epoch:10, batch: 588,  loss: 1.0056949853897095\n",
      "epoch:10, batch: 589,  loss: 1.0643178224563599\n",
      "epoch:10, batch: 590,  loss: 0.8573348522186279\n",
      "epoch:10, batch: 591,  loss: 0.9204466342926025\n",
      "epoch:10, batch: 592,  loss: 0.9681603908538818\n",
      "epoch:10, batch: 593,  loss: 1.0135383605957031\n",
      "epoch:10, batch: 594,  loss: 0.8605179786682129\n",
      "epoch:10, batch: 595,  loss: 1.2433550357818604\n",
      "epoch:10, batch: 596,  loss: 1.124413013458252\n",
      "epoch:10, batch: 597,  loss: 1.0647004842758179\n",
      "epoch:10, batch: 598,  loss: 1.012020468711853\n",
      "epoch:10, batch: 599,  loss: 0.9923691153526306\n",
      "epoch:10, batch: 600,  loss: 1.1539373397827148\n",
      "epoch:10, batch: 601,  loss: 1.0721075534820557\n",
      "epoch:10, batch: 602,  loss: 0.8834320902824402\n",
      "epoch:10, batch: 603,  loss: 0.8918025493621826\n",
      "epoch:10, batch: 604,  loss: 1.0631150007247925\n",
      "epoch:10, batch: 605,  loss: 1.1141631603240967\n",
      "epoch:10, batch: 606,  loss: 1.3346831798553467\n",
      "epoch:10, batch: 607,  loss: 1.214635968208313\n",
      "epoch:10, batch: 608,  loss: 1.0501502752304077\n",
      "epoch:10, batch: 609,  loss: 1.0417588949203491\n",
      "epoch:10, batch: 610,  loss: 0.9529927372932434\n",
      "epoch:10, batch: 611,  loss: 1.113195538520813\n",
      "epoch:10, batch: 612,  loss: 1.2547904253005981\n",
      "epoch:10, batch: 613,  loss: 1.0912119150161743\n",
      "epoch:10, batch: 614,  loss: 1.205614686012268\n",
      "epoch:10, batch: 615,  loss: 0.8810656666755676\n",
      "epoch:10, batch: 616,  loss: 0.9684002995491028\n",
      "epoch:10, batch: 617,  loss: 1.026167869567871\n",
      "epoch:10, batch: 618,  loss: 0.8340467810630798\n",
      "epoch:10, batch: 619,  loss: 0.9964584112167358\n",
      "epoch:10, batch: 620,  loss: 1.0339837074279785\n",
      "epoch:10, batch: 621,  loss: 1.0560030937194824\n",
      "epoch:10, batch: 622,  loss: 0.9884011745452881\n",
      "epoch:10, batch: 623,  loss: 0.8363771438598633\n",
      "epoch:10, batch: 624,  loss: 0.8846026062965393\n",
      "epoch:10, batch: 625,  loss: 1.2886275053024292\n",
      "epoch:10, batch: 626,  loss: 0.9226788282394409\n",
      "epoch:10, batch: 627,  loss: 0.8283204436302185\n",
      "epoch:10, batch: 628,  loss: 0.9952254891395569\n",
      "epoch:10, batch: 629,  loss: 1.0653388500213623\n",
      "epoch:10, batch: 630,  loss: 0.8633536696434021\n",
      "epoch:10, batch: 631,  loss: 1.1376628875732422\n",
      "epoch:10, batch: 632,  loss: 1.1160666942596436\n",
      "epoch:10, batch: 633,  loss: 1.0856187343597412\n",
      "epoch:10, batch: 634,  loss: 0.9442372918128967\n",
      "epoch:10, batch: 635,  loss: 0.8868053555488586\n",
      "epoch:10, batch: 636,  loss: 0.7992954850196838\n",
      "epoch:10, batch: 637,  loss: 1.2392690181732178\n",
      "epoch:10, batch: 638,  loss: 1.0498849153518677\n",
      "epoch:10, batch: 639,  loss: 0.9080087542533875\n",
      "epoch:10, batch: 640,  loss: 0.8509517312049866\n",
      "epoch:10, batch: 641,  loss: 0.9971984028816223\n",
      "epoch:10, batch: 642,  loss: 0.8992680907249451\n",
      "epoch:10, batch: 643,  loss: 1.2710663080215454\n",
      "epoch:10, batch: 644,  loss: 0.8069939613342285\n",
      "epoch:10, batch: 645,  loss: 1.147761344909668\n",
      "epoch:10, batch: 646,  loss: 1.0237547159194946\n",
      "epoch:10, batch: 647,  loss: 0.8832628726959229\n",
      "epoch:10, batch: 648,  loss: 0.8944851160049438\n",
      "epoch:10, batch: 649,  loss: 0.7998897433280945\n",
      "epoch:10, batch: 650,  loss: 1.2240538597106934\n",
      "epoch:10, batch: 651,  loss: 0.7932884693145752\n",
      "epoch:10, batch: 652,  loss: 1.0087549686431885\n",
      "epoch:10, batch: 653,  loss: 1.1727548837661743\n",
      "epoch:10, batch: 654,  loss: 1.0518680810928345\n",
      "epoch:10, batch: 655,  loss: 1.0203568935394287\n",
      "epoch:10, batch: 656,  loss: 0.8193548917770386\n",
      "epoch:10, batch: 657,  loss: 1.1971749067306519\n",
      "epoch:10, batch: 658,  loss: 1.4887969493865967\n",
      "epoch:10, batch: 659,  loss: 1.0634093284606934\n",
      "epoch:10, batch: 660,  loss: 0.8098381757736206\n",
      "epoch:10, batch: 661,  loss: 1.437429666519165\n",
      "epoch:10, batch: 662,  loss: 1.0220035314559937\n",
      "epoch:10, batch: 663,  loss: 0.9305226802825928\n",
      "epoch:10, batch: 664,  loss: 1.1668375730514526\n",
      "epoch:10, batch: 665,  loss: 1.0486934185028076\n",
      "epoch:10, batch: 666,  loss: 0.9383970499038696\n",
      "epoch:10, batch: 667,  loss: 0.8664723038673401\n",
      "epoch:10, batch: 668,  loss: 0.7612695693969727\n",
      "epoch:10, batch: 669,  loss: 0.8552874326705933\n",
      "epoch:10, batch: 670,  loss: 1.2466621398925781\n",
      "epoch:10, batch: 671,  loss: 1.2561184167861938\n",
      "epoch:10, batch: 672,  loss: 0.9657511115074158\n",
      "epoch:10, batch: 673,  loss: 1.0903759002685547\n",
      "epoch:10, batch: 674,  loss: 1.4160254001617432\n",
      "epoch:10, batch: 675,  loss: 0.9028372764587402\n",
      "epoch:10, batch: 676,  loss: 0.9284996390342712\n",
      "epoch:10, batch: 677,  loss: 0.8958054184913635\n",
      "epoch:10, batch: 678,  loss: 0.7021729350090027\n",
      "epoch:10, batch: 679,  loss: 0.7976258993148804\n",
      "epoch:10, batch: 680,  loss: 0.7267491221427917\n",
      "epoch:10, batch: 681,  loss: 0.8334495425224304\n",
      "epoch:10, batch: 682,  loss: 1.1674132347106934\n",
      "epoch:10, batch: 683,  loss: 1.2831600904464722\n",
      "epoch:10, batch: 684,  loss: 1.0218247175216675\n",
      "epoch:10, batch: 685,  loss: 1.1102873086929321\n",
      "epoch:10, batch: 686,  loss: 0.9100322127342224\n",
      "epoch:10, batch: 687,  loss: 0.8592858910560608\n",
      "epoch:10, batch: 688,  loss: 1.1761912107467651\n",
      "epoch:10, batch: 689,  loss: 1.2254927158355713\n",
      "epoch:10, batch: 690,  loss: 0.9468519687652588\n",
      "epoch:10, batch: 691,  loss: 1.292942762374878\n",
      "epoch:10, batch: 692,  loss: 0.7466211318969727\n",
      "epoch:10, batch: 693,  loss: 0.9680593013763428\n",
      "epoch:10, batch: 694,  loss: 0.732930600643158\n",
      "epoch:10, batch: 695,  loss: 1.2149254083633423\n",
      "epoch:10, batch: 696,  loss: 0.9306284785270691\n",
      "epoch:10, batch: 697,  loss: 1.066308617591858\n",
      "epoch:10, batch: 698,  loss: 0.8820549845695496\n",
      "epoch:10, batch: 699,  loss: 0.9482720494270325\n",
      "epoch:10, batch: 700,  loss: 1.0625139474868774\n",
      "epoch:10, batch: 701,  loss: 0.8874987363815308\n",
      "epoch:10, batch: 702,  loss: 1.0592626333236694\n",
      "epoch:10, batch: 703,  loss: 1.1411036252975464\n",
      "epoch:10, batch: 704,  loss: 0.8583446145057678\n",
      "epoch:10, batch: 705,  loss: 0.9717713594436646\n",
      "epoch:10, batch: 706,  loss: 0.882161021232605\n",
      "epoch:10, batch: 707,  loss: 0.9846307039260864\n",
      "epoch:10, batch: 708,  loss: 0.8367372155189514\n",
      "epoch:10, batch: 709,  loss: 0.7995954751968384\n",
      "epoch:10, batch: 710,  loss: 0.8873233199119568\n",
      "epoch:10, batch: 711,  loss: 0.8661447763442993\n",
      "epoch:10, batch: 712,  loss: 0.9271246790885925\n",
      "epoch:10, batch: 713,  loss: 0.818074643611908\n",
      "epoch:10, batch: 714,  loss: 0.8881794810295105\n",
      "epoch:10, batch: 715,  loss: 0.9336981773376465\n",
      "epoch:10, batch: 716,  loss: 1.1525994539260864\n",
      "epoch:10, batch: 717,  loss: 0.8588038086891174\n",
      "epoch:10, batch: 718,  loss: 0.6182888746261597\n",
      "epoch:10, batch: 719,  loss: 1.1092091798782349\n",
      "epoch:10, batch: 720,  loss: 0.911594808101654\n",
      "epoch:10, batch: 721,  loss: 1.23472261428833\n",
      "epoch:10, batch: 722,  loss: 0.8952361345291138\n",
      "epoch:10, batch: 723,  loss: 0.9766821265220642\n",
      "epoch:10, batch: 724,  loss: 0.9081265926361084\n",
      "epoch:10, batch: 725,  loss: 1.3247064352035522\n",
      "epoch:10, batch: 726,  loss: 1.2658308744430542\n",
      "epoch:10, batch: 727,  loss: 1.029264211654663\n",
      "epoch:10, batch: 728,  loss: 1.0467345714569092\n",
      "epoch:10, batch: 729,  loss: 0.9728094935417175\n",
      "epoch:10, batch: 730,  loss: 1.0116604566574097\n",
      "epoch:10, batch: 731,  loss: 1.2806622982025146\n",
      "epoch:10, batch: 732,  loss: 1.2639033794403076\n",
      "epoch:10, batch: 733,  loss: 1.0054458379745483\n",
      "epoch:10, batch: 734,  loss: 1.1273521184921265\n",
      "epoch:10, batch: 735,  loss: 0.8070595264434814\n",
      "epoch:10, batch: 736,  loss: 0.883257269859314\n",
      "epoch:10, batch: 737,  loss: 1.2881702184677124\n",
      "epoch:10, batch: 738,  loss: 0.7820671200752258\n",
      "epoch:10, batch: 739,  loss: 0.7541835308074951\n",
      "epoch:10, batch: 740,  loss: 1.1066124439239502\n",
      "epoch:10, batch: 741,  loss: 0.9192314743995667\n",
      "epoch:10, batch: 742,  loss: 0.9931765198707581\n",
      "epoch:10, batch: 743,  loss: 0.9471912384033203\n",
      "epoch:10, batch: 744,  loss: 0.9819396734237671\n",
      "epoch:10, batch: 745,  loss: 0.9564642310142517\n",
      "epoch:10, batch: 746,  loss: 0.818520724773407\n",
      "epoch:10, batch: 747,  loss: 1.1662712097167969\n",
      "epoch:10, batch: 748,  loss: 1.0698018074035645\n",
      "epoch:10, batch: 749,  loss: 0.7642462253570557\n",
      "epoch:10, batch: 750,  loss: 1.2199583053588867\n",
      "epoch:10, batch: 751,  loss: 0.942590594291687\n",
      "epoch:10, batch: 752,  loss: 0.8232532739639282\n",
      "epoch:10, batch: 753,  loss: 0.7255765795707703\n",
      "epoch:10, batch: 754,  loss: 1.0719660520553589\n",
      "epoch:10, batch: 755,  loss: 0.7319566011428833\n",
      "epoch:10, batch: 756,  loss: 0.8461868166923523\n",
      "epoch:10, batch: 757,  loss: 0.8688373565673828\n",
      "epoch:10, batch: 758,  loss: 1.0257813930511475\n",
      "epoch:10, batch: 759,  loss: 0.8912484049797058\n",
      "epoch:10, batch: 760,  loss: 1.0609898567199707\n",
      "epoch:10, batch: 761,  loss: 0.783803403377533\n",
      "epoch:10, batch: 762,  loss: 1.0096056461334229\n",
      "epoch:10, batch: 763,  loss: 0.8438969850540161\n",
      "epoch:10, batch: 764,  loss: 1.2783738374710083\n",
      "epoch:10, batch: 765,  loss: 1.198722004890442\n",
      "epoch:10, batch: 766,  loss: 1.2760519981384277\n",
      "epoch:10, batch: 767,  loss: 0.8501258492469788\n",
      "epoch:10, batch: 768,  loss: 0.8763661980628967\n",
      "epoch:10, batch: 769,  loss: 1.0597554445266724\n",
      "epoch:10, batch: 770,  loss: 1.1023101806640625\n",
      "epoch:10, batch: 771,  loss: 1.008307695388794\n",
      "epoch:10, batch: 772,  loss: 0.8374038934707642\n",
      "epoch:10, batch: 773,  loss: 0.8830952048301697\n",
      "epoch:10, batch: 774,  loss: 1.3830296993255615\n",
      "epoch:10, batch: 775,  loss: 0.9347297549247742\n",
      "epoch:10, batch: 776,  loss: 1.4104809761047363\n",
      "epoch:10, batch: 777,  loss: 0.8964456915855408\n",
      "epoch:10, batch: 778,  loss: 1.1366044282913208\n",
      "epoch:10, batch: 779,  loss: 1.0296188592910767\n",
      "epoch:10, batch: 780,  loss: 1.1411187648773193\n",
      "epoch:10, batch: 781,  loss: 0.8395712375640869\n",
      "epoch:10, batch: 782,  loss: 1.2210372686386108\n",
      "epoch:10, batch: 783,  loss: 1.2723824977874756\n",
      "epoch:10, batch: 784,  loss: 1.2079133987426758\n",
      "epoch:10, batch: 785,  loss: 1.0200382471084595\n",
      "epoch:10, batch: 786,  loss: 0.802365243434906\n",
      "epoch:10, batch: 787,  loss: 1.207146167755127\n",
      "epoch:10, batch: 788,  loss: 0.723786473274231\n",
      "epoch:10, batch: 789,  loss: 0.8138732314109802\n",
      "epoch:10, batch: 790,  loss: 1.3109842538833618\n",
      "epoch:10, batch: 791,  loss: 0.9024059772491455\n",
      "epoch:10, batch: 792,  loss: 1.0743658542633057\n",
      "epoch:10, batch: 793,  loss: 1.1993050575256348\n",
      "epoch:10, batch: 794,  loss: 1.0473637580871582\n",
      "epoch:10, batch: 795,  loss: 0.7987521886825562\n",
      "epoch:10, batch: 796,  loss: 1.011972188949585\n",
      "epoch:10, batch: 797,  loss: 1.14053475856781\n",
      "epoch:10, batch: 798,  loss: 0.9393758773803711\n",
      "epoch:10, batch: 799,  loss: 0.8387495875358582\n",
      "epoch:10, batch: 800,  loss: 1.1155540943145752\n",
      "epoch:10, batch: 801,  loss: 1.0427942276000977\n",
      "epoch:10, batch: 802,  loss: 0.9127934575080872\n",
      "epoch:10, batch: 803,  loss: 0.6246535181999207\n",
      "epoch:10, batch: 804,  loss: 1.0352572202682495\n",
      "epoch:10, batch: 805,  loss: 1.053252935409546\n",
      "epoch:10, batch: 806,  loss: 0.7310140132904053\n",
      "epoch:10, batch: 807,  loss: 1.3341379165649414\n",
      "epoch:10, batch: 808,  loss: 1.0848554372787476\n",
      "epoch:10, batch: 809,  loss: 1.1126841306686401\n",
      "epoch:10, batch: 810,  loss: 1.239119529724121\n",
      "epoch:10, batch: 811,  loss: 1.0311232805252075\n",
      "epoch:10, batch: 812,  loss: 0.8743150234222412\n",
      "epoch:10, batch: 813,  loss: 0.9232165217399597\n",
      "epoch:10, batch: 814,  loss: 1.172828197479248\n",
      "epoch:10, batch: 815,  loss: 1.4925823211669922\n",
      "epoch:10, batch: 816,  loss: 1.0632305145263672\n",
      "epoch:10, batch: 817,  loss: 0.8651047945022583\n",
      "epoch:10, batch: 818,  loss: 1.0534021854400635\n",
      "epoch:10, batch: 819,  loss: 1.1165575981140137\n",
      "epoch:10, batch: 820,  loss: 0.9575026035308838\n",
      "epoch:10, batch: 821,  loss: 0.9368391036987305\n",
      "epoch:10, batch: 822,  loss: 1.215824007987976\n",
      "epoch:10, batch: 823,  loss: 0.8907110691070557\n",
      "epoch:10, batch: 824,  loss: 0.9591857194900513\n",
      "epoch:10, batch: 825,  loss: 1.1166203022003174\n",
      "epoch:10, batch: 826,  loss: 0.8916535377502441\n",
      "epoch:10, batch: 827,  loss: 0.8341138958930969\n",
      "epoch:10, batch: 828,  loss: 0.9833870530128479\n",
      "epoch:10, batch: 829,  loss: 0.9742571115493774\n",
      "epoch:10, batch: 830,  loss: 0.875143826007843\n",
      "epoch:10, batch: 831,  loss: 0.7151831388473511\n",
      "epoch:10, batch: 832,  loss: 1.0197371244430542\n",
      "epoch:10, batch: 833,  loss: 0.9246191382408142\n",
      "epoch:10, batch: 834,  loss: 0.9125942587852478\n",
      "epoch:10, batch: 835,  loss: 0.880193293094635\n",
      "epoch:10, batch: 836,  loss: 1.0494781732559204\n",
      "epoch:10, batch: 837,  loss: 1.0245320796966553\n",
      "epoch:10, batch: 838,  loss: 0.9081609845161438\n",
      "epoch:10, batch: 839,  loss: 0.8119682669639587\n",
      "epoch:10, batch: 840,  loss: 0.9181477427482605\n",
      "epoch:10, batch: 841,  loss: 0.8750156760215759\n",
      "epoch:10, batch: 842,  loss: 1.1132090091705322\n",
      "epoch:10, batch: 843,  loss: 1.1663440465927124\n",
      "epoch:10, batch: 844,  loss: 0.9120572805404663\n",
      "epoch:10, batch: 845,  loss: 0.8883492946624756\n",
      "epoch:10, batch: 846,  loss: 0.947464644908905\n",
      "epoch:10, batch: 847,  loss: 0.9980117082595825\n",
      "epoch:10, batch: 848,  loss: 0.9788049459457397\n",
      "epoch:10, batch: 849,  loss: 0.8696607351303101\n",
      "epoch:10, batch: 850,  loss: 1.1061015129089355\n",
      "epoch:10, batch: 851,  loss: 1.014998435974121\n",
      "epoch:10, batch: 852,  loss: 0.8692723512649536\n",
      "epoch:10, batch: 853,  loss: 1.1260541677474976\n",
      "epoch:10, batch: 854,  loss: 1.1595948934555054\n",
      "epoch:10, batch: 855,  loss: 0.858830451965332\n",
      "epoch:10, batch: 856,  loss: 0.762140691280365\n",
      "epoch:10, batch: 857,  loss: 0.9848198294639587\n",
      "epoch:10, batch: 858,  loss: 1.0453497171401978\n",
      "epoch:10, batch: 859,  loss: 1.1285276412963867\n",
      "epoch:10, batch: 860,  loss: 0.783158540725708\n",
      "epoch:10, batch: 861,  loss: 1.1507866382598877\n",
      "epoch:10, batch: 862,  loss: 0.8436721563339233\n",
      "epoch:10, batch: 863,  loss: 1.1084136962890625\n",
      "epoch:10, batch: 864,  loss: 0.8836618661880493\n",
      "epoch:10, batch: 865,  loss: 1.113921880722046\n",
      "epoch:10, batch: 866,  loss: 0.8203389048576355\n",
      "epoch:10, batch: 867,  loss: 0.981645941734314\n",
      "epoch:10, batch: 868,  loss: 1.051426649093628\n",
      "epoch:10, batch: 869,  loss: 1.2313507795333862\n",
      "epoch:10, batch: 870,  loss: 0.8849987387657166\n",
      "epoch:10, batch: 871,  loss: 0.9342480897903442\n",
      "epoch:10, batch: 872,  loss: 1.1294113397598267\n",
      "epoch:10, batch: 873,  loss: 1.345808982849121\n",
      "epoch:10, batch: 874,  loss: 0.8747626543045044\n",
      "epoch:10, batch: 875,  loss: 1.1123429536819458\n",
      "epoch:10, batch: 876,  loss: 1.0617343187332153\n",
      "epoch:10, batch: 877,  loss: 1.0266520977020264\n",
      "epoch:10, batch: 878,  loss: 1.0607455968856812\n",
      "epoch:10, batch: 879,  loss: 1.2212611436843872\n",
      "epoch:10, batch: 880,  loss: 0.8810836672782898\n",
      "epoch:10, batch: 881,  loss: 1.2137131690979004\n",
      "epoch:10, batch: 882,  loss: 1.0215635299682617\n",
      "epoch:10, batch: 883,  loss: 1.2313107252120972\n",
      "epoch:10, batch: 884,  loss: 1.097378134727478\n",
      "epoch:10, batch: 885,  loss: 1.1437045335769653\n",
      "epoch:10, batch: 886,  loss: 1.0723090171813965\n",
      "epoch:10, batch: 887,  loss: 0.8786324262619019\n",
      "epoch:10, batch: 888,  loss: 0.7669051289558411\n",
      "epoch:10, batch: 889,  loss: 0.9941835999488831\n",
      "epoch:10, batch: 890,  loss: 0.8305174112319946\n",
      "epoch:10, batch: 891,  loss: 0.8048891425132751\n",
      "epoch:10, batch: 892,  loss: 0.9193787574768066\n",
      "epoch:10, batch: 893,  loss: 1.2440178394317627\n",
      "epoch:10, batch: 894,  loss: 0.7232522964477539\n",
      "epoch:10, batch: 895,  loss: 1.0314234495162964\n",
      "epoch:10, batch: 896,  loss: 0.9822158813476562\n",
      "epoch:10, batch: 897,  loss: 1.1615015268325806\n",
      "epoch:10, batch: 898,  loss: 1.2099051475524902\n",
      "epoch:10, batch: 899,  loss: 0.7335034012794495\n",
      "epoch:10, batch: 900,  loss: 1.1371732950210571\n",
      "epoch:10, batch: 901,  loss: 1.02662992477417\n",
      "epoch:10, batch: 902,  loss: 1.0703980922698975\n",
      "epoch:10, batch: 903,  loss: 0.8261123299598694\n",
      "epoch:10, batch: 904,  loss: 1.218302607536316\n",
      "epoch:10, batch: 905,  loss: 0.9449988007545471\n",
      "epoch:10, batch: 906,  loss: 0.8800144791603088\n",
      "epoch:10, batch: 907,  loss: 1.0976897478103638\n",
      "epoch:10, batch: 908,  loss: 0.925895631313324\n",
      "epoch:10, batch: 909,  loss: 0.9288516640663147\n",
      "epoch:10, batch: 910,  loss: 1.0099034309387207\n",
      "epoch:10, batch: 911,  loss: 0.9779090285301208\n",
      "epoch:10, batch: 912,  loss: 0.9654709100723267\n",
      "epoch:10, batch: 913,  loss: 0.9074326157569885\n",
      "epoch:10, batch: 914,  loss: 1.1230467557907104\n",
      "epoch:10, batch: 915,  loss: 0.9589568376541138\n",
      "epoch:10, batch: 916,  loss: 1.2828587293624878\n",
      "epoch:10, batch: 917,  loss: 0.6762083172798157\n",
      "epoch:10, batch: 918,  loss: 0.8745664358139038\n",
      "epoch:10, batch: 919,  loss: 0.9497381448745728\n",
      "epoch:10, batch: 920,  loss: 1.1735554933547974\n",
      "epoch:10, batch: 921,  loss: 0.8964234590530396\n",
      "epoch:10, batch: 922,  loss: 0.8654873371124268\n",
      "epoch:10, batch: 923,  loss: 0.7909247875213623\n",
      "epoch:10, batch: 924,  loss: 0.9977176189422607\n",
      "epoch:10, batch: 925,  loss: 1.0301287174224854\n",
      "epoch:10, batch: 926,  loss: 0.8904109597206116\n",
      "epoch:10, batch: 927,  loss: 0.7882415056228638\n",
      "epoch:10, batch: 928,  loss: 1.268877625465393\n",
      "epoch:10, batch: 929,  loss: 1.142301321029663\n",
      "epoch:10, batch: 930,  loss: 0.8310118317604065\n",
      "epoch:10, batch: 931,  loss: 0.7444673776626587\n",
      "epoch:10, batch: 932,  loss: 0.9609150290489197\n",
      "epoch:10, batch: 933,  loss: 1.2668873071670532\n",
      "epoch:10, batch: 934,  loss: 0.9095463156700134\n",
      "epoch:10, batch: 935,  loss: 1.0040485858917236\n",
      "epoch:10, batch: 936,  loss: 1.0473389625549316\n",
      "epoch:10, batch: 937,  loss: 1.091585636138916\n",
      "epoch:10, batch: 938,  loss: 1.516355037689209\n",
      "epoch:10, batch: 939,  loss: 0.9655539393424988\n",
      "epoch:10, batch: 940,  loss: 1.2099226713180542\n",
      "epoch:10, batch: 941,  loss: 0.8668794631958008\n",
      "epoch:10, batch: 942,  loss: 1.1907978057861328\n",
      "epoch:10, batch: 943,  loss: 1.2939841747283936\n",
      "epoch:10, batch: 944,  loss: 0.8844233155250549\n",
      "epoch:10, batch: 945,  loss: 1.065863013267517\n",
      "epoch:10, batch: 946,  loss: 1.1407243013381958\n",
      "epoch:10, batch: 947,  loss: 1.116316556930542\n",
      "epoch:10, batch: 948,  loss: 0.8080640435218811\n",
      "epoch:10, batch: 949,  loss: 1.323485255241394\n",
      "epoch:10, batch: 950,  loss: 0.702825129032135\n",
      "epoch:10, batch: 951,  loss: 1.1300042867660522\n",
      "epoch:10, batch: 952,  loss: 1.1576879024505615\n",
      "epoch:10, batch: 953,  loss: 0.84454745054245\n",
      "epoch:10, batch: 954,  loss: 1.0627267360687256\n",
      "epoch:10, batch: 955,  loss: 1.2195358276367188\n",
      "epoch:10, batch: 956,  loss: 0.8414429426193237\n",
      "epoch:10, batch: 957,  loss: 1.1136075258255005\n",
      "epoch:10, batch: 958,  loss: 0.9092512726783752\n",
      "epoch:10, batch: 959,  loss: 1.0830994844436646\n",
      "epoch:10, batch: 960,  loss: 1.1056002378463745\n",
      "epoch:10, batch: 961,  loss: 1.0065196752548218\n",
      "epoch:10, batch: 962,  loss: 0.9609068036079407\n",
      "epoch:10, batch: 963,  loss: 0.6635095477104187\n",
      "epoch:10, batch: 964,  loss: 0.9907962083816528\n",
      "epoch:10, batch: 965,  loss: 0.8364134430885315\n",
      "epoch:10, batch: 966,  loss: 1.150221586227417\n",
      "epoch:10, batch: 967,  loss: 0.6335573196411133\n",
      "epoch:10, batch: 968,  loss: 1.4235886335372925\n",
      "epoch:10, batch: 969,  loss: 1.0491726398468018\n",
      "epoch:10, batch: 970,  loss: 0.8025121688842773\n",
      "epoch:10, batch: 971,  loss: 1.0077393054962158\n",
      "epoch:10, batch: 972,  loss: 0.8952517509460449\n",
      "epoch:10, batch: 973,  loss: 1.2341657876968384\n",
      "epoch:10, batch: 974,  loss: 1.1543495655059814\n",
      "epoch:10, batch: 975,  loss: 0.8907285332679749\n",
      "epoch:10, batch: 976,  loss: 0.9777538776397705\n",
      "epoch:10, batch: 977,  loss: 1.147417426109314\n",
      "epoch:10, batch: 978,  loss: 0.9902927875518799\n",
      "epoch:10, batch: 979,  loss: 0.9252394437789917\n",
      "epoch:10, batch: 980,  loss: 0.8255749940872192\n",
      "epoch:10, batch: 981,  loss: 1.2019116878509521\n",
      "epoch:10, batch: 982,  loss: 1.0531189441680908\n",
      "epoch:10, batch: 983,  loss: 0.9136945009231567\n",
      "epoch:10, batch: 984,  loss: 0.8467775583267212\n",
      "epoch:10, batch: 985,  loss: 0.9720413684844971\n",
      "epoch:10, batch: 986,  loss: 1.2005784511566162\n",
      "epoch:10, batch: 987,  loss: 0.9841420650482178\n",
      "epoch:10, batch: 988,  loss: 0.8222342729568481\n",
      "epoch:10, batch: 989,  loss: 0.9597695469856262\n",
      "epoch:10, batch: 990,  loss: 1.1298441886901855\n",
      "epoch:10, batch: 991,  loss: 1.0194463729858398\n",
      "epoch:10, batch: 992,  loss: 1.3776153326034546\n",
      "epoch:10, batch: 993,  loss: 0.8591398596763611\n",
      "epoch:10, batch: 994,  loss: 0.7862474918365479\n",
      "epoch:10, batch: 995,  loss: 0.9571048021316528\n",
      "epoch:10, batch: 996,  loss: 1.1805763244628906\n",
      "epoch:10, batch: 997,  loss: 1.1988365650177002\n",
      "epoch:10, batch: 998,  loss: 1.1061596870422363\n",
      "epoch:10, batch: 999,  loss: 0.8271254301071167\n",
      "epoch:10, batch: 1000,  loss: 0.9906042814254761\n",
      "epoch:10, batch: 1001,  loss: 1.05719792842865\n",
      "epoch:10, batch: 1002,  loss: 1.0058027505874634\n",
      "epoch:10, batch: 1003,  loss: 1.2098253965377808\n",
      "epoch:10, batch: 1004,  loss: 0.8364064693450928\n",
      "epoch:10, batch: 1005,  loss: 1.0246326923370361\n",
      "epoch:10, batch: 1006,  loss: 0.8367536067962646\n",
      "epoch:10, batch: 1007,  loss: 1.0575768947601318\n",
      "epoch:10, batch: 1008,  loss: 0.9896001815795898\n",
      "epoch:10, batch: 1009,  loss: 1.2702608108520508\n",
      "epoch:10, batch: 1010,  loss: 1.2757741212844849\n",
      "epoch:10, batch: 1011,  loss: 0.9833256602287292\n",
      "epoch:10, batch: 1012,  loss: 0.7304248213768005\n",
      "epoch:10, batch: 1013,  loss: 1.0075838565826416\n",
      "epoch:10, batch: 1014,  loss: 0.769331693649292\n",
      "epoch:10, batch: 1015,  loss: 0.9504402875900269\n",
      "epoch:10, batch: 1016,  loss: 1.0529446601867676\n",
      "epoch:10, batch: 1017,  loss: 1.284522294998169\n",
      "epoch:10, batch: 1018,  loss: 1.0411694049835205\n",
      "epoch:10, batch: 1019,  loss: 1.0018173456192017\n",
      "epoch:10, batch: 1020,  loss: 1.0809094905853271\n",
      "epoch:10, batch: 1021,  loss: 0.7131201028823853\n",
      "epoch:10, batch: 1022,  loss: 1.0549507141113281\n",
      "epoch:10, batch: 1023,  loss: 0.9909374117851257\n",
      "epoch:10, batch: 1024,  loss: 1.2196804285049438\n",
      "epoch:10, batch: 1025,  loss: 0.9155634045600891\n",
      "epoch:10, batch: 1026,  loss: 0.9102556109428406\n",
      "epoch:10, batch: 1027,  loss: 0.9856468439102173\n",
      "epoch:10, batch: 1028,  loss: 0.8877003192901611\n",
      "epoch:10, batch: 1029,  loss: 0.853527307510376\n",
      "epoch:10, batch: 1030,  loss: 1.0672450065612793\n",
      "epoch:10, batch: 1031,  loss: 1.0738179683685303\n",
      "epoch:10, batch: 1032,  loss: 1.033270001411438\n",
      "epoch:10, batch: 1033,  loss: 1.0122430324554443\n",
      "epoch:10, batch: 1034,  loss: 1.2667250633239746\n",
      "epoch:10, batch: 1035,  loss: 0.8231114745140076\n",
      "epoch:10, batch: 1036,  loss: 0.9198158383369446\n",
      "epoch:10, batch: 1037,  loss: 0.853564441204071\n",
      "epoch:10, batch: 1038,  loss: 1.3844243288040161\n",
      "epoch:10, batch: 1039,  loss: 0.9041747450828552\n",
      "epoch:10, batch: 1040,  loss: 0.9570930004119873\n",
      "epoch:10, batch: 1041,  loss: 0.889726459980011\n",
      "epoch:10, batch: 1042,  loss: 1.1932326555252075\n",
      "epoch:10, batch: 1043,  loss: 1.1539936065673828\n",
      "epoch:10, batch: 1044,  loss: 1.1544103622436523\n",
      "epoch:10, batch: 1045,  loss: 1.073857307434082\n",
      "epoch:10, batch: 1046,  loss: 0.8553135991096497\n",
      "epoch:10, batch: 1047,  loss: 1.2490341663360596\n",
      "epoch:10, batch: 1048,  loss: 0.8219407200813293\n",
      "epoch:10, batch: 1049,  loss: 1.113999605178833\n",
      "epoch:10, batch: 1050,  loss: 0.8262251019477844\n",
      "epoch:10, batch: 1051,  loss: 0.7662792801856995\n",
      "epoch:10, batch: 1052,  loss: 1.167757272720337\n",
      "epoch:10, batch: 1053,  loss: 0.9244755506515503\n",
      "epoch:10, batch: 1054,  loss: 1.0546938180923462\n",
      "epoch:10, batch: 1055,  loss: 0.7942425608634949\n",
      "epoch:10, batch: 1056,  loss: 0.7789453268051147\n",
      "epoch:10, batch: 1057,  loss: 1.1183558702468872\n",
      "epoch:10, batch: 1058,  loss: 1.1649802923202515\n",
      "epoch:10, batch: 1059,  loss: 0.9785664677619934\n",
      "epoch:10, batch: 1060,  loss: 1.00160551071167\n",
      "epoch:10, batch: 1061,  loss: 1.2692456245422363\n",
      "epoch:10, batch: 1062,  loss: 1.0421578884124756\n",
      "epoch:10, batch: 1063,  loss: 0.9396741390228271\n",
      "epoch:10, batch: 1064,  loss: 0.9413272738456726\n",
      "epoch:10, batch: 1065,  loss: 1.2048155069351196\n",
      "epoch:10, batch: 1066,  loss: 1.184757947921753\n",
      "epoch:10, batch: 1067,  loss: 0.8866975903511047\n",
      "epoch:10, batch: 1068,  loss: 0.6844230890274048\n",
      "epoch:10, batch: 1069,  loss: 0.7781319618225098\n",
      "epoch:10, batch: 1070,  loss: 1.0186269283294678\n",
      "epoch:10, batch: 1071,  loss: 1.2715754508972168\n",
      "epoch:10, batch: 1072,  loss: 0.8848198056221008\n",
      "epoch:10, batch: 1073,  loss: 1.025810718536377\n",
      "epoch:10, batch: 1074,  loss: 0.9840586185455322\n",
      "epoch:10, batch: 1075,  loss: 1.2868291139602661\n",
      "epoch:10, batch: 1076,  loss: 0.9089285731315613\n",
      "epoch:10, batch: 1077,  loss: 0.9837533831596375\n",
      "epoch:10, batch: 1078,  loss: 0.8102742433547974\n",
      "epoch:10, batch: 1079,  loss: 1.2208917140960693\n",
      "epoch:10, batch: 1080,  loss: 1.1247493028640747\n",
      "epoch:10, batch: 1081,  loss: 1.3554011583328247\n",
      "epoch:10, batch: 1082,  loss: 1.2951688766479492\n",
      "epoch:10, batch: 1083,  loss: 1.1449275016784668\n",
      "epoch:10, batch: 1084,  loss: 1.1136313676834106\n",
      "epoch:10, batch: 1085,  loss: 0.939468502998352\n",
      "epoch:10, batch: 1086,  loss: 1.0015288591384888\n",
      "epoch:10, batch: 1087,  loss: 0.8035007119178772\n",
      "epoch:10, batch: 1088,  loss: 0.8420029878616333\n",
      "epoch:10, batch: 1089,  loss: 0.8577796816825867\n",
      "epoch:10, batch: 1090,  loss: 0.7143296599388123\n",
      "epoch:10, batch: 1091,  loss: 1.1375820636749268\n",
      "epoch:10, batch: 1092,  loss: 0.897717297077179\n",
      "epoch:10, batch: 1093,  loss: 1.4044415950775146\n",
      "epoch:10, batch: 1094,  loss: 0.7797538638114929\n",
      "epoch:10, batch: 1095,  loss: 0.9612885117530823\n",
      "epoch:10, batch: 1096,  loss: 1.0779763460159302\n",
      "epoch:10, batch: 1097,  loss: 0.8999184370040894\n",
      "epoch:10, batch: 1098,  loss: 1.0864523649215698\n",
      "epoch:10, batch: 1099,  loss: 0.8712507486343384\n",
      "epoch:10, batch: 1100,  loss: 0.8341272473335266\n",
      "epoch:10, batch: 1101,  loss: 0.9906631112098694\n",
      "epoch:10, batch: 1102,  loss: 1.2019153833389282\n",
      "epoch:10, batch: 1103,  loss: 1.005286693572998\n",
      "epoch:10, batch: 1104,  loss: 0.7713708281517029\n",
      "epoch:10, batch: 1105,  loss: 0.6975767016410828\n",
      "epoch:10, batch: 1106,  loss: 1.1038273572921753\n",
      "epoch:10, batch: 1107,  loss: 0.9117092490196228\n",
      "epoch:10, batch: 1108,  loss: 1.1060986518859863\n",
      "epoch:10, batch: 1109,  loss: 0.8495550155639648\n",
      "epoch:10, batch: 1110,  loss: 1.0449169874191284\n",
      "epoch:10, batch: 1111,  loss: 1.0896286964416504\n",
      "epoch:10, batch: 1112,  loss: 1.027641773223877\n",
      "epoch:10, batch: 1113,  loss: 1.1386405229568481\n",
      "epoch:10, batch: 1114,  loss: 1.0046738386154175\n",
      "epoch:10, batch: 1115,  loss: 0.735722005367279\n",
      "epoch:10, batch: 1116,  loss: 1.0095605850219727\n",
      "epoch:10, batch: 1117,  loss: 1.2352538108825684\n",
      "epoch:10, batch: 1118,  loss: 1.1004786491394043\n",
      "epoch:10, batch: 1119,  loss: 0.9162994623184204\n",
      "epoch:10, batch: 1120,  loss: 0.8346477746963501\n",
      "epoch:10, batch: 1121,  loss: 0.9438983201980591\n",
      "epoch:10, batch: 1122,  loss: 0.8601914644241333\n",
      "epoch:10, batch: 1123,  loss: 1.1036986112594604\n",
      "epoch:10, batch: 1124,  loss: 1.078270435333252\n",
      "epoch:10, batch: 1125,  loss: 0.9515328407287598\n",
      "epoch:10, batch: 1126,  loss: 1.157554030418396\n",
      "epoch:10, batch: 1127,  loss: 0.9049533009529114\n",
      "epoch:10, batch: 1128,  loss: 0.9349631071090698\n",
      "epoch:10, batch: 1129,  loss: 0.9002459049224854\n",
      "epoch:10, batch: 1130,  loss: 1.1596399545669556\n",
      "epoch:10, batch: 1131,  loss: 0.7475588917732239\n",
      "epoch:10, batch: 1132,  loss: 0.7773166298866272\n",
      "epoch:10, batch: 1133,  loss: 1.0235580205917358\n",
      "epoch:10, batch: 1134,  loss: 0.962357223033905\n",
      "epoch:10, batch: 1135,  loss: 0.9135386943817139\n",
      "epoch:10, batch: 1136,  loss: 0.8370947241783142\n",
      "epoch:10, batch: 1137,  loss: 1.3175607919692993\n",
      "epoch:10, batch: 1138,  loss: 1.2725796699523926\n",
      "epoch:10, batch: 1139,  loss: 0.8439706563949585\n",
      "epoch:10, batch: 1140,  loss: 0.8389898538589478\n",
      "epoch:10, batch: 1141,  loss: 0.7768025994300842\n",
      "epoch:10, batch: 1142,  loss: 0.7627518177032471\n",
      "epoch:10, batch: 1143,  loss: 0.9402813911437988\n",
      "epoch:10, batch: 1144,  loss: 0.8816338777542114\n",
      "epoch:10, batch: 1145,  loss: 0.8905637264251709\n",
      "epoch:10, batch: 1146,  loss: 1.078035831451416\n",
      "epoch:10, batch: 1147,  loss: 1.3841590881347656\n",
      "epoch:10, batch: 1148,  loss: 1.0401511192321777\n",
      "epoch:10, batch: 1149,  loss: 0.9061519503593445\n",
      "epoch:10, batch: 1150,  loss: 0.8741475939750671\n",
      "epoch:10, batch: 1151,  loss: 1.3252592086791992\n",
      "epoch:10, batch: 1152,  loss: 0.9920576810836792\n",
      "epoch:10, batch: 1153,  loss: 1.0700738430023193\n",
      "epoch:10, batch: 1154,  loss: 0.9176087975502014\n",
      "epoch:10, batch: 1155,  loss: 0.8316923379898071\n",
      "epoch:10, batch: 1156,  loss: 1.0167738199234009\n",
      "epoch:10, batch: 1157,  loss: 0.9548518061637878\n",
      "epoch:10, batch: 1158,  loss: 0.7410326600074768\n",
      "epoch:10, batch: 1159,  loss: 1.1657211780548096\n",
      "epoch:10, batch: 1160,  loss: 0.964121401309967\n",
      "epoch:10, batch: 1161,  loss: 0.9634312987327576\n",
      "epoch:10, batch: 1162,  loss: 1.0167447328567505\n",
      "epoch:10, batch: 1163,  loss: 0.8825765252113342\n",
      "epoch:10, batch: 1164,  loss: 1.2174955606460571\n",
      "epoch:10, batch: 1165,  loss: 0.9360201954841614\n",
      "epoch:10, batch: 1166,  loss: 1.3423166275024414\n",
      "epoch:10, batch: 1167,  loss: 1.2230346202850342\n",
      "epoch:10, batch: 1168,  loss: 0.9268287420272827\n",
      "epoch:10, batch: 1169,  loss: 0.8309113383293152\n",
      "epoch:10, batch: 1170,  loss: 0.9190625548362732\n",
      "epoch:10, batch: 1171,  loss: 1.4360523223876953\n",
      "epoch:10, batch: 1172,  loss: 0.8778347969055176\n",
      "epoch:10, batch: 1173,  loss: 0.7664687037467957\n",
      "epoch:10, batch: 1174,  loss: 0.8738318085670471\n",
      "epoch:10, batch: 1175,  loss: 1.084181547164917\n",
      "epoch:10, batch: 1176,  loss: 1.1780517101287842\n",
      "epoch:10, batch: 1177,  loss: 0.9018000364303589\n",
      "epoch:10, batch: 1178,  loss: 0.9736230969429016\n",
      "epoch:10, batch: 1179,  loss: 1.1874195337295532\n",
      "epoch:10, batch: 1180,  loss: 0.953673243522644\n",
      "epoch:10, batch: 1181,  loss: 1.1507493257522583\n",
      "epoch:10, batch: 1182,  loss: 1.3531726598739624\n",
      "epoch:10, batch: 1183,  loss: 1.0108414888381958\n",
      "epoch:10, batch: 1184,  loss: 0.8219142556190491\n",
      "epoch:10, batch: 1185,  loss: 0.9273343682289124\n",
      "epoch:10, batch: 1186,  loss: 1.1350538730621338\n",
      "epoch:10, batch: 1187,  loss: 1.040641188621521\n",
      "epoch:10, batch: 1188,  loss: 0.8991220593452454\n",
      "epoch:10, batch: 1189,  loss: 0.9144935607910156\n",
      "epoch:10, batch: 1190,  loss: 0.7627647519111633\n",
      "epoch:10, batch: 1191,  loss: 1.129689335823059\n",
      "epoch:10, batch: 1192,  loss: 0.7221201658248901\n",
      "epoch:10, batch: 1193,  loss: 1.40266752243042\n",
      "epoch:10, batch: 1194,  loss: 1.0096936225891113\n",
      "epoch:10, batch: 1195,  loss: 1.001428484916687\n",
      "epoch:10, batch: 1196,  loss: 0.9686040878295898\n",
      "epoch:10, batch: 1197,  loss: 0.8961541652679443\n",
      "epoch:10, batch: 1198,  loss: 1.1468168497085571\n",
      "epoch:10, batch: 1199,  loss: 0.9948131442070007\n",
      "epoch:10, batch: 1200,  loss: 1.2161785364151\n",
      "epoch:10, batch: 1201,  loss: 0.8658623695373535\n",
      "epoch:10, batch: 1202,  loss: 0.967478334903717\n",
      "epoch:10, batch: 1203,  loss: 1.0260971784591675\n",
      "epoch:10, batch: 1204,  loss: 0.9633729457855225\n",
      "epoch:10, batch: 1205,  loss: 0.823227047920227\n",
      "epoch:10, batch: 1206,  loss: 0.9110515713691711\n",
      "epoch:10, batch: 1207,  loss: 1.0490403175354004\n",
      "epoch:10, batch: 1208,  loss: 0.8329601883888245\n",
      "epoch:10, batch: 1209,  loss: 1.027326226234436\n",
      "epoch:10, batch: 1210,  loss: 1.046142816543579\n",
      "epoch:10, batch: 1211,  loss: 1.1683458089828491\n",
      "epoch:10, batch: 1212,  loss: 1.1057473421096802\n",
      "epoch:10, batch: 1213,  loss: 1.01897394657135\n",
      "epoch:10, batch: 1214,  loss: 1.3226559162139893\n",
      "epoch:10, batch: 1215,  loss: 0.9543229937553406\n",
      "epoch:10, batch: 1216,  loss: 0.9320666193962097\n",
      "epoch:10, batch: 1217,  loss: 0.8750231266021729\n",
      "epoch:10, batch: 1218,  loss: 0.9764980673789978\n",
      "epoch:10, batch: 1219,  loss: 0.8326524496078491\n",
      "epoch:10, batch: 1220,  loss: 1.179497480392456\n",
      "epoch:10, batch: 1221,  loss: 0.9619411826133728\n",
      "epoch:10, batch: 1222,  loss: 1.126335620880127\n",
      "epoch:10, batch: 1223,  loss: 0.778654158115387\n",
      "epoch:10, batch: 1224,  loss: 1.1262998580932617\n",
      "epoch:10, batch: 1225,  loss: 0.8625722527503967\n",
      "epoch:10, batch: 1226,  loss: 1.0219682455062866\n",
      "epoch:10, batch: 1227,  loss: 1.1222628355026245\n",
      "epoch:10, batch: 1228,  loss: 0.90386962890625\n",
      "epoch:10, batch: 1229,  loss: 0.9687215089797974\n",
      "epoch:10, batch: 1230,  loss: 0.8601934909820557\n",
      "epoch:10, batch: 1231,  loss: 0.9534243941307068\n",
      "epoch:10, batch: 1232,  loss: 0.719269335269928\n",
      "epoch:10, batch: 1233,  loss: 0.99237459897995\n",
      "epoch:10, batch: 1234,  loss: 1.0902878046035767\n",
      "epoch:10, batch: 1235,  loss: 1.0253554582595825\n",
      "epoch:10, batch: 1236,  loss: 1.0225251913070679\n",
      "epoch:10, batch: 1237,  loss: 1.032867431640625\n",
      "epoch:10, batch: 1238,  loss: 0.9523209929466248\n",
      "epoch:10, batch: 1239,  loss: 1.1940293312072754\n",
      "epoch:10, batch: 1240,  loss: 0.902180016040802\n",
      "epoch:10, batch: 1241,  loss: 0.9090723991394043\n",
      "epoch:10, batch: 1242,  loss: 1.3424739837646484\n",
      "epoch:10, batch: 1243,  loss: 0.7699868083000183\n",
      "epoch:10, batch: 1244,  loss: 0.9753052592277527\n",
      "epoch:10, batch: 1245,  loss: 1.034092903137207\n",
      "epoch:10, batch: 1246,  loss: 1.2021061182022095\n",
      "epoch:10, batch: 1247,  loss: 0.8549706339836121\n",
      "epoch:10, batch: 1248,  loss: 0.948585569858551\n",
      "epoch:10, batch: 1249,  loss: 0.9481470584869385\n",
      "epoch:10, batch: 1250,  loss: 1.1120963096618652\n",
      "epoch:10, batch: 1251,  loss: 0.9525696635246277\n",
      "epoch:10, batch: 1252,  loss: 1.137041449546814\n",
      "epoch:10, batch: 1253,  loss: 0.8968379497528076\n",
      "epoch:10, batch: 1254,  loss: 1.0562677383422852\n",
      "epoch:10, batch: 1255,  loss: 0.91255784034729\n",
      "epoch:10, batch: 1256,  loss: 0.8744574785232544\n",
      "epoch:10, batch: 1257,  loss: 1.0893614292144775\n",
      "epoch:10, batch: 1258,  loss: 0.6791201233863831\n",
      "epoch:10, batch: 1259,  loss: 0.6272326707839966\n",
      "epoch:10, batch: 1260,  loss: 0.9595425128936768\n",
      "epoch:10, batch: 1261,  loss: 0.7968366146087646\n",
      "epoch:10, batch: 1262,  loss: 0.7519621253013611\n",
      "epoch:10, batch: 1263,  loss: 0.9061198830604553\n",
      "epoch:10, batch: 1264,  loss: 1.0471906661987305\n",
      "epoch:10, batch: 1265,  loss: 0.9647260308265686\n",
      "epoch:10, batch: 1266,  loss: 1.0460792779922485\n",
      "epoch:10, batch: 1267,  loss: 0.8204230666160583\n",
      "epoch:10, batch: 1268,  loss: 1.10977303981781\n",
      "epoch:10, batch: 1269,  loss: 0.7952874302864075\n",
      "epoch:10, batch: 1270,  loss: 0.9303645491600037\n",
      "epoch:10, batch: 1271,  loss: 0.9284217357635498\n",
      "epoch:10, batch: 1272,  loss: 0.6724724173545837\n",
      "epoch:10, batch: 1273,  loss: 0.863005518913269\n",
      "epoch:10, batch: 1274,  loss: 1.066135287284851\n",
      "epoch:10, batch: 1275,  loss: 0.7708776593208313\n",
      "epoch:10, batch: 1276,  loss: 0.8745582103729248\n",
      "epoch:10, batch: 1277,  loss: 0.7878607511520386\n",
      "epoch:10, batch: 1278,  loss: 1.0206040143966675\n",
      "epoch:10, batch: 1279,  loss: 1.1348081827163696\n",
      "epoch:10, batch: 1280,  loss: 1.0721358060836792\n",
      "epoch:10, batch: 1281,  loss: 0.9033567309379578\n",
      "epoch:10, batch: 1282,  loss: 0.8879031538963318\n",
      "epoch:10, batch: 1283,  loss: 1.0669887065887451\n",
      "epoch:10, batch: 1284,  loss: 0.8537724614143372\n",
      "epoch:10, batch: 1285,  loss: 1.1174744367599487\n",
      "epoch:10, batch: 1286,  loss: 1.188226342201233\n",
      "epoch:10, batch: 1287,  loss: 0.8770290017127991\n",
      "epoch:10, batch: 1288,  loss: 1.0271862745285034\n",
      "epoch:10, batch: 1289,  loss: 0.8936352729797363\n",
      "epoch:10, batch: 1290,  loss: 1.0361855030059814\n",
      "epoch:10, batch: 1291,  loss: 0.794802725315094\n",
      "epoch:10, batch: 1292,  loss: 1.297565221786499\n",
      "epoch:10, batch: 1293,  loss: 0.9355437159538269\n",
      "epoch:10, batch: 1294,  loss: 1.00282883644104\n",
      "epoch:10, batch: 1295,  loss: 1.162156581878662\n",
      "epoch:10, batch: 1296,  loss: 0.755715548992157\n",
      "epoch:10, batch: 1297,  loss: 0.757095992565155\n",
      "epoch:10, batch: 1298,  loss: 0.8480542302131653\n",
      "epoch:10, batch: 1299,  loss: 1.1949158906936646\n",
      "epoch:10, batch: 1300,  loss: 0.9257687926292419\n",
      "epoch:10, batch: 1301,  loss: 0.986181378364563\n",
      "epoch:10, batch: 1302,  loss: 0.9480071663856506\n",
      "epoch:10, batch: 1303,  loss: 1.1444631814956665\n",
      "epoch:10, batch: 1304,  loss: 0.9448949694633484\n",
      "epoch:10, batch: 1305,  loss: 0.6925694942474365\n",
      "epoch:10, batch: 1306,  loss: 1.077582597732544\n",
      "epoch:10, batch: 1307,  loss: 0.9924580454826355\n",
      "epoch:10, batch: 1308,  loss: 1.1686490774154663\n",
      "epoch:10, batch: 1309,  loss: 1.1599839925765991\n",
      "epoch:10, batch: 1310,  loss: 1.052539587020874\n",
      "epoch:10, batch: 1311,  loss: 0.9919987916946411\n",
      "epoch:10, batch: 1312,  loss: 1.1705480813980103\n",
      "epoch:10, batch: 1313,  loss: 0.9157123565673828\n",
      "epoch:10, batch: 1314,  loss: 0.9622963070869446\n",
      "epoch:10, batch: 1315,  loss: 1.3120113611221313\n",
      "epoch:10, batch: 1316,  loss: 1.354887843132019\n",
      "epoch:10, batch: 1317,  loss: 1.1021411418914795\n",
      "epoch:10, batch: 1318,  loss: 0.892967939376831\n",
      "epoch:10, batch: 1319,  loss: 1.1691652536392212\n",
      "epoch:10, batch: 1320,  loss: 1.1094378232955933\n",
      "epoch:10, batch: 1321,  loss: 1.1264698505401611\n",
      "epoch:10, batch: 1322,  loss: 1.1784380674362183\n",
      "epoch:10, batch: 1323,  loss: 1.0892679691314697\n",
      "epoch:10, batch: 1324,  loss: 0.8106676936149597\n",
      "epoch:10, batch: 1325,  loss: 0.936202883720398\n",
      "epoch:10, batch: 1326,  loss: 0.8160641193389893\n",
      "epoch:10, batch: 1327,  loss: 1.0314165353775024\n",
      "epoch:10, batch: 1328,  loss: 1.0004390478134155\n",
      "epoch:10, batch: 1329,  loss: 0.8356353044509888\n",
      "epoch:10, batch: 1330,  loss: 1.162043571472168\n",
      "epoch:10, batch: 1331,  loss: 1.1780095100402832\n",
      "epoch:10, batch: 1332,  loss: 0.7440294623374939\n",
      "epoch:10, batch: 1333,  loss: 0.8797734379768372\n",
      "epoch:10, batch: 1334,  loss: 1.506644368171692\n",
      "epoch:10, batch: 1335,  loss: 1.0090663433074951\n",
      "epoch:10, batch: 1336,  loss: 0.886737585067749\n",
      "epoch:10, batch: 1337,  loss: 0.7679615616798401\n",
      "epoch:10, batch: 1338,  loss: 0.9290898442268372\n",
      "epoch:10, batch: 1339,  loss: 0.993187427520752\n",
      "epoch:10, batch: 1340,  loss: 0.9981231093406677\n",
      "epoch:10, batch: 1341,  loss: 1.055893898010254\n",
      "epoch:10, batch: 1342,  loss: 1.2009652853012085\n",
      "epoch:10, batch: 1343,  loss: 0.9584826231002808\n",
      "epoch:10, batch: 1344,  loss: 0.6424198746681213\n",
      "epoch:10, batch: 1345,  loss: 0.9107767343521118\n",
      "epoch:10, batch: 1346,  loss: 0.9569516777992249\n",
      "epoch:10, batch: 1347,  loss: 1.018004059791565\n",
      "epoch:10, batch: 1348,  loss: 1.0260428190231323\n",
      "epoch:10, batch: 1349,  loss: 0.8359717130661011\n",
      "epoch:10, batch: 1350,  loss: 1.0595279932022095\n",
      "epoch:10, batch: 1351,  loss: 1.041883945465088\n",
      "epoch:10, batch: 1352,  loss: 1.0234909057617188\n",
      "epoch:10, batch: 1353,  loss: 1.3955949544906616\n",
      "epoch:10, batch: 1354,  loss: 1.1822725534439087\n",
      "epoch:10, batch: 1355,  loss: 1.1311314105987549\n",
      "epoch:10, batch: 1356,  loss: 0.9840027093887329\n",
      "epoch:10, batch: 1357,  loss: 0.8198087811470032\n",
      "epoch:10, batch: 1358,  loss: 1.3201225996017456\n",
      "epoch:10, batch: 1359,  loss: 1.1841007471084595\n",
      "epoch:10, batch: 1360,  loss: 1.0024991035461426\n",
      "epoch:10, batch: 1361,  loss: 1.1235034465789795\n",
      "epoch:10, batch: 1362,  loss: 0.8427478075027466\n",
      "epoch:10, batch: 1363,  loss: 0.8827703595161438\n",
      "epoch:10, batch: 1364,  loss: 1.2519832849502563\n",
      "epoch:10, batch: 1365,  loss: 0.8983159065246582\n",
      "epoch:10, batch: 1366,  loss: 0.7550505995750427\n",
      "epoch:10, batch: 1367,  loss: 0.8486477136611938\n",
      "epoch:10, batch: 1368,  loss: 0.660114586353302\n",
      "epoch:10, batch: 1369,  loss: 1.3144749402999878\n",
      "epoch:10, batch: 1370,  loss: 0.9584726095199585\n",
      "epoch:10, batch: 1371,  loss: 1.0266855955123901\n",
      "epoch:10, batch: 1372,  loss: 0.9016439914703369\n",
      "epoch:10, batch: 1373,  loss: 1.1007875204086304\n",
      "epoch:10, batch: 1374,  loss: 1.2078182697296143\n",
      "epoch:10, batch: 1375,  loss: 1.160256028175354\n",
      "epoch:10, batch: 1376,  loss: 1.4344453811645508\n",
      "epoch:10, batch: 1377,  loss: 1.2428407669067383\n",
      "epoch:10, batch: 1378,  loss: 0.800042450428009\n",
      "epoch:10, batch: 1379,  loss: 0.8813561797142029\n",
      "epoch:10, batch: 1380,  loss: 0.9200953245162964\n",
      "epoch:10, batch: 1381,  loss: 1.0615997314453125\n",
      "epoch:10, batch: 1382,  loss: 0.7358869910240173\n",
      "epoch:10, batch: 1383,  loss: 0.8971922397613525\n",
      "epoch:10, batch: 1384,  loss: 0.8925212025642395\n",
      "epoch:10, batch: 1385,  loss: 0.7880205512046814\n",
      "epoch:10, batch: 1386,  loss: 1.021789312362671\n",
      "epoch:10, batch: 1387,  loss: 0.80484938621521\n",
      "epoch:10, batch: 1388,  loss: 1.194435954093933\n",
      "epoch:10, batch: 1389,  loss: 0.6572544574737549\n",
      "epoch:10, batch: 1390,  loss: 0.7599859237670898\n",
      "epoch:10, batch: 1391,  loss: 1.2526479959487915\n",
      "epoch:10, batch: 1392,  loss: 0.8260796070098877\n",
      "epoch:10, batch: 1393,  loss: 1.0278302431106567\n",
      "epoch:10, batch: 1394,  loss: 0.8455311059951782\n",
      "epoch:10, batch: 1395,  loss: 1.1712926626205444\n",
      "epoch:10, batch: 1396,  loss: 0.9681663513183594\n",
      "epoch:10, batch: 1397,  loss: 0.8158984780311584\n",
      "epoch:10, batch: 1398,  loss: 0.6772805452346802\n",
      "epoch:10, batch: 1399,  loss: 0.6991308927536011\n",
      "epoch:10, batch: 1400,  loss: 1.0056275129318237\n",
      "epoch:10, batch: 1401,  loss: 1.2225396633148193\n",
      "epoch:10, batch: 1402,  loss: 0.6657744646072388\n",
      "epoch:10, batch: 1403,  loss: 0.7823481559753418\n",
      "epoch:10, batch: 1404,  loss: 0.9419429302215576\n",
      "epoch:10, batch: 1405,  loss: 1.4389973878860474\n",
      "epoch:10, batch: 1406,  loss: 1.1917756795883179\n",
      "epoch:10, batch: 1407,  loss: 1.0390843152999878\n",
      "epoch:10, batch: 1408,  loss: 0.8153722882270813\n",
      "epoch:10, batch: 1409,  loss: 1.1100987195968628\n",
      "epoch:10, batch: 1410,  loss: 0.8850794434547424\n",
      "epoch:10, batch: 1411,  loss: 0.9058347344398499\n",
      "epoch:10, batch: 1412,  loss: 0.8424505591392517\n",
      "epoch:10, batch: 1413,  loss: 0.7419860363006592\n",
      "epoch:10, batch: 1414,  loss: 1.020195484161377\n",
      "epoch:10, batch: 1415,  loss: 1.0136699676513672\n",
      "epoch:10, batch: 1416,  loss: 1.16537606716156\n",
      "epoch:10, batch: 1417,  loss: 1.1312437057495117\n",
      "epoch:10, batch: 1418,  loss: 1.1465749740600586\n",
      "epoch:10, batch: 1419,  loss: 0.9924116730690002\n",
      "epoch:10, batch: 1420,  loss: 1.060880422592163\n",
      "epoch:10, batch: 1421,  loss: 1.3091880083084106\n",
      "epoch:10, batch: 1422,  loss: 1.1160610914230347\n",
      "epoch:10, batch: 1423,  loss: 1.0716723203659058\n",
      "epoch:10, batch: 1424,  loss: 1.040076494216919\n",
      "epoch:10, batch: 1425,  loss: 1.2430824041366577\n",
      "epoch:10, batch: 1426,  loss: 0.9864262938499451\n",
      "epoch:10, batch: 1427,  loss: 1.082095980644226\n",
      "epoch:10, batch: 1428,  loss: 1.0321279764175415\n",
      "epoch:10, batch: 1429,  loss: 0.982924222946167\n",
      "epoch:10, batch: 1430,  loss: 1.031731128692627\n",
      "epoch:10, batch: 1431,  loss: 1.566914439201355\n",
      "epoch:10, batch: 1432,  loss: 1.2913758754730225\n",
      "epoch:10, batch: 1433,  loss: 1.0062623023986816\n",
      "epoch:10, batch: 1434,  loss: 1.0688389539718628\n",
      "epoch:10, batch: 1435,  loss: 0.6935851573944092\n",
      "epoch:10, batch: 1436,  loss: 0.9399096965789795\n",
      "epoch:10, batch: 1437,  loss: 0.9578955769538879\n",
      "epoch:10, batch: 1438,  loss: 1.102161169052124\n",
      "epoch:10, batch: 1439,  loss: 1.06512451171875\n",
      "epoch:10, batch: 1440,  loss: 1.2332465648651123\n",
      "epoch:10, batch: 1441,  loss: 0.9394400715827942\n",
      "epoch:10, batch: 1442,  loss: 0.9613581895828247\n",
      "epoch:10, batch: 1443,  loss: 1.3340091705322266\n",
      "epoch:10, batch: 1444,  loss: 0.9188138246536255\n",
      "epoch:10, batch: 1445,  loss: 0.9748646020889282\n",
      "epoch:10, batch: 1446,  loss: 0.8495709300041199\n",
      "epoch:10, batch: 1447,  loss: 0.7499335408210754\n",
      "epoch:10, batch: 1448,  loss: 0.953626811504364\n",
      "epoch:10, batch: 1449,  loss: 0.9234791994094849\n",
      "epoch:10, batch: 1450,  loss: 0.8630653619766235\n",
      "epoch:10, batch: 1451,  loss: 1.1177914142608643\n",
      "epoch:10, batch: 1452,  loss: 1.0821056365966797\n",
      "epoch:10, batch: 1453,  loss: 0.8284800052642822\n",
      "epoch:10, batch: 1454,  loss: 1.088085651397705\n",
      "epoch:10, batch: 1455,  loss: 1.0125561952590942\n",
      "epoch:10, batch: 1456,  loss: 0.9630911350250244\n",
      "epoch:10, batch: 1457,  loss: 0.9876282215118408\n",
      "epoch:10, batch: 1458,  loss: 0.8584550023078918\n",
      "epoch:10, batch: 1459,  loss: 1.174108624458313\n",
      "epoch:10, batch: 1460,  loss: 0.9831904172897339\n",
      "epoch:10, batch: 1461,  loss: 1.048887848854065\n",
      "epoch:10, batch: 1462,  loss: 1.0555410385131836\n",
      "epoch:10, batch: 1463,  loss: 0.9390823841094971\n",
      "epoch:10, batch: 1464,  loss: 1.1850358247756958\n",
      "epoch:10, batch: 1465,  loss: 0.9656618237495422\n",
      "epoch:10, batch: 1466,  loss: 0.8674432635307312\n",
      "epoch:10, batch: 1467,  loss: 1.0544328689575195\n",
      "epoch:10, batch: 1468,  loss: 1.4679803848266602\n",
      "epoch:10, batch: 1469,  loss: 0.866192102432251\n",
      "epoch:10, batch: 1470,  loss: 1.4075230360031128\n",
      "epoch:10, batch: 1471,  loss: 1.064985752105713\n",
      "epoch:10, batch: 1472,  loss: 1.2202750444412231\n",
      "epoch:10, batch: 1473,  loss: 0.7431250810623169\n",
      "epoch:10, batch: 1474,  loss: 1.027124047279358\n",
      "epoch:10, batch: 1475,  loss: 0.9134700298309326\n",
      "epoch:10, batch: 1476,  loss: 1.15060555934906\n",
      "epoch:10, batch: 1477,  loss: 0.8814542293548584\n",
      "epoch:10, batch: 1478,  loss: 0.7027006149291992\n",
      "epoch:10, batch: 1479,  loss: 1.1610043048858643\n",
      "epoch:10, batch: 1480,  loss: 1.0145407915115356\n",
      "epoch:10, batch: 1481,  loss: 1.27948796749115\n",
      "epoch:10, batch: 1482,  loss: 0.8068743944168091\n",
      "epoch:10, batch: 1483,  loss: 0.8716251850128174\n",
      "epoch:10, batch: 1484,  loss: 1.2645926475524902\n",
      "epoch:10, batch: 1485,  loss: 1.3453015089035034\n",
      "epoch:10, batch: 1486,  loss: 1.0475881099700928\n",
      "epoch:10, batch: 1487,  loss: 1.1886427402496338\n",
      "epoch:10, batch: 1488,  loss: 0.8889437317848206\n",
      "epoch:10, batch: 1489,  loss: 0.9994036555290222\n",
      "epoch:10, batch: 1490,  loss: 0.783661961555481\n",
      "epoch:10, batch: 1491,  loss: 1.3211873769760132\n",
      "epoch:10, batch: 1492,  loss: 0.7833221554756165\n",
      "epoch:10, batch: 1493,  loss: 1.1120953559875488\n",
      "epoch:10, batch: 1494,  loss: 0.8020502924919128\n",
      "epoch:10, batch: 1495,  loss: 0.900007426738739\n",
      "epoch:10, batch: 1496,  loss: 1.0756961107254028\n",
      "epoch:10, batch: 1497,  loss: 1.1135274171829224\n",
      "epoch:10, batch: 1498,  loss: 0.8615546822547913\n",
      "epoch:10, batch: 1499,  loss: 0.9179008603096008\n",
      "epoch:10, batch: 1500,  loss: 1.1577694416046143\n",
      "epoch:10, batch: 1501,  loss: 0.9477192163467407\n",
      "epoch:10, batch: 1502,  loss: 1.0661752223968506\n",
      "epoch:10, batch: 1503,  loss: 1.1069148778915405\n",
      "epoch:10, batch: 1504,  loss: 1.1294465065002441\n",
      "epoch:10, batch: 1505,  loss: 0.9211868643760681\n",
      "epoch:10, batch: 1506,  loss: 1.091023564338684\n",
      "epoch:10, batch: 1507,  loss: 1.0467089414596558\n",
      "epoch:10, batch: 1508,  loss: 1.2625446319580078\n",
      "epoch:10, batch: 1509,  loss: 0.839188814163208\n",
      "epoch:10, batch: 1510,  loss: 1.0932561159133911\n",
      "epoch:10, batch: 1511,  loss: 0.8934923410415649\n",
      "epoch:10, batch: 1512,  loss: 1.7633136510849\n",
      "epoch:10, batch: 1513,  loss: 0.9217234253883362\n",
      "epoch:10, batch: 1514,  loss: 1.0830241441726685\n",
      "epoch:10, batch: 1515,  loss: 1.0386319160461426\n",
      "epoch:10, batch: 1516,  loss: 0.9856652021408081\n",
      "epoch:10, batch: 1517,  loss: 1.0704841613769531\n",
      "epoch:10, batch: 1518,  loss: 1.20930016040802\n",
      "epoch:10, batch: 1519,  loss: 0.7284687161445618\n",
      "epoch:10, batch: 1520,  loss: 0.9098465442657471\n",
      "epoch:10, batch: 1521,  loss: 1.0635440349578857\n",
      "epoch:10, batch: 1522,  loss: 0.7726916074752808\n",
      "epoch:10, batch: 1523,  loss: 1.0763671398162842\n",
      "epoch:10, batch: 1524,  loss: 0.8511461615562439\n",
      "epoch:10, batch: 1525,  loss: 0.7600560784339905\n",
      "epoch:10, batch: 1526,  loss: 0.7964752316474915\n",
      "epoch:10, batch: 1527,  loss: 1.1446928977966309\n",
      "epoch:10, batch: 1528,  loss: 1.028452754020691\n",
      "epoch:10, batch: 1529,  loss: 0.9086385369300842\n",
      "epoch:10, batch: 1530,  loss: 0.8815139532089233\n",
      "epoch:10, batch: 1531,  loss: 0.9738853573799133\n",
      "epoch:10, batch: 1532,  loss: 1.0802675485610962\n",
      "epoch:10, batch: 1533,  loss: 0.899724543094635\n",
      "epoch:10, batch: 1534,  loss: 1.0215312242507935\n",
      "epoch:10, batch: 1535,  loss: 0.8502365946769714\n",
      "epoch:10, batch: 1536,  loss: 1.1808061599731445\n",
      "epoch:10, batch: 1537,  loss: 0.9034533500671387\n",
      "epoch:10, batch: 1538,  loss: 0.8374739289283752\n",
      "epoch:10, batch: 1539,  loss: 0.9069949388504028\n",
      "epoch:10, batch: 1540,  loss: 1.420031189918518\n",
      "epoch:10, batch: 1541,  loss: 0.8483087420463562\n",
      "epoch:10, batch: 1542,  loss: 1.0562423467636108\n",
      "epoch:10, batch: 1543,  loss: 1.0095316171646118\n",
      "epoch:10, batch: 1544,  loss: 1.20537269115448\n",
      "epoch:10, batch: 1545,  loss: 0.8148394227027893\n",
      "epoch:10, batch: 1546,  loss: 1.0660595893859863\n",
      "epoch:10, batch: 1547,  loss: 0.9927184581756592\n",
      "epoch:10, batch: 1548,  loss: 0.95027756690979\n",
      "epoch:10, batch: 1549,  loss: 0.8418241739273071\n",
      "epoch:10, batch: 1550,  loss: 0.8351438045501709\n",
      "epoch:10, batch: 1551,  loss: 0.7210291624069214\n",
      "epoch:10, batch: 1552,  loss: 0.931242048740387\n",
      "epoch:10, batch: 1553,  loss: 0.811263382434845\n",
      "epoch:10, batch: 1554,  loss: 0.8181552886962891\n",
      "epoch:10, batch: 1555,  loss: 0.9575273990631104\n",
      "epoch:10, batch: 1556,  loss: 0.9251204133033752\n",
      "epoch:10, batch: 1557,  loss: 1.0578705072402954\n",
      "epoch:10, batch: 1558,  loss: 0.8545895218849182\n",
      "epoch:10, batch: 1559,  loss: 0.8374894857406616\n",
      "epoch:10, batch: 1560,  loss: 1.0559523105621338\n",
      "epoch:10, batch: 1561,  loss: 0.948678731918335\n",
      "epoch:10, batch: 1562,  loss: 0.8332794904708862\n",
      "epoch:10, batch: 1563,  loss: 0.9049927592277527\n",
      "epoch:10, batch: 1564,  loss: 0.8687233924865723\n",
      "epoch:10, batch: 1565,  loss: 1.2154723405838013\n",
      "epoch:10, batch: 1566,  loss: 0.9403365254402161\n",
      "epoch:10, batch: 1567,  loss: 0.9835704565048218\n",
      "epoch:10, batch: 1568,  loss: 0.7627803087234497\n",
      "epoch:10, batch: 1569,  loss: 1.225199818611145\n",
      "epoch:10, batch: 1570,  loss: 1.2597129344940186\n",
      "epoch:10, batch: 1571,  loss: 0.9907553195953369\n",
      "epoch:10, batch: 1572,  loss: 0.7981663942337036\n",
      "epoch:10, batch: 1573,  loss: 1.1136101484298706\n",
      "epoch:10, batch: 1574,  loss: 0.9739287495613098\n",
      "epoch:10, batch: 1575,  loss: 1.1739747524261475\n",
      "epoch:10, batch: 1576,  loss: 1.1176397800445557\n",
      "epoch:10, batch: 1577,  loss: 1.1614224910736084\n",
      "epoch:10, batch: 1578,  loss: 1.4098793268203735\n",
      "epoch:10, batch: 1579,  loss: 1.1967110633850098\n",
      "epoch:10, batch: 1580,  loss: 1.0224659442901611\n",
      "epoch:10, batch: 1581,  loss: 0.7964041829109192\n",
      "epoch:10, batch: 1582,  loss: 1.1404248476028442\n",
      "epoch:10, batch: 1583,  loss: 1.0102136135101318\n",
      "epoch:10, batch: 1584,  loss: 1.1115679740905762\n",
      "epoch:10, batch: 1585,  loss: 1.21384859085083\n",
      "epoch:10, batch: 1586,  loss: 1.0766489505767822\n",
      "epoch:10, batch: 1587,  loss: 1.1721404790878296\n",
      "epoch:10, batch: 1588,  loss: 0.8421878218650818\n",
      "epoch:10, batch: 1589,  loss: 1.31754469871521\n",
      "epoch:10, batch: 1590,  loss: 0.9536834955215454\n",
      "epoch:10, batch: 1591,  loss: 1.0726555585861206\n",
      "epoch:10, batch: 1592,  loss: 1.3273069858551025\n",
      "epoch:10, batch: 1593,  loss: 1.3111315965652466\n",
      "epoch:10, batch: 1594,  loss: 1.0402512550354004\n",
      "epoch:10, batch: 1595,  loss: 0.975760281085968\n",
      "epoch:10, batch: 1596,  loss: 0.7726747393608093\n",
      "epoch:10, batch: 1597,  loss: 1.1762588024139404\n",
      "epoch:10, batch: 1598,  loss: 1.0852444171905518\n",
      "epoch:10, batch: 1599,  loss: 1.4213334321975708\n",
      "epoch:10, batch: 1600,  loss: 0.9877316951751709\n",
      "epoch:10, batch: 1601,  loss: 0.956589937210083\n",
      "epoch:10, batch: 1602,  loss: 0.9450245499610901\n",
      "epoch:10, batch: 1603,  loss: 1.125741958618164\n",
      "epoch:10, batch: 1604,  loss: 0.9402338266372681\n",
      "epoch:10, batch: 1605,  loss: 0.7118473649024963\n",
      "epoch:10, batch: 1606,  loss: 1.027007818222046\n",
      "epoch:10, batch: 1607,  loss: 0.8912875652313232\n",
      "epoch:10, batch: 1608,  loss: 0.7660140991210938\n",
      "epoch:10, batch: 1609,  loss: 1.1025104522705078\n",
      "epoch:10, batch: 1610,  loss: 0.9911928176879883\n",
      "epoch:10, batch: 1611,  loss: 1.12285578250885\n",
      "epoch:10, batch: 1612,  loss: 1.1815340518951416\n",
      "epoch:10, batch: 1613,  loss: 0.9144526124000549\n",
      "epoch:10, batch: 1614,  loss: 0.9586183428764343\n",
      "epoch:10, batch: 1615,  loss: 1.07487952709198\n",
      "epoch:10, batch: 1616,  loss: 0.9235180616378784\n",
      "epoch:10, batch: 1617,  loss: 1.1394932270050049\n",
      "epoch:10, batch: 1618,  loss: 1.0243158340454102\n",
      "epoch:10, batch: 1619,  loss: 1.0346168279647827\n",
      "epoch:10, batch: 1620,  loss: 0.8701143860816956\n",
      "epoch:10, batch: 1621,  loss: 1.1773256063461304\n",
      "epoch:10, batch: 1622,  loss: 0.8273472785949707\n",
      "epoch:10, batch: 1623,  loss: 0.9193131923675537\n",
      "epoch:10, batch: 1624,  loss: 1.2196139097213745\n",
      "epoch:10, batch: 1625,  loss: 0.8981594443321228\n",
      "epoch:10, batch: 1626,  loss: 1.048018455505371\n",
      "epoch:10, batch: 1627,  loss: 0.6914636492729187\n",
      "epoch:10, batch: 1628,  loss: 0.825535237789154\n",
      "epoch:10, batch: 1629,  loss: 0.9622132182121277\n",
      "epoch:10, batch: 1630,  loss: 0.9473929405212402\n",
      "epoch:10, batch: 1631,  loss: 1.1303925514221191\n",
      "epoch:10, batch: 1632,  loss: 0.7497446537017822\n",
      "epoch:10, batch: 1633,  loss: 0.9192444086074829\n",
      "epoch:10, batch: 1634,  loss: 1.1665234565734863\n",
      "epoch:10, batch: 1635,  loss: 0.8729240894317627\n",
      "epoch:10, batch: 1636,  loss: 1.201098084449768\n",
      "epoch:10, batch: 1637,  loss: 0.9089167714118958\n",
      "epoch:10, batch: 1638,  loss: 1.2500108480453491\n",
      "epoch:10, batch: 1639,  loss: 1.09272038936615\n",
      "epoch:10, batch: 1640,  loss: 0.7969439029693604\n",
      "epoch:10, batch: 1641,  loss: 1.1196763515472412\n",
      "epoch:10, batch: 1642,  loss: 1.2842837572097778\n",
      "epoch:10, batch: 1643,  loss: 0.9320524334907532\n",
      "epoch:10, batch: 1644,  loss: 0.8853119015693665\n",
      "epoch:10, batch: 1645,  loss: 0.852923572063446\n",
      "epoch:10, batch: 1646,  loss: 1.0187439918518066\n",
      "epoch:10, batch: 1647,  loss: 1.0043874979019165\n",
      "epoch:10, batch: 1648,  loss: 1.194087028503418\n",
      "epoch:10, batch: 1649,  loss: 0.9792899489402771\n",
      "epoch:10, batch: 1650,  loss: 1.0321124792099\n",
      "epoch:10, batch: 1651,  loss: 1.0835055112838745\n",
      "epoch:10, batch: 1652,  loss: 0.9406836628913879\n",
      "epoch:10, batch: 1653,  loss: 0.8563879132270813\n",
      "epoch:10, batch: 1654,  loss: 1.1589919328689575\n",
      "epoch:10, batch: 1655,  loss: 1.017524242401123\n",
      "epoch:10, batch: 1656,  loss: 1.0323386192321777\n",
      "epoch:10, batch: 1657,  loss: 1.2114108800888062\n",
      "epoch:10, batch: 1658,  loss: 1.1210862398147583\n",
      "epoch:10, batch: 1659,  loss: 0.9153589010238647\n",
      "epoch:10, batch: 1660,  loss: 0.8838056325912476\n",
      "epoch:10, batch: 1661,  loss: 1.0303736925125122\n",
      "epoch:10, batch: 1662,  loss: 0.7629966735839844\n",
      "epoch:10, batch: 1663,  loss: 0.7536489367485046\n",
      "epoch:10, batch: 1664,  loss: 1.2749232053756714\n",
      "epoch:10, batch: 1665,  loss: 1.231558084487915\n",
      "epoch:10, batch: 1666,  loss: 1.2837282419204712\n",
      "epoch:10, batch: 1667,  loss: 0.6113519072532654\n",
      "epoch:10, batch: 1668,  loss: 1.4585071802139282\n",
      "epoch:10, batch: 1669,  loss: 0.835239827632904\n",
      "epoch:10, batch: 1670,  loss: 1.0880882740020752\n",
      "epoch:10, batch: 1671,  loss: 1.076331377029419\n",
      "epoch:10, batch: 1672,  loss: 0.9723281860351562\n",
      "epoch:10, batch: 1673,  loss: 0.8770936131477356\n",
      "epoch:10, batch: 1674,  loss: 0.7106053829193115\n",
      "epoch:10, batch: 1675,  loss: 1.2812529802322388\n",
      "epoch:10, batch: 1676,  loss: 0.9655619263648987\n",
      "epoch:10, batch: 1677,  loss: 1.0923793315887451\n",
      "epoch:10, batch: 1678,  loss: 1.0467125177383423\n",
      "epoch:10, batch: 1679,  loss: 1.1095378398895264\n",
      "epoch:10, batch: 1680,  loss: 0.8934377431869507\n",
      "epoch:10, batch: 1681,  loss: 0.8329795002937317\n",
      "epoch:10, batch: 1682,  loss: 1.152422547340393\n",
      "epoch:10, batch: 1683,  loss: 0.9067099690437317\n",
      "epoch:10, batch: 1684,  loss: 1.0466524362564087\n",
      "epoch:10, batch: 1685,  loss: 0.7621606588363647\n",
      "epoch:10, batch: 1686,  loss: 1.1578773260116577\n",
      "epoch:10, batch: 1687,  loss: 0.9490776062011719\n",
      "epoch:10, batch: 1688,  loss: 0.7377850413322449\n",
      "epoch:10, batch: 1689,  loss: 0.9174836874008179\n",
      "epoch:10, batch: 1690,  loss: 1.1565905809402466\n",
      "epoch:10, batch: 1691,  loss: 0.6968422532081604\n",
      "epoch:10, batch: 1692,  loss: 0.6214842796325684\n",
      "epoch:10, batch: 1693,  loss: 0.9443843960762024\n",
      "epoch:10, batch: 1694,  loss: 1.2494484186172485\n",
      "epoch:10, batch: 1695,  loss: 0.6793854832649231\n",
      "epoch:10, batch: 1696,  loss: 1.3012433052062988\n",
      "epoch:10, batch: 1697,  loss: 1.1048340797424316\n",
      "epoch:10, batch: 1698,  loss: 1.4697383642196655\n",
      "epoch:10, batch: 1699,  loss: 0.9680060148239136\n",
      "epoch:10, batch: 1700,  loss: 0.910554826259613\n",
      "epoch:10, batch: 1701,  loss: 0.8920583128929138\n",
      "epoch:10, batch: 1702,  loss: 1.0413239002227783\n",
      "epoch:10, batch: 1703,  loss: 0.8634670972824097\n",
      "epoch:10, batch: 1704,  loss: 0.9347248077392578\n",
      "epoch:10, batch: 1705,  loss: 0.9778653979301453\n",
      "epoch:10, batch: 1706,  loss: 1.0527260303497314\n",
      "epoch:10, batch: 1707,  loss: 1.231751799583435\n",
      "epoch:10, batch: 1708,  loss: 0.9552737474441528\n",
      "epoch:10, batch: 1709,  loss: 1.105130910873413\n",
      "epoch:10, batch: 1710,  loss: 1.1796109676361084\n",
      "epoch:10, batch: 1711,  loss: 0.8877683281898499\n",
      "epoch:10, batch: 1712,  loss: 1.161062240600586\n",
      "epoch:10, batch: 1713,  loss: 0.8077700734138489\n",
      "epoch:10, batch: 1714,  loss: 1.1926311254501343\n",
      "epoch:10, batch: 1715,  loss: 1.3889760971069336\n",
      "epoch:10, batch: 1716,  loss: 1.2123841047286987\n",
      "epoch:10, batch: 1717,  loss: 1.2733674049377441\n",
      "epoch:10, batch: 1718,  loss: 1.3714494705200195\n",
      "epoch:10, batch: 1719,  loss: 1.1759984493255615\n",
      "epoch:10, batch: 1720,  loss: 0.8602250814437866\n",
      "epoch:10, batch: 1721,  loss: 0.9496396780014038\n",
      "epoch:10, batch: 1722,  loss: 1.3185389041900635\n",
      "epoch:10, batch: 1723,  loss: 0.8134360909461975\n",
      "epoch:10, batch: 1724,  loss: 1.201918125152588\n",
      "epoch:10, batch: 1725,  loss: 1.215264081954956\n",
      "epoch:10, batch: 1726,  loss: 1.0482003688812256\n",
      "epoch:10, batch: 1727,  loss: 1.0178927183151245\n",
      "epoch:10, batch: 1728,  loss: 0.8371263742446899\n",
      "epoch:10, batch: 1729,  loss: 0.9737449288368225\n",
      "epoch:10, batch: 1730,  loss: 1.0983526706695557\n",
      "epoch:10, batch: 1731,  loss: 1.159112811088562\n",
      "epoch:10, batch: 1732,  loss: 1.2819081544876099\n",
      "epoch:10, batch: 1733,  loss: 1.004797101020813\n",
      "epoch:10, batch: 1734,  loss: 1.0496788024902344\n",
      "epoch:10, batch: 1735,  loss: 0.9427557587623596\n",
      "epoch:10, batch: 1736,  loss: 1.1164158582687378\n",
      "epoch:10, batch: 1737,  loss: 1.0866522789001465\n",
      "epoch:10, batch: 1738,  loss: 0.8596946597099304\n",
      "epoch:10, batch: 1739,  loss: 1.1532973051071167\n",
      "epoch:10, batch: 1740,  loss: 0.946747899055481\n",
      "epoch:10, batch: 1741,  loss: 1.2266457080841064\n",
      "epoch:10, batch: 1742,  loss: 1.0548899173736572\n",
      "epoch:10, batch: 1743,  loss: 1.2365833520889282\n",
      "epoch:10, batch: 1744,  loss: 1.0853520631790161\n",
      "epoch:10, batch: 1745,  loss: 0.9326469302177429\n",
      "epoch:10, batch: 1746,  loss: 1.1591050624847412\n",
      "epoch:10, batch: 1747,  loss: 1.1132228374481201\n",
      "epoch:10, batch: 1748,  loss: 1.0976474285125732\n",
      "epoch:10, batch: 1749,  loss: 0.724876880645752\n",
      "epoch:10, batch: 1750,  loss: 0.889618456363678\n",
      "epoch:10, batch: 1751,  loss: 1.228518009185791\n",
      "epoch:10, batch: 1752,  loss: 0.9678230285644531\n",
      "epoch:10, batch: 1753,  loss: 0.868481457233429\n",
      "epoch:10, batch: 1754,  loss: 0.8942508697509766\n",
      "epoch:10, batch: 1755,  loss: 0.9584813117980957\n",
      "epoch:10, batch: 1756,  loss: 0.977130115032196\n",
      "epoch:10, batch: 1757,  loss: 1.2799476385116577\n",
      "epoch:10, batch: 1758,  loss: 0.6146237254142761\n",
      "epoch:10, batch: 1759,  loss: 1.0568761825561523\n",
      "epoch:10, batch: 1760,  loss: 0.8255033493041992\n",
      "epoch:10, batch: 1761,  loss: 1.1698832511901855\n",
      "epoch:10, batch: 1762,  loss: 0.9060925245285034\n",
      "epoch:10, batch: 1763,  loss: 0.8654490113258362\n"
     ]
    }
   ],
   "source": [
    "from IPython.utils.sysinfo import num_cpus\n",
    "#train the neural network\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'epoch:{epoch + 1}, batch: {i+1},  loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtD6Op4f727n",
    "outputId": "22c4bbd0-26a2-4356-b7f8-3e689ae784b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 69 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDqBdb8SQb8C",
    "outputId": "1a305e05-6d0c-4041-d97c-f1cb2d7ec979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 70 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the train images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1651dc0de6d24f58b17f8efeac64a692": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6cb91f0412434f22b3e059487c7d1224",
       "IPY_MODEL_65a1b6403b974035878c886adc934dd5",
       "IPY_MODEL_4a517d4427f84b0086bbfbc8a4293df9"
      ],
      "layout": "IPY_MODEL_b31be24fbebb49b9ac176550846f30bb"
     }
    },
    "33bd9b6d29e4409f9a6f679f349bd3fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "449464383d1a4b08aeaf1024afd60ed9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a2e7850b4064076a7bcc15cf3ac0e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a517d4427f84b0086bbfbc8a4293df9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a40b7331fdd64a34abe201482ad8e500",
      "placeholder": "​",
      "style": "IPY_MODEL_449464383d1a4b08aeaf1024afd60ed9",
      "value": " 561753746/561753746 [00:05&lt;00:00, 91546503.78it/s]"
     }
    },
    "65a1b6403b974035878c886adc934dd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33bd9b6d29e4409f9a6f679f349bd3fc",
      "max": 561753746,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a3949979b0e44739bd7d5ba02295b887",
      "value": 561753746
     }
    },
    "6cb91f0412434f22b3e059487c7d1224": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b914fb7adafe48ce8a170dfe020c163b",
      "placeholder": "​",
      "style": "IPY_MODEL_4a2e7850b4064076a7bcc15cf3ac0e4a",
      "value": "100%"
     }
    },
    "a3949979b0e44739bd7d5ba02295b887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a40b7331fdd64a34abe201482ad8e500": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b31be24fbebb49b9ac176550846f30bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b914fb7adafe48ce8a170dfe020c163b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
